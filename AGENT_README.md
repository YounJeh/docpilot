# DocPilot Agent - Assistant Conversationnel RAG

## Jour 5 : Agent + CLI/mini-UI & Qualit√©

DocPilot Agent est un assistant conversationnel intelligent qui utilise votre documentation GitHub et Google Drive pour r√©pondre √† vos questions. Il consomme le service MCP (Model Context Protocol) pour effectuer des recherches s√©mantiques et utilise des LLM (OpenAI ou Vertex AI) pour g√©n√©rer des r√©ponses contextualis√©es.

## üöÄ Fonctionnalit√©s

### ü§ñ Agent Conversationnel
- **Recherche s√©mantique** : Utilise le service MCP pour rechercher dans vos documents
- **Prompt RAG** : Construit des prompts enrichis avec le contexte pertinent
- **Citations** : Fournit des sources avec liens vers les documents originaux
- **Fallback intelligent** : R√©pond "je ne sais pas" quand l'information n'est pas disponible
- **Support multi-LLM** : Compatible OpenAI et Vertex AI

### üîß Interfaces d'utilisation
- **CLI moderne** : Interface en ligne de commande avec Typer et Rich
- **Interface web** : Application Streamlit intuitive
- **Filtres avanc√©s** : Par source (GitHub/Drive), repository, type MIME
- **Param√®tres configurables** : top-k, seuil de similarit√©

### üìä Observabilit√© & Qualit√©
- **Logs structur√©s** : Avec trace_id, latences, m√©triques d√©taill√©es
- **M√©triques temps r√©el** : Temps de r√©ponse, chunks analys√©s, taux de succ√®s
- **Gestion d'erreurs** : Fallback gracieux avec logging des erreurs
- **Session tracking** : Suivi des conversations et statistiques

## üì¶ Installation

```bash
# Cloner le repository
git clone <your-repo>
cd docpilot

# Installer les d√©pendances avec uv
uv sync

# Ou avec pip
pip install -e .
```

## ‚öôÔ∏è Configuration

### Variables d'environnement

```bash
# Service MCP (requis)
export MCP_URL="http://localhost:8000"

# Pour OpenAI (par d√©faut)
export OPENAI_API_KEY="your_openai_api_key"

# Pour Vertex AI
export PROJECT_ID="your_gcp_project_id"
export LLM_PROVIDER="vertex"

# Optionnel : logs
export LOG_LEVEL="INFO"
```

### Fichier .env
Cr√©ez un fichier `.env` dans le r√©pertoire racine :

```env
MCP_URL=http://localhost:8000
OPENAI_API_KEY=your_openai_api_key
PROJECT_ID=your_gcp_project_id
LLM_PROVIDER=openai
```

## üñ•Ô∏è Utilisation CLI

### Commandes de base

```bash
# Question simple
python cli.py ask "Comment d√©ployer une application sur Cloud Run ?"

# Avec filtres
python cli.py ask "Configuration Docker" --source github --top-k 5

# Filtrage par type de fichier
python cli.py ask "API endpoints" --mime text/markdown --threshold 0.8

# Filtrage par repository
python cli.py ask "Installation" --repo mon-projet --top-k 10

# Format JSON pour int√©gration
python cli.py ask "Documentation" --format json

# Utiliser Vertex AI
python cli.py ask "Machine learning" --llm vertex --project-id my-project
```

### Options disponibles

```bash
# Afficher l'aide
python cli.py ask --help

# V√©rifier l'√©tat du syst√®me
python cli.py health

# Configuration actuelle
python cli.py config --show
```

### Exemples complets

```bash
# Recherche dans GitHub uniquement
python cli.py ask "Comment configurer le CI/CD ?" \
  --source github \
  --top-k 5 \
  --threshold 0.7 \
  --verbose

# Recherche dans Google Drive
python cli.py ask "Processus d'onboarding" \
  --source gdrive \
  --mime application/pdf \
  --top-k 3

# Recherche sp√©cifique √† un repository
python cli.py ask "API documentation" \
  --repo my-api-project \
  --mime text/markdown \
  --format json
```

## üåê Interface Web Streamlit

### Lancement

```bash
# D√©marrer l'interface web
streamlit run streamlit_app.py

# Avec port personnalis√©
streamlit run streamlit_app.py --server.port 8501
```

### Fonctionnalit√©s

- **Interface intuitive** : Questions/r√©ponses en temps r√©el
- **Filtres visuels** : Configuration par interface graphique
- **M√©triques en direct** : Temps de r√©ponse, chunks analys√©s
- **Historique** : Conversations pr√©c√©dentes
- **Export** : Possibilit√© d'exporter les r√©ponses
- **Health check** : V√©rification de l'√©tat du syst√®me

## üß™ Tests

### Test complet du syst√®me

```bash
# Ex√©cuter la suite de tests
python test_agent.py
```

### Tests manuels

```bash
# Test de sant√© du syst√®me
python cli.py health --verbose

# Test avec questions d'exemple
python cli.py ask "Comment d√©ployer sur Cloud Run ?" --source github
python cli.py ask "Configuration Docker" --mime text/markdown
python cli.py ask "API endpoints" --top-k 7
```

## üìä Observabilit√©

### Logs structur√©s

L'agent g√©n√®re des logs structur√©s avec :
- **trace_id** : Identifiant unique par requ√™te
- **Timings** : Latences d√©taill√©es (search, LLM, total)
- **M√©triques** : Chunks scann√©s, sources utilis√©es
- **Filtres** : Param√®tres de recherche appliqu√©s
- **Erreurs** : Gestion et logging des erreurs

### M√©triques de session

```python
# Acc√®s aux statistiques via l'API
stats = agent.get_observability_stats()
print(f"Taux de succ√®s: {stats['session_stats']['success_rate']:.1%}")
print(f"Temps moyen: {stats['session_stats']['avg_response_time']:.3f}s")
```

### Exemple de logs

```json
{
  "timestamp": "2024-10-28T10:30:00Z",
  "event": "request_completed",
  "trace_id": "abc123-def456",
  "question": "Comment d√©ployer sur Cloud Run ?",
  "response_time": 2.456,
  "search_time": 0.123,
  "llm_time": 2.100,
  "chunks_scanned": 8,
  "fallback_used": false,
  "source_filter": "github",
  "top_k_requested": 10
}
```

## üîß Architecture

### Composants principaux

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   CLI/Web UI    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  DocPilot Agent ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ   MCP Service   ‚îÇ
‚îÇ   (Interface)   ‚îÇ    ‚îÇ     (Core)      ‚îÇ    ‚îÇ   (Backend)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                        ‚îÇ             ‚îÇ
                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇ OpenAI  ‚îÇ   ‚îÇ Vertex  ‚îÇ
                  ‚îÇ   API   ‚îÇ   ‚îÇ   AI    ‚îÇ
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Flux de traitement

1. **R√©ception** : Question utilisateur via CLI/Web
2. **Filtrage** : Application des filtres de recherche
3. **Recherche** : Appel au service MCP pour recherche s√©mantique
4. **Contextualisation** : Construction du prompt RAG
5. **G√©n√©ration** : Appel au LLM pour g√©n√©ration de r√©ponse
6. **Post-traitement** : Extraction des sources et formatage
7. **Logging** : Enregistrement des m√©triques et traces

## üõ†Ô∏è D√©veloppement

### Structure du projet

```
docpilot/
‚îú‚îÄ‚îÄ knowledge_copilot/
‚îÇ   ‚îú‚îÄ‚îÄ agent.py              # Agent principal
‚îÇ   ‚îú‚îÄ‚îÄ observability.py      # Logs et m√©triques
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ cli.py                    # Interface CLI
‚îú‚îÄ‚îÄ streamlit_app.py          # Interface web
‚îú‚îÄ‚îÄ test_agent.py             # Tests
‚îú‚îÄ‚îÄ main.py                   # API MCP (existant)
‚îî‚îÄ‚îÄ pyproject.toml            # Configuration
```

### Ajout de nouvelles fonctionnalit√©s

```python
# Exemple : nouveau provider LLM
class CustomLLMProvider(LLMProvider):
    async def generate_response(self, prompt: str) -> str:
        # Votre impl√©mentation
        pass

# Utilisation
agent = create_agent(
    mcp_url="http://localhost:8000",
    llm_provider=CustomLLMProvider()
)
```

## üìà Performance

### Optimisations

- **Cache** : Mise en cache des embeddings et r√©sultats
- **Parall√©lisation** : Recherche et traitement parall√®les
- **Streaming** : R√©ponses progressives pour les longues requ√™tes
- **Pagination** : Gestion efficace des gros volumes

### M√©triques typiques

- **Temps de recherche** : 100-500ms
- **Temps LLM** : 1-3s (selon le mod√®le)
- **Temps total** : 1.5-4s
- **Pr√©cision** : 85-95% (selon la qualit√© des documents)

## üîí S√©curit√©

### Bonnes pratiques

- **Variables d'environnement** : Cl√©s API s√©curis√©es
- **Validation** : Validation des entr√©es utilisateur
- **Rate limiting** : Protection contre l'abus
- **Logging** : Pas de donn√©es sensibles dans les logs

## üìö Exemples d'usage

### Cas d'usage typiques

1. **Documentation technique**
   ```bash
   python cli.py ask "Comment configurer SSL/TLS ?" --source github
   ```

2. **Proc√©dures m√©tier**
   ```bash
   python cli.py ask "Processus d'approbation budget" --source gdrive
   ```

3. **API Reference**
   ```bash
   python cli.py ask "Endpoint authentification" --mime text/markdown
   ```

4. **Troubleshooting**
   ```bash
   python cli.py ask "Erreur 500 API" --repo production-logs
   ```

## üêõ D√©pannage

### Probl√®mes courants

**Service MCP inaccessible**
```bash
# V√©rifier l'√©tat
python cli.py health

# V√©rifier la connectivit√©
curl http://localhost:8000/health
```

**Erreurs d'authentification LLM**
```bash
# V√©rifier les variables d'environnement
echo $OPENAI_API_KEY
echo $PROJECT_ID
```

**Pas de r√©sultats pertinents**
```bash
# R√©duire le seuil de similarit√©
python cli.py ask "votre question" --threshold 0.5

# Augmenter le top-k
python cli.py ask "votre question" --top-k 20
```

## ü§ù Contribution

Pour contribuer au projet :

1. Fork le repository
2. Cr√©er une branche feature
3. Ajouter des tests pour les nouvelles fonctionnalit√©s
4. V√©rifier que tous les tests passent
5. Cr√©er une Pull Request

## üìÑ Licence

MIT License - Voir le fichier LICENSE pour plus de d√©tails.

---

**DocPilot v1.0** - Assistant RAG avec Vertex AI et pgvector  
üöÅ *Votre copilote pour naviguer dans votre documentation*