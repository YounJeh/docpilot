{
  "folder_id": "1_99vZxxKKxeMnPvCGiwvDgzWWK1qV8JY",
  "documents_count": 4,
  "chunks_count": 28,
  "documents": [
    {
      "source": "gdrive",
      "uri": "gdrive://1B2HYO2ne9MmjXdDKxGJNucT62HtNo-ECECBswbxn2qs",
      "title": "docuemnt_test",
      "mime": "application/vnd.google-apps.document",
      "content_hash": "51d91a522f33e26ffd271601822eb1d7ca1d9119ba705468c62cfd20216e532a",
      "raw_text": "﻿test je suis bien",
      "metadata": {
        "drive_file_id": "1B2HYO2ne9MmjXdDKxGJNucT62HtNo-ECECBswbxn2qs",
        "name": "docuemnt_test",
        "mimeType": "application/vnd.google-apps.document",
        "modifiedTime": "2025-10-27T09:04:50.735Z",
        "webViewLink": "https://docs.google.com/document/d/1B2HYO2ne9MmjXdDKxGJNucT62HtNo-ECECBswbxn2qs/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z"
      }
    },
    {
      "source": "gdrive",
      "uri": "gdrive://1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0",
      "title": "Rapport de Stage Youn Jéhanno",
      "mime": "application/vnd.google-apps.document",
      "content_hash": "2bd4dfc3a1adad44451831e812b1191c587f82160a47b287a476fef0a222002d",
      "raw_text": "﻿INSA Rennes\r\nMÉMOIRE DE STAGE\r\nModélisation statistique du risque de défaut\r\n\r\n  \r\n\r\n\r\n\r\nMémoire CONFIDENTIEL\r\n\r\n\r\n4ème année Génie Mathématiques\r\nINSA Rennes\r\nNom de l’entreprise : Crédit Mutuel ARKEA\r\nResponsable de stage : Guillaume Le Coz\r\nEnseignant référent : Loïc Hervé\r\nDate du stage : 13 Janvier au 6 Mars 2020\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n________________\r\n\r\n\r\nSommaire\r\n\r\n\r\nRemerciements\r\nNote de Confidentialité\r\n1. Introduction\r\n2. Présentation de l’entreprise\r\n2.1. Groupe Arkéa\r\n2.1.1 Historique et organisation\r\n2.1.2 Quelques chiffres\r\n2.1.3 Souhait d’indépendance d’Arkéa\r\n2.1.4 Organisation de la direction des risques et du département Modélisation Statistique des Risques\r\n3. Environnement de travail\r\n3.1. Mise en situation\r\n3.1.1 Définition population observée\r\n3.1.2 Problématique actuelle\r\n3.2. Environnement de travail\r\n3.3.Description des bases de données utilisées\r\n4. Constitution de la base de données\r\n4.1. Détermination de la variable cible\r\n4.2. Exploration des données\r\n4.3. Définition du périmètre\r\n4.4. Construction de la base\r\n5. Analyse de la qualité des données\r\n5.1. Préparation des données\r\n5.2. Maquette d’exploration des données\r\n5.3. Redécoupage des variables (CHAID)\r\n5.4. Recodage des données manquantes\r\n6. Développement des modèles\r\n6.1. Sélection des variables discriminantes\r\n6.1.1. ACP\r\n6.1.2. V de Cramer\r\n6.1.3. Test de corrélation\r\n6.2. Echantillonnage\r\n6.3. Restriction des variables discriminantes - Stepwise & Bootstrap\r\n8.3.1 Forward step\r\n6.3.2 Bootstrap\r\n6.4. Sélection d’un ou plusieurs modèles\r\n6.5. Résultat\r\n6.6. Analyse des résultats\r\n7. Découpage du score\r\n7.1. Construction et validation du modèle\r\n7.1.1 Courbe ROC\r\n7.1.2 Indice de Gini\r\n8. Compétences développées\r\n\r\n\r\n________________\r\n\r\n\r\nRemerciements\r\n\r\n\r\nécrire les remerciements\r\n\r\n\r\nNote de Confidentialité\r\nCe présent rapport ainsi que toutes les informations qu’il contient sont strictement confidentiels exclusivement au corps professoral de l’INSA de Rennes ou à une personne interne à l’équipe statistique du département modélisation statistique des risques.\r\nLes chiffres présents dans ce rapport ont été volontairement faussés toujours dans cet objectif de confidentialité. Ils restent toutefois cohérents.\r\nMerci de bien vouloir respecter la confidentialité de ce document.\r\n________________\r\n\r\n\r\n\r\n\r\n1. Introduction\r\n\r\n\r\nDans le cadre de ma formation de 4ème année Génie Mathématiques à l’INSA de Rennes, j’ai effectué un stage d’une durée de 2 mois au siège du Crédit Mutuel Arkéa, à Brest. J’ai intégré le département Modélisation Statistique des risques au sein du service “Modélisation Système de Notation Interne”. Le département est chargé d’évaluer le risque de crédit des clients en fonction de leur profil, le risque opérationnel ainsi que le calcul de l'exigence en fonds propres associée aux risques pris par la banque. A cela, s'ajoutent différentes études statistiques pour la Direction en raison de nombreux et nouveaux projets impliquants pour le Groupe.\r\nMa mission a consisté à créer un nouveau modèle statistique permettant d’obtenir une meilleure prédiction du défaut des clients épargnants purs et des clients avec compte créditeur et sans crédit.\r\n2. Présentation de l’entreprise\r\n2.1. Groupe Arkéa\r\n2.1.1 Historique et organisation\r\nLe Groupe ARKÉA est un groupe coopératif et mutualiste qui comprend plus de 20 filiales spécialisées qui couvrent tous les métiers de la banque, de la finance et de l'assurance, comme Fortuneo (banque en ligne), Suravenir Assurances, Federal Finance (gestion d'actifs pour compte de tiers), et Financo (crédit à la consommation). Son siège social est implanté au Relecq-Kerhuon dans le Finistère.\r\nIl est né de l'alliance des fédérations de Crédit Mutuel de Bretagne et du Sud-Ouest (CMB, CMSO). C’est le deuxième pôle régional du Crédit Mutuel. Ancré sur ses territoires et comptant près de 10 000 salariés. Il regroupe un grand nombre de fonctions supports et de métiers d'expertise (Finance, Organisation, Audit, Marketing, Juridique,...).\r\n2.1.2 Quelques chiffres\r\nLe Groupe ARKÉA en 2020 représente :\r\n❏     4,5 millions de clients\r\n❏     10 500 salariés\r\n❏     111.2 millions d'euros encours d'épargne (épargne financière, épargne assurance, dépôts).\r\n❏     6.7 milliards d'euros de fonds propres\r\n❏     437 millions d'euros de résultat net\r\n❏     135 milliards d’euros de bilan comptable \r\n2.1.3 Contexte d’indépendance\r\nDepuis 2015, le groupe Arkéa souhaite devenir indépendant du Crédit Mutuel. Cette désaffiliation est prévue pour janvier 2021. \r\nDans ce cadre, il a été engagé des travaux préparatoires de refonte complète des modèles statistiques (aujourd’hui réalisés sur l’ensemble du Crédit Mutuel) sur la population Arkéa.\r\nL’objectif de ces travaux est de prouver à la Banque Centrale Européenne qu’elle est capable de fonctionner indépendamment.\r\n\r\n\r\nCe stage s’inscrit dans ce contexte.\r\n2.1.4 Organisation de la direction des risques et du département Modélisation Statistique des Risques\r\nLe Groupe ARKÉA est divisé en différentes directions avec leurs activités et objectifs propres. L’une des directions, la direction des risques, est celle dans laquelle j’ai effectué mon stage. Elle est composée de 130 collaborateurs dont les principales missions sont la mise en place et le pilotage du dispositif global de gestion des risques du groupe.\r\nLe département Modélisation Statistique des Risques est composé d’une dizaine de statisticiens/data scientists. Les activités principales des différents services au sein du département sont les suivants :\r\n❏        Service Gestion de la donnée et modélisation du risque opérationnel\r\n●           Construire, suivre et maintenir les bases de données clients, métiers, IT,…  servant aux études et à la modélisation statistiques du Groupe et des filiales.\r\n●          Contribuer à la cartographie globale des risques du Groupe et en déduire un état de reporting à des fins de communication aux organes dirigeants.\r\n●     Elaborer et quantifier des modèles de risques opérationnels dans le cadre du calcul de l’exigence en fonds propres.\r\n●         Détecter et contribuer à la prévention d’incidents opérationnels via la création de modèles de comportement de détection de fraude externe.\r\n❏        Service « Modélisation Système de Notation Interne »\r\n●     Créer et assurer la maintenance des modèles de notation interne statistiques (pour les fédérations ou les filiales) en réalisant des études statistiques de comportement des clients en termes de risques de crédit.\r\n●           Rédiger des cahiers des charges de cotation à destination de l'informatique et assurer la coordination de la mise en place des nouveaux algorithmes de notation (recette, assistance de second niveau, suivi des modèles etc…), en lien avec l'informatique et toutes les structures du Groupe impliquées dans la maîtrise des risques et dans la qualité des données en amont de la cotation (direction des engagements, filiales …).\r\n❏     Service « Paramètres et provisions du risque de crédit »\r\n●         Procéder statistiquement aux calculs de paramètres selon les normes bâloise ou comptable IFRS 9 (Probabilité de défaut - PD, Pertes en cas de défaut - LGD et Facteur de conversion d’encours hors bilan en bilan - CCF), afin de permettre les calculs de l’exigence de fonds propres au titre du risque de crédit et du niveau de provisionnement du groupe.\r\n●          Accompagner le process par le suivi et la maintenance des modèles de protection (bâlois et provisionnement), en lien avec l'informatique et toutes les structures du Groupe impliquées dans la maîtrise des risques et dans la qualité des données en amont des systèmes de détermination des paramètres (direction des engagements, filiales …).\r\n3. Environnement de travail\r\n3.1. Mise en situation\r\n3.1.1 Définition population observée\r\nActuellement, les clients particuliers (personnes physiques) sont répartis en 11 segments distincts avec l’ordre des priorisation ci-dessous:\r\n  \r\n\r\nLes 6 segments 19,18,17 et 15 ont des cotations forfaitaires (même note attribuée à tous les membres de chaque segment) basées sur une étude des taux de défauts observés.\r\nLes 5 autres segments possèdent un algorithme statistiques spécifique qui attribue une cotation à ses membres. L’étude se concentre sur les segments 10 et 11.\r\nÉpargnant pur : clients sans prêt, sans risque en cours, qui n’est pas nouveau client et qui n’a    pas de compte courant.\r\nCompte courant créditeur sans prêt : clients sans prêt, sans risque en cours et qui n’est pas nouveaux client avec au moins un compte courant et avec au maximum un jour de débit sur les 6 mois d’observation.\r\n3.1.2 Problématique actuelle\r\nLa segmentation actuelle des populations pourrait être améliorée en regroupant les deux segment, « Épargnants purs » et « CC créditeur », qui présentent un faible nombre d’observations de défaut et des similarités d’un point de vue comportement et TD.\r\nActuellement, le modèle Épargnants Purs présente des insuffisances en terme de performance lié à ses faibles volumétries.\r\nMa mission a été de regrouper ces deux types de clients en un seul segment afin de voir si une meilleure prédiction globale serait réalisable.\r\n3.2. Environnement de travail\r\n\r\n\r\nLes bases de données utilisées sont issues de la BNS (Base Nationale Statistique) et c’est le logiciel R qui a servi à construire le modèle.\r\n\r\n\r\nL’étude des données a principalement été réalisée à partir de maquettes excel.\r\n\r\n\r\nPour ce projet, la majorité des fonctions utilisées sont issues d’un package interne ModelSNI regroupant un ensemble de syntaxes pour la construction d’un modèle de cotation.\r\n3.3.Description des bases de données utilisées\r\n\r\n\r\nLe modèle a été construit à partir des 8 bases de données suivantes :\r\n  \r\n\r\n4. Constitution de la base de données\r\n4.1. Détermination de la variable cible\r\n\r\n\r\nLa variable cible correspond au passage en défaut (appelée top_def). Elle vaut :\r\n\r\n\r\n* 1, si le client était sain en début de période d’observation du passage en défaut et est passé en défaut au cours de la période (qui dure 12 mois);\r\n* 0, si le client était sain en début de période et est resté sain pendant toute la période.\r\n4.2. Exploration des données\r\n\r\n\r\nAvant de commencer à manipuler les bases de données, il est important de se familiariser avec les données et de comprendre les variables. Un fichier Excel décrit l’ensemble des variables utilisées et facilite la compréhension de la base de données. Cette phase d’exploration va aussi permettre de repérer d'éventuelles valeurs aberrantes ainsi que des valeurs manquantes\r\n4.3. Définition du périmètre\r\n\r\n\r\nIl est important de définir le périmètre de l’étude pour travailler avec les bonnes données.\r\n\r\n\r\n* Périmètre définit pour l’étude : \r\n   * CMB (Crédit Mutuel de Bretagne), CMSO (Crédit Mutuel du Sud Ouest) et Fortunéo\r\n   * Segment 10 et 11, Épargnant pur et Compte courant créditeur sans prêt\r\n\r\n\r\n* Horizon de prédiction : 1 an\r\n\r\n\r\n* Dates de la base de modélisation :\r\n   * Base de modélisation : concaténation de 2 bases ( Ce choix ne garantit pas l’indépendance de nos observations mais permet d’avoir une base de données plus importante et évite les effets de saisonnalité)\r\n      * Base à décembre 2015 avec avec défaut observé sur l’année 2016\r\n      * Base à juin 2017 avec avec défaut observé entre juin 2016 et juin 2017\r\n\r\n\r\n\r\n\r\n  \r\n\r\n\r\n\r\n4.4. Construction de la base\r\n\r\n\r\nLa base est construite à partir des bases de décembre 2015 et de juin 2017, elles mêmes construites grâce à la jointure de l’ensemble des bases décrites dans le tableaux précédent.\r\n\r\n\r\nCette partie est très importante et chronophage car il faut que la base soit la plus complète et la plus fiable possible afin de pouvoir obtenir des résultat plus satisfaisants. \r\nIl a fallu faire face à de nombreux problèmes comme des individus manquants dans certaines bases. Les bases étant de plus très volumineuses, il est d’autant plus difficiles de les trouver.\r\nLa vérification des bases de données à chaques étapes de conception est donc très importante.\r\n\r\n\r\nLa base construite est alors constituée de 1 600 000 observations et de 312 variables.\r\n\r\n\r\n5. Analyse de la qualité des données\r\n5.1. Préparation des données\r\n\r\n\r\nLa préparation des données est composée de deux parties :\r\n\r\n\r\n* Recodage de certaines variables\r\n   * ex : Les données manquantes en (9999999 ou 0) sont recodées en NA\r\n       Les dates < 1901 ou égales à 0 sont recodées à NA car il s’agit d’une\r\n      codification liées aux valeurs manquantes\r\n       Les variables avec >100 modalités sont recodées afin de pouvoir appliquer les \r\n      méthodes statistiques de sélection de variables\r\n       Les valeurs avec des signes aberrants afin de s’assurer une bonne qualité des \r\n      données\r\n\r\n\r\n* Construction de nouvelles variables (moyennes, région) :\r\n   * 26 variables calculant la moyenne sur 6 mois de variables relatives aux comportements bancaires\r\n   * 2 variables indiquant la région d’habitation des tiers\r\n   * 7 variables calculant des sommes de soldes sur différents comptes épargnes\r\n\r\n\r\nLa construction de nouvelles variables a plusieurs avantages :\r\n\r\n\r\n* Fait entrer dans le modèle plusieurs variables en évitant la présence de variables corrélées\r\n* Limite le nombre de variables dans le modèle\r\n* Prend en compte un historique du comportement du client\r\n5.2. Maquette d’exploration des données \r\n\r\n\r\nUne fois les nouvelles variables créées, il reste de nombreuses variables qui n’apporte aucune information pour notre étude. Il faut donc les identifier et les supprimer dans un soucis de clarté. Pour cela, nous utilisons des maquettes d’exploration des données par Entité (1 :CMB, 3: CMSO, 21: Fortuneo). Cette étude par entité permet de s’assurer une homogénéité de la donnée entre les différents périmètres étudiés :\r\n\r\n\r\n* Données quantitatives : les valeurs étudiées sont les suivantes :\r\n   * nombre de valeurs manquantes\r\n   * variable constante ou non\r\n   * moyenne\r\n   * ecart-type\r\n   * minimum\r\n   * max\r\n   * quantiles 0.01, 0.05, 0.10, 0.20, 0.25,0.30, 0.40,0.50, 0.60, 0.70, 0.75,0.80, 0.90, 0.95, 0.99\r\n   * le pourcentage de 0.\r\n  \r\n                     \r\n\tMaquette d’exploration des données des variables quantitatives pour LE CMB (1)\r\n\t\r\n\r\nExemple :\r\n\r\n\r\nLes variables ddbu_dero et dfin_dero sont tout le temps égales à 0. Elles sont constantes et ne sont pas gardées dans le modèle.\r\n\r\n\r\nLa variables R_EPAMT_EPATOT possède des centiles chez Fortunéo au moins 10 fois supérieurs aux centiles du CMB et du CMSO. La variable est tout de même gardée car ses informations sont intéressantes.\r\n\r\n\r\n\r\n\r\n* Données qualitatives : les valeurs étudiées sont les suivantes :\r\n   * nombre de valeurs manquantes\r\n   * variable constante ou non\r\n   * effectif\r\n   * fréquence\r\n\r\n\r\n  \r\n\r\n\tMaquette d’exploration des données des variables qualitatives\r\n\t\r\n\r\n  \r\n\r\n\tLégende des alertes des variables qualitatives\r\n\t\r\n\r\nA partir de toutes ces informations, il faut déterminer quelles variables sont utiles et lesquelles ne le sont pas. \r\nLes variables constantes ou uniquement constituées de valeurs manquantes ne sont pas gardées.  Nous supprimons 173 variables.\r\n\r\n\r\nPour les autres variables, la décision revient à l’appréciation du statisticien. En fonction des expériences des modèles précédents et de notre jugement, 35 variables sont supprimées.\r\n\r\n\r\nSuite à l’exploration, nous supprimons 208 variables. Notre base contient alors 118 variables. \r\n5.3. Redécoupage des variables (CHAID)\r\nL’algorithme CHAID regroupe pour chaque variable explicative ses modalités les moins liées à la variable à expliquer, c’est-à-dire celles dont les taux de défaut sont les plus voisins au vu du test du khi-2. Il le fait de proche en proche par paires de modalités puis de regroupements de modalités, jusqu’à ce que ne subsistent plus que des regroupements de modalités aux taux de défaut suffisamment distincts.\r\n\r\n\r\nCe redécoupage est utile car nous manipulons des bases de données contenant de nombreuses observations. Cela va permettre de répartir la population en groupes homogènes, avec des variables discriminantes plus performantes. \r\n\r\n\r\n* Transformation des variables continues : discrétisation\r\n\r\n\r\n* Transformation des variables qualitatives : regroupement de modalités\r\n\r\n\r\nLe découpage ne doit pas être trop fin de manière à être le plus discriminant possible. Dans notre cas, pour certaines variables, on observe une tendance linéaire du taux de défaut en fonction des valeurs de la variable. Il est donc important que notre découpage suive cette tendance. Pour cela il faut trouver le bon seuil pour notre découpage car un découpage trop fin ne respectera pas la règle mais un découpage trop large fera perdre de l’information.\r\n\r\n\r\nLa base est maintenant constituée uniquement de données qualitatives. Il reste à corriger le problème des valeurs manquantes.\r\n5.4. Recodage des données manquantes\r\n\r\n\r\nLes valeurs manquantes de chaque variable sont regroupées dans une modalité puis leur taux de défauts est analysé. Selon les volumes observés, les données manquantes sont affectées à la catégorie la plus proche (en terme de taux de défaut) ou bien une modalité spécifique aux données manquantes est créée.\r\n\r\n\r\nLorsque le taux de valeurs manquantes est supérieur à 10% pour une variable, on considère les données manquantes comme une modalité spécifique.\r\nDans le cas contraire, celles-ci sont regroupées avec la modalité ayant le taux de défaut le plus proche.\r\n\r\n\r\n\r\n\r\n\r\n\r\n  \r\n\r\n\tMaquette du recodage des valeurs manquantes\r\n\t\r\n\r\n\r\n\r\nDans cet exemple, les données manquantes de la variables decp_EXI_AVOIR2 représentent moins de 10 % des observations. Elles sont donc regroupées avec la modalité [-INF,0] qui est la modalité possédant le taux de défaut le plus proche des valeurs manquantes.\r\n\r\n\r\nUne analyse “à dire d’expert” est également réalisée afin de s’assurer de la cohérence du découpage et de l’affectation des valeurs manquantes. En effet, dans certains cas où le nombre de valeurs manquantes est faible, le taux de défaut observé n’est pas significatif. L’affectation à la modalité ayant le taux de défaut le plus proche n’est donc pas forcément la meilleure. Il convient donc de l’affecter à la modalité répondant le mieux à la logique bancaire.\r\n\r\n\r\nCette étape permet aussi de vérifier les découpages qui ont été effectué par l’algorithme CHAID. Si une modalité contient trop peu d’individus, elle est regroupée avec la modalité la plus proche en terme de valeur de la variable.\r\n\r\n\r\n6. Développement des modèles\r\n6.1. Sélection des variables discriminantes\r\n6.1.1. ACP \r\n\r\n\r\nL’ACP permet d’étudier et de visualiser des corrélations entre les variables, afin d’éventuellement limiter le nombre de variables à mesurer par la suite. L’objectif est d’obtenir des variables non corrélées qui sont des combinaisons linéaires des variables de départ, afin d’utiliser ces variables dans les méthodes de modélisation utilisées par la suite telles que la régression logistique ou l’analyse discriminante. Elle permet d’identifier des groupes homogènes d’observations, ou au contraire des observations atypiques.\r\n\r\n\r\nNous obtenons alors 25 composantes principales. A chaque composante principale, nous affectons les variables ayant un coefficient de corrélation supérieur à 0,7.\r\n6.1.2. V de Cramer\r\nPour estimer si les modalités sont probablement indépendantes ou au contraire liées, on a coutume de procéder à un test du V de Cramer qui reste stable si l’on augmente la taille de l’échantillon dans les mêmes proportions inter-modalités. Plus le V de Cramer d’une variable est proche de zéro, plus il y a indépendance entre les deux variables étudiées. \r\nNous réalisons un test du V de Cramer entre la variable cible top_def et toutes les variables du modèle :\r\n  \r\n\r\nLa variable decp_risq_deg correspond à l’identifiant d’un risque dégradant. Elle possède un V de Cramer très élevé car les individus ayant un risque dégradant entraînant une dégradation de la cotation primaire ont un fort taux de défaut.\r\nLa variables decp_solde_cpt_ep2_PEP correspond au solde moyen mensuel du mois 2 sur le Plan Epargne Populaire de la personne. Elle possède un V de Cramer faible car la variable prend ses valeurs sur un mois. Ce temps n’est pas assez long pour avoir des valeurs significatives.\r\nOn retient :\r\n* Pour chaque composante principale de l’ACP, la ou les variables ayant le plus fort V de Cramer.\r\n* On ne retient pas les variables ayant un V de Cramer < 0,03. Dans notre cas, les V de Cramer sont très faible car le défaut est très dur a expliquer pour notre population donc cette règle n’est pas appliquée pour certains cas et nous essayerons de prendre les variables ayant des V de Cramer suffisamment grands.\r\n* Les moyennes, ratios ou autres variables combinées sont privilégiées par rapport aux variables simples (ex: moyenne sur 6 mois plutôt que valeur sur un mois)\r\n* Les variables sélectionnées dans l’ancien modèle. Ces variables sont censées être les plus explicatives pour les segments clients épargnants purs ainsi que les clients avec compte créditeur et sans crédit.\r\nVariables sélectionnées pour la modélisation : 28\r\nL’idée est de ne pas se limiter aux variables ayant un V de Cramer élevé, mais de sélectionner des variables moins discriminantes mais qui apportent une information supplémentaire.\r\n6.1.3. Test de corrélation\r\nOn réalise un test du V de Cramer entre les 28 variables sélectionnées pour la modélisation pour observer les corrélations. Les variables ayant une corrélation trop élevée ( >0,70) sont retirées si elles possèdent un V de Cramer inférieur à la variable avec laquelle elles sont corrélées. En cas de V de Cramer très proches, les variables préexistantes sont préférées car plus fiables. Celles-ci font l’objet de suivis périodiques.\r\n  \r\n\r\n\tCorrélation entre les variables PCS, decp_nbj_depaut, decp_NBJ_DEPAUT_6M et decp_EXI_AVOIR2\r\n\t\r\n\r\nLes variables decp_nbj_depaut, decp_NBJ_DEPAUT_6M et decp_EXI_AVOIR2 ont des corrélations supérieures à 0,70. Cela signifie qu’elles possèdent globalement la même information. Seul decp_NBJ_DEPAUT_6M est gardées car c’est la variable ayant le meilleur V de Cramer.\r\nLa variable PCS est gardée car faiblement corrélée à toutes les autres. Sa corrélation est faible par rapport aux autres variables car la PCS correspond à la catégorie socioprofessionnelle du foyer et les autres variables font référence à des données bancaires. Cela apporte donc une information complémentaire.\r\nVariables sélectionnées pour la modélisation : 21\r\n6.2. Echantillonnage\r\nDécoupage de la base de modélisation en deux échantillons :\r\n\r\n\r\n* Échantillon d’apprentissage : 70% de la base de modélisation. Sert à construire les modèles\r\n\r\n\r\n* Échantillon test : 30% de la base de modélisation. Sert à valider les modèles.\r\n\r\n\r\n  \r\n\r\n\r\n\r\nChaque échantillon possède la même proportion de défauts.\r\n\r\n\r\n6.3. Restriction des variables discriminantes \r\n                        - Stepwise & Bootstrap\r\n6.3.1 Régression stepwise\r\nLa régression Stepwise consiste à ajouter et à supprimer itérativement des prédicteurs, afin de trouver le sous-ensemble de variables dans l'ensemble de données résultant en le modèle le plus performant. L’entrée et la sortie d’une variable est testé  avec le test de rapport de vraisemblance.\r\nLe test de vraisemblance permet de comparer un modèle avec un sous-modèle et d’évaluer l’intérêt de la présence des termes complémentaires. On fait entrer ainsi de manière ascendante la modalité de la variable qui apparaît comme la plus significative au vu du test de rapport de vraisemblance. Puis, on teste la suppression de chaque variable du modèle avec le test de vraisemblance. Le processus s’arrête lorsque le modèle ne s’améliore plus.\r\nNous appliquons la fonction à notre base complète ainsi que sur notre base d’apprentissage. Les résultats sont les suivants :\r\n\r\n\r\n  \r\n\r\n\tRégression Stepwise sur échantillon d’apprentissage\r\n\t\r\n\r\nLes variables apparues parmi les 10 premières variables sont retenues. Nous retenons aussi les variables apparues dans le Stepwise sur échantillon total et qui ne sont pas apparues dans le Stepwise sur l’échantillon d’apprentissage. \r\nAu total, 14 variables sont retenues suite aux stepwises.\r\n6.3.2 Bootstrap\r\nL’idée du bootstrap est d’utiliser l’échantillon des observations pour permettre une inférence statistique plus fine. On réalise un certain nombre d’échantillons – qualifiés d’échantillon bootstrap- obtenus par tirage aléatoire d’observations de l’échantillon initial. Sur chacun des échantillons bootstrap, on estime les différents paramètres du modèle. On obtient par conséquent une suite de paramètres. Sous certaines conditions de régularité, la théorie montre que la distribution de la suite de paramètres obtenus converge vers la réelle distribution du paramètre.\r\nToutefois, l’inconvénient réside dans les importantes capacités de calcul que l’application de ces techniques exige. A chaque fois qu’un échantillon bootstrap est constitué, une étape d’estimation des paramètres doit être réalisée. On pourra voir qu’en pratique notre fonction est effectivement longue en temps d’exécution (3 jours pour 20 échantillons bootstrap).\r\nNous réalisons une régression logistique à sélection ascendante pour chacun des échantillons bootstrap.\r\n\r\n\r\nLe tableau suivant montre nos résultats pour 5 des 20 échantillons bootstrap ainsi que nombre d'occurrences et l’étape moyenne d’apparition.\r\n\r\n\r\n  \r\n\r\n\tBootstrap sur échantillon d’apprentissage\r\n\t\r\n\r\nLes variables apparues au moins 2 fois sur les 20 regression bootstrap ou dont l’étape moyenne d’apparition est inférieure à 6 sont conservées.\r\nSuite au Bootstrap, 11 variables apparaissent au moins deux fois dans les 20 échantillons bootstrap.  \r\n\r\n\r\nA l’issue de nos sélections Stepwise et Bootstrap, nous choisissons de garder 15 variables.\r\n6.4. Étapes Sélection d’un ou plusieurs modèles\r\n\r\n\r\nL’objectif est de trouver un modèle avec les variables les plus discriminantes parmis notre ensemble de 13 variables. \r\n\r\n\r\nToutes les combinaisons de 1 à 5 variables sont testées (au delà de 5, le temps de calcul devient prohibitif). Sur chacune de ces combinaisons, nous lançons une régression logistique . \r\n\r\n\r\nPour chaque combinaison, un modèle de régression logistique est construit. La comparaison de la qualité des modèles se fait avec les indicateurs suivants :\r\n* la déviance\r\n* le R²\r\n* l’aire sous la courbe ROC pour l’échantillon d’apprentissage\r\n* l’aire sous la courbe ROC du modèle construit sur l’échantillon d’apprentissage et testé sur l’échantillon de test\r\n* la moyenne des statistiques de Wald\r\n* le nombre de statistiques de Wald inférieures à 1,96²\r\n* le degré de liberté, c’est-à-dire le nombre total de modalités dans le modèle\r\nComme il est difficile de trouver une combinaison optimisant simultanément tous ces critères, nous retenons les meilleurs compromis. \r\nIci, l’objectif du test de Wald est de vérifier si chaques prédicteur apporte de l’information supplémentaire. Le prédicteur importe de l’information lorsque sa statistiques de test est inférieures à 1,96² (quantile de la loi normale centrée réduite à 0,95 et au carré). Nous décidons de retenir les modèles uniquement composé de prédicteur apportant de l’information. Nous décidons de choisir les modèles dont toutes les statistiques de Wald sont supérieures à 1,96².\r\nC’est principalement la valeur de l’aire sous la courbe ROC du modèle construit sur l’échantillon d’apprentissage et testé sur l’échantillon de test qui sera observée. Nous considérons aussi le nombre de degrés de liberté des modèles, en évitant un nombre trop élevé synonyme de manque de robustesse et un nombre trop faible synonyme de manque de finesse de la notation (plus difficile à découper en classes de cotation risque).\r\nNous retenons ensuite la variable qui est présente dans la majorité des combinaisons sélectionnées. Nous réitérons les tests précédents, en ajoutant, à chaque nouvelle combinaison de 5 variables, la variable sélectionnée à l'étape précédente et maintenant entrée dans la sélection. \r\n  \r\n  \r\n\r\n\tComparaison du meilleur modèle à 5 variables et du meilleur modèle à 6 variables\r\n\r\n\r\n\tDans notre cas, l’information ajouté avec un modèle à 6 variable n’est pas significatif par rapport au modèle à 5 variables (l’aire sous la courbe ROC du modèle construit sur l’échantillon d’apprentissage et testé sur l’échantillon de test passe de 0,765 à 0,769). Son nombre de degré de liberté est plus important que le modèle à 5 variable ( 10 degré de liberté en plus pour le modèle à 6 variables). Le modèle à 5 variables est donc préféré car il apporte suffisamment d’informations et il est plus robuste.\r\nL’aire sous la courbe ROC du modèle construit sur l’échantillon d’apprentissage et testé sur l’échantillon de test est de 0,765. Cette valeur est légèrement supérieure à l’aire sous la courbe ROC pour l’échantillon d’apprentissage qui est de 0,764. Son indice de Gini sur l’échantillon d’apprentissage vaut 0,53. Il vaut 0,52 sur la base entière.\r\nNotre modèle est ensuite lancé sur la base test qui regarde le passage en défaut des clients sur la période de juin 2018 à juin 2019. L’indice de Gini obtenu est \r\n\r\nDescription des 5 variables retenues :\r\n* NBPROD_HORSEP : Nombre de produits hors épargne \r\n  \r\n\r\n* SOL_FIN_MOIS : Solde en fin de mois de tous les produits d’épargne\r\n  \r\n\r\n* SLD_CPTEPA1_6M : Moyenne du solde moyen sur livrets sur 6 mois\r\n  \r\n\r\n* nbjdeb_cum : Cumul du nombre de jours débiteurs\r\n  \r\n\r\n* p1_csp : Code socioprofessionnel\r\n  \r\n\r\n\r\n\r\nCes tableaux permettent d’observer une valeur croissante du taux de défaut en fonction du produit hors épargne et du cumul du nombre de jours débiteurs.\r\nLe taux de défaut en fonction du nombre de produits hors épargne n’est pas linéaire. Il peut remettre en cause le découpage. Le taux de défaut des clients possédant un produit hors épargne est proche du taux de défaut des clients possédant 0 où 2 produits hors épargnes. Un regroupement des modalités pourrait être envisagé dans une future version\r\nD’autre part, le taux de défaut à une valeur décroissante en fonction de la valeur du solde sur livret et des soldes en fin de mois de tous les produits d’épargnes.\r\nDe plus nous pouvons observer que selon la catégorie socioprofessionnelle, le risque de défaut peut être multiplié par 5.\r\n\r\n\r\nDans notre nouveau modèle, seule la valeur du solde sur livret qui est présente dans les deux modèles actuels se retrouve dans le modèle.\r\n6.5. Résultat\r\nLe modèle de régression logistique est calculé sur les variables précédemment définies par la formule suivante : \r\nLOGIT                 =             - 5,67735               \r\n                                     - 1,37607                     si           NBPROD_HORSEP  ≤  0 ou non renseignée\r\n                                    +0                          si           0 < NBPROD_HORSEP ≤  1\r\n                                   + 0,18250                    si           1 < NBPROD_HORSEP ≤  2\r\n                                    + 0,39927                   si           2 < NBPROD_HORSEP ≤  3\r\n                + 0,76042                  si           3 < NBPROD_HORSEP  \r\n                                     - 1,24787                    si           SOL_FIN_MOIS  < 64,1\r\n                                    +  0,99118                   si           64,1 < SOL_FIN_MOIS ≤  803,16\r\n                                   + 0,55623                    si           803,16 < SOL_FIN_MOIS ≤  4054,34\r\n                                    + 0,33262                  si           4054,34 < SOL_FIN_MOIS ≤  11382,19\r\n                                    + 0                          si           11382,19 < SOL_FIN_MOIS ≤  25047,69\r\n                - 0,21347                  si           25047,69 < SOL_FIN_MOIS \r\n                                     + 0,79870                     si           nbjdeb_cum > 0 ou manquant\r\n                                    + 0                          si        nbjdeb_cum ≤  0\r\n                                      + 0,77437                    si           SLD_CPTEPA1_6M  < 404,76\r\n                                    +  0,34317                  si           404,76 < SLD_CPTEPA1_6M ≤  4078,82\r\n                                   + 0,05398                    si           4078,82 < SLD_CPTEPA1_6M ≤  14595,24\r\n                                   + 0                    si           14595,24 < SLD_CPTEPA1_6M\r\n                                    + 0,15753                si           SLD_CPTEPA1_6M manquant\r\n                                    + 0,39780                    si           p1_csp   ∈ {21,23,64,68,65,81,67}\r\n                                    + 0,21288                    si           p1_csp   ∈ {22,35,55,69,83,56,60}\r\n                                   - 0,58768                    si           p1_csp   ∈ {33,86,34,80,38,84}\r\n                                    - 0,41803                 si           p1_csp   ∈ {37,45,47,54,52}\r\n                                    + 0,08307                    si           p1_csp   ∈ {62,85,63}\r\n                                    + 0                           si           p1_csp   ∈ {11,43,31,48,12,13,46,53}\r\n                                    - 0,67290                   si           p1_csp   ∈ {42,44,70} ou manquant\r\n7. Découpage du score\r\n\r\n\r\nLe score est obtenu sur la base test qui regarde le passage en défaut des clients sur la période de juin 2018 à juin 2019. C’est sur cette base que le découpage du score est effectué.\r\nCette étape a pour but de découper le score obtenu précédemment, qui est une variable continue comprise entre 0 et 1 (probabilité), en 9 classes de cotation (A+, A-, B+, B-, C+, C-, D+, D-, E+) associées à des taux de défaut croissants (de A+ à E+). La valeur des bornes (taux de défaut) des classes de cotation sont des valeurs préexistante.\r\nNotre objectif va être de regrouper nos clients dans chacune des classes de cotation. Pour ce faire, nous calculons dans un premier temps les centiles du score. Puis grâce à l’analyse de ces centiles via des tableaux croisés et des représentations graphiques, nous essayons de former des regroupement de centiles homogène et terme de score qui correspondent aux classes de cotation. Cela revient à essayer d’obtenir des classes de cotation avec des taux de défaut de faible écart-type à l’intérieur de chacune des classes mais différents entre les classes. Ce regroupement se fait manuellement. Le principale critère étant que le taux de défaut du centile médian pour chaque regroupement corresponde avec la valeur du taux de défaut moyen de la classe de cotation.\r\n\r\n\r\n9. Conclusion\r\n\r\n\r\nLe nouveau modèle prédictif des clients Épargnants purs et des clients Compte Créditeur sans prêts possède un indice de Gini de 0,49 sur la base de test pour la période entre juin 2018 et juin 2019. Cet indice de Gini est faible mais il s’explique par une insuffisances en terme de volumétries de défauts rendant une prédiction difficile. Néanmoins, le nouveau modèle obtient une meilleur performance que les deux modèles précédents. Le modèle actuel Épargnants Purs possède un indice de Gini égal à 0,21. Le modèle actuel Compte Créditeur sans prêt possède un indice de Gini égal à 0,51. Le nouveau modèle améliore donc fortement la performance pour le segment Épargnants Purs et légèrement pour le segment Compte Créditeur sans prêt.\r\nLe nouveau modèle sera donc préféré aux deux modèles actuels.\r\nCependant, le nouveau modèle peut encore être amélioré en effectuant un meilleur découpage des variables. Un redécoupage plus large apporterai plus de robustesse et pourrait amener à une meilleure prédiction.\r\n\r\n\r\n8. Compétences développées\r\n\r\n\r\nCe stage m’a permis de développer plusieurs compétences :\r\n\r\n\r\nCapacité de communication\r\n\r\n\r\nLa réalisation de mon stage a nécessité la communication avec les membres de l’équipe du service Modélisation des risques. Toutes les semaines, une présentation au reste de l’équipe de Modélisation SNI a lieu. Cette réunion permet donc de présenter les projets sur lesquels on travaille au reste de l’équipe. C’est un moment d’échange où tout le monde essaye d’aider dans les différents projets.  \r\nPour communiquer mes résultats il m’a aussi fallu les rendre compréhensible pour les autres membres de l’équipe à l’aide de graphes bien choisis.\r\n\r\n\r\nCapacité à se familiariser à un nouvel environnement\r\n\r\n\r\nLe monde bancaire était un monde qui m’était inconnu avant de faire mon stage. Il m’a donc fallu apprendre beaucoup de nouvelle notions en peu de temps afin de comprendre les données avec lesquels j’ai travaillé. Cela m’a permis de mieux comprendre les problématiques et d’analyser les résultats.\r\n\r\n\r\nCapacité de rigueur \r\n\r\n\r\nMes travaux effectués seront utilisés dans des projets futurs. Mon travail devait donc être soigneusement décrit afin que n’importe quelle personne puisse comprendre ce que j’ai effectué lors de mon stage. Cela m’a appris à structurer mon travail.\r\n\r\n\r\n10. Résumé\r\n\r\n\r\nExigence réglementaire de la BCE dans le cadre des accords Bâlois \r\nLe Crédit Mutuel Arkéa doit\r\n\r\n\r\nDans le cadre de ma formation de 4ème année Génie Mathématiques à l’INSA de Rennes, j’ai effectué un stage d’une durée de 2 mois au siège du Crédit Mutuel Arkéa, à Brest. J’ai intégré le département Modélisation Statistique des risques au sein du service “Modélisation Système de Notation Interne”. Le département est chargé d’évaluer le risque de crédit des clients en fonction de leur profil, le risque opérationnel ainsi que le calcul de l'exigence en fonds propres associée aux risques pris par la banque. A cela, s'ajoutent différentes études statistiques pour la Direction en raison de nombreux et nouveaux projets impliquants pour le Groupe.\r\nMa mission a consisté à créer un nouveau modèle statistique permettant d’obtenir une meilleure prédiction du défaut des clients épargnants purs et des clients avec compte créditeur et sans crédit.\r\n\r\n\r\n11. Annexe\r\n\r\n\r\nRépartition du taux de défaut de chaques modalités pour chaques variables",
      "metadata": {
        "drive_file_id": "1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0",
        "name": "Rapport de Stage Youn Jéhanno",
        "mimeType": "application/vnd.google-apps.document",
        "modifiedTime": "2021-06-10T13:23:55.779Z",
        "webViewLink": "https://docs.google.com/document/d/1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z"
      }
    },
    {
      "source": "gdrive",
      "uri": "gdrive://1chkov0A3gKCQGhUwQd-gOlSYfwfyDZISBGRdLqPEQ48",
      "title": "Modélisation Statistique Risque",
      "mime": "application/vnd.google-apps.presentation",
      "content_hash": "85f763b97b72aee482148446ca1e726608dc8eb06300d336793b527571ce5a4e",
      "raw_text": "Création d’un modèle \nstatistique de scoring pour \nla modélisation du risque de \ncrédit des particuliers \nYoun Jéhanno, Mars 2020\n1\nÉtapes de la modélisation\nAnalyse de la qualité des données\nAnalyse de la qualité dDéveloppement des modèles\nes donnéesDéveloppement des modèles\nContexte\n● Contexte réglementaire (BCE, DR, équipe \nstatistique)\n● Problématique actuelle\nConstitution de la base de données\nAnalyse de la qualité des données\nDéveloppement des modèles\nDécoupage du score\n● Déﬁnition de la variable cible\n● Déﬁnition du périmètre\n● Construction de la base\n● Préparation des données\n● Exploration des données\n● Redécoupage des variables\n● Recodage des données manquantes\n● Sélection des variables discriminantes (ACP/ V \nde Cramer)\n● Régression logistique\n● Sélection du modèle\n● Découpage du score\n● Conclusion\n2\nContexte\n■ Stage de 2 mois réalisé dans le Service Modélisation SNI, au sein de la Direction des Risques\n■ Modèles statistiques : exigence réglementaire de la BCE dans le cadre des accords Bâlois (une \nmaîtrise complète des risques bancaires)\n■ Dans un contexte de volonté d’indépendance du Groupe Arkéa, l'objectif est de refondre \nl’ensemble des modèles du système de notation interne\n■ Problématique actuelle\n● Particuliers découpés en 11 segments \n(algorithmes spécifiques ou cotation forfaitaire)\n● Performance insuffisante des modèles\nde prévision des risques de crédit des \nParticuliers sur les segments 10 et 11\n■ Objectif du stage \n● Créer un nouveau modèle statistique regroupant les \nsegments 10 et 11 et permettant d’obtenir une \nmeilleure prédiction du risque \nSegments étudiés 3\nÉtapes de la modélisation\nAnalyse de la qualité des données\nAnalyse de la qualité dDéveloppement des modèles\nes donnéesDéveloppement des modèles\nContexte\n● Contexte réglementaire (BCE, réglementaire, \nDR, équipe statistique)\n● Problématique actuelle\nConstitution de la base de données\nAnalyse de la qualité des données\nDéveloppement des modèles\nDécoupage du score\n● Déﬁnition de la variable cible\n● Déﬁnition du périmètre\n● Construction de la base\n● Préparation des données\n● Exploration des données\n● Redécoupage des variables\n● Recodage des données manquantes\n● Sélection des variables discriminantes (ACP/ V \nde Cramer)\n● Régression logistique\n● Sélection du modèle\n● Découpage du score\n● Conclusion\n4\nvariable cible\n■ Variable à prédire : risque (critère réglementaire), identifié par les événements risques suivants\n● Créances douteuses  (présentant un risque probable ou certain de non recouvrement total ou partiel d’une créance. \nEx :  impayé depuis 90j dans le cadre des particuliers)\n● Contentieux (reflet de la situation d’un client dont la situation critique nécessite l’intervention de la maîtrise des \nrisques ou des services juridiques du Crédit Mutuel Arkéa)\n● Incidents de paiement, Surendettement, Interdit judiciaire, Liquidation judiciaire, etc …\n■ En terme informatique, la variable cible vaut :\n● 1 : si survenance d’un des risques ci-dessus dans les 12 mois.\n● 0 : sinon\n■ Les clients non sains en début de période sont exclus de la base de modélisation\nObjectif de l’étude : mesurer la probabilité de survenance d’un risque au cours des 12 prochains \nmois\n5\nConstitution de la base de données\n■ Périmètre défini pour l’étude :\n● Entités : CMB, CMSO, Fortuneo\n■ Dates de la base de modélisation\n● Base de décembre 2015 avec risque observé sur l’année 2016\n● Base de juin 2017 avec risque observé jusqu’à juin 2018 \n■ Données exploitées :\n● Données des clients présents dans l’entrepôt de données Arkéa \n(données socio-économiques, données de fonctionnement de compte, risques…)\nDéﬁnition du périmètre\n■ Dates de la base de test\n● Base de juin 2018 avec risque \nobservé jusqu’à juin 2019 \nn ~ 1 600 000 observations\n312 variables\n6\nConstitution de la base de données\n■ R : package interne ModelSNI\n● Permet d’harmoniser l’utilisation des différentes fonctions nécessaires à la modélisation sous R\n■ Maquette Excel : mise en forme des résultats de modélisation pour les étapes suivantes :\n● Exploration des données\n● Recodage des données manquantes\n● Sélection des variables\n● Construction du modèle\n● Calibrage du modèle\nOutils utilisés\n7\nÉtapes de la modélisation\nAnalyse de la qualité des données\nAnalyse de la qualité dDéveloppement des modèles\nes donnéesDéveloppement des modèles\nContexte\n● Contexte réglementaire (BCE, réglementaire, \nDR, équipe statistique)\n● Problématique actuelle\nConstitution de la base de données\nAnalyse de la qualité des données\nDéveloppement des modèles\nDécoupage du score\n● Déﬁnition de la variable cible\n● Déﬁnition du périmètre\n● Construction de la base\n● Préparation des données\n● Exploration des données\n● Redécoupage des variables\n● Recodage des données manquantes\n● Sélection des variables discriminantes (ACP/ V \nde Cramer)\n● Régression logistique\n● Sélection du modèle\n● Découpage du score\n● Conclusion\n8\nAnalyse de la qualité des données\n■ Recodage de certaines variables\n● ex : Les données manquantes (999999 ou 0) sont recodées à NA\n■ Construction de nouvelles variables \n● ex : Moyenne sur 6 mois des données de fonctionnement de compte afin de capter des tendances sur \nplusieurs mois et éviter ainsi une trop grande sensibilité aux évènements exceptionnels et aux \nvariations saisonnières\n■ Problématique rencontrée\n● Individus absents dans certaines bases de données.\nExemple : - Épargnants purs ne disposant pas de comptes courants\n- Clients récents n’ayant pas d’informations bancaire sur les premiers \nmois entraînant des valeurs manquantes lors de la création de moyennes.\nPréparation des données\nn ~ 1 600 000 observations\n312 variables\n9\nAnalyse de la qualité des données\n■ Objectif de l’exploration :\n● Repérer les variables constantes, manquantes, redondantes\n■ Maquette d’exploration des données quantitatives par entité\nExploration des données\n■ Maquette d’exploration des données qualitatives par entité\n■ Résultats : L’exploration a permis de supprimer 194 variables \nn ~ 1 600 000 observations\n118 variables\n10\nAnalyse de la qualité des données\n■ Objectif :\n● Répartir la population en groupes homogènes, avec des variables discriminantes plus performantes.\n■ Algorithme CHAID\n● Méthode d’arbre de décision qui permet de discrétiser, en 2 classes ou plus, les variables en les \ndécoupant en déciles puis en utilisant le critère du χ2 pour regrouper les classes les plus proches en \nterme de taux de risque\n● Variables continues : discrétisation\n➢ Ex : Nombre produit hors épargne : [-Inf,0] ; (0,1] ; (1,2] ; (2,3] ; (3, Inf]\n● Variables qualitatives : regroupement des modalités\n➢ Ex : Regroupement de certaines catégories socioprofessionnelles\n■ Résultat : \n● 117 variables sont créées\n● Si une variable a été redécoupée, on ne conserve pas la variable d’origine\n \nRedécoupage des variables (CHAID) \n11\nAnalyse de la qualité des données\n■ Objectif :\n● Exploiter les données manquantes\n■ Regroupement modalité valeurs manquantes, \n2 cas : \n● Création modalité valeurs manquantes\n(proportion > 10%)\n● Affectation à une modalité \n(proportion < 10%)\n● Affectation logique bancaire\n■ Résultat :\n● 60 variables conservent les valeurs manquantes comme une modalité à part entière\n● Pour 36 variables, les valeurs manquantes sont affectées à une modalité\nRecodage des données manquantes\nMaquette de recodage des valeurs manquantes\n12\nÉtapes de la modélisation\nAnalyse de la qualité des données\nAnalyse de la qualité dDéveloppement des modèles\nes donnéesDéveloppement des modèles\nContexte\n● Contexte réglementaire (BCE, réglementaire, \nDR, équipe statistique)\n● Problématique actuelle\nConstitution de la base de données\nAnalyse de la qualité des données\nDéveloppement des modèles\nDécoupage du score\n● Déﬁnition de la variable cible\n● Déﬁnition du périmètre\n● Construction de la base\n● Préparation des données\n● Exploration des données\n● Redécoupage des variables\n● Recodage des données manquantes\n● Sélection des variables discriminantes (ACP/ V \nde Cramer)\n● Régression logistique\n● Sélection du modèle\n● Découpage du score\n● Conclusion\n13\nDéveloppement des modèles \n■ Objectif\n● Diminuer le nombre de variables pour permettre la réalisation d’une régression logistique\n■ V de Cramer\n● Évalue la liaison entre les variables \ndu modèle et la variable cible\n■ ACP\n● Création de groupes de variables homogènes en \nterme de corrélation\n● Au total : 25 composantes principales\n■ Critères de sélection\n● Variable avec le plus fort V de Cramer \npour chaque composante principale\n● V de Cramer > 0,03\n● Variables modèles actuels\nSélection des variables discriminantes\n■ Résultat\n● Ces méthodes ont permis de retenir les \n28 variables les plus discriminantes \nparmi les 118 sélectionnées à l’étape \nprécédente\nn ~ 1 600 000 \nobservations\n28 variables\n14\n\nDéveloppement des modèles \n■ Objectif \n● Supprimer les variables contenant la même information\n■ Test de Corrélation \n● Corrélation > 0,80\n■ Critères de sélection\n● Variable avec le plus fort V de Cramer\n■ Résultat:\n● Cette étude permet de supprimer 7 variables et de garder 21 variables tout en \npréservant la diversité de l’information\nSélection des variables discriminantes\nTest de corrélation entre 4 variables présélectionnées\nn ~ 1 600 000 observations\n21 variables\n15\nDéveloppement des modèles \n■ Objectif :\n● La régression Stepwise consiste à ajouter itérativement des prédicteurs, afin de trouver le \nsous-ensemble de variables résultant en le modèle le plus performant\n■ Régression Stepwise\n● Base complète\n● Base d’apprentissage\n● 20 échantillons bootstrap\n■ Sélection des variables\n● Etape moyenne d’apparition inférieure à 10 \n● Apparues au moins 2 fois dans les 20 régressions\n■ Résultat :\n● La régression stepwise sur la base complète et la base d’apprentissage permet de \nsélectionner 13 variables\n● Les régression stepwise sur les 20 échantillons bootstrap font ressortir 2 nouvelles variables\nRestriction des variables discriminantes\nn ~ 1 600 000 observations\n15 variables\nBootstrap sur échantillon d’apprentissage\n16\nDéveloppement des modèles \n■ Test combinaisons de 5 variables\n● Critère de sélection\n➢ Aucune Statistique de Wald inférieures à 1,96², assure \nla performance des variables du modèle  \n➢ Aire sous la courbe ROC\n➢ Degré de liberté\n■ Test combinaisons de 5 variables avec 1 variable \nprésélectionnée \n● Évite de tester toutes les combinaisons de 6 variables trop chronophage (~6h pour 5 variables)\n● Variable présélectionnée : présente dans la majorité des combinaisons précédentes, ici, \ndecp_NBPROD_HORSEP (nombre de produits hors épargne)\n● Mêmes critères de sélection\n■ Résultat :\n● Le meilleur modèle à 6 variables n’augmente pas significativement la qualité. C’est le meilleur modèle \nà 5 variables qui est retenu. Son indice de Gini sur l’échantillon test vaut 0,53 (acceptable).\nSélection du modèle\nComparaison du meilleur modèle à 5 variables et du \nmeilleur modèle à 6 variables\n17\nDéveloppement des modèles \nVariables sélectionnées\n■ Observation\n● Les graphes des variables 1 et 2 montrent que plus le solde sur livret \net des produits d’épargne est faible et plus le client a des chances \nd’avoir une survenance de risques\n● Selon la catégorie socioprofessionnelle, le taux de survenance de \nrisques peut être multiplié par 5\nVariable 1\nVariable 2\nVariable 3\n18\n\nDéveloppement des modèles \nVariables sélectionnées\n■ Observation\n● Variable 4 : Les clients ayant au moins 1 jour débiteur sur 6 mois ont beaucoup plus de chance d’avoir une \nsurvenance de risques\n● Variable 5 : Le nombre de produit hors épargne augmente le taux de survenance de risques\n● Remarque : Le taux de survenance de risques en fonction du nombre de produits hors épargne n’est pas linéaire. \nIl peut remettre en cause le découpage qui aurait pu être large de façon à respecter la tendance croissante du \ntaux de survenance de risques\nVariable 4 \nVariable 5\n19\nÉtapes de la modélisation\nAnalyse de la qualité des données\nAnalyse de la qualité dDéveloppement des modèles\nes donnéesDéveloppement des modèles\nContexte\n● Contexte réglementaire (BCE, réglementaire, \nDR, équipe statistique)\n● Problématique actuelle\nConstitution de la base de données\nAnalyse de la qualité des données\nDéveloppement des modèles\nDécoupage du score\n● Déﬁnition de la variable cible\n● Déﬁnition du périmètre\n● Construction de la base\n● Préparation des données\n● Exploration des données\n● Redécoupage des variables\n● Recodage des données manquantes\n● Sélection des variables discriminantes (ACP/ V \nde Cramer)\n● Régression logistique\n● Sélection du modèle\n● Découpage du score\n● Conclusion\n20\nDécoupage du score\n■ Objectif :\n● Découper le score, en 9 classes de cotation associées à des taux de risque croissants (de A+ à E+)\n● Outil de comparaison des segments Particuliers\n■ Score calculé sur la base de test (Juin 2018 à Juin 2019)\n■ Etapes : \n● Découpage du score en centile\n● Regroupement et affectation des centiles aux classes de \ncotation de manière à ce que le centile médian des \nregroupements de centiles soit proche de la moyenne \nde la classe de cotation \n■ Résultat :\n● Score découpé de B+ à D-\nDécoupage du score\nMaquette découpage du score \npour 1 centile sur 10\nGrille de cotation unique \npour les particuliers\n21\nConclusion\n■ Comparaison avec les modèles actuels :\n● Indice de Gini sur période Juin 2018 à Juin 2019:\n● ⅓ Épargnants purs et ⅔ Compte Créditeur sans prêt → Amélioration globale\n■ Ouverture :\n● Découpage plus large des modalités lors du CHAID → Gagner en robustesse\n■ Plan personnel :\n● Découverte du monde de l’entreprise et bancaire\n● Capacité à s’intégrer dans un environnement nouveau\n22\nROC  de la base de test \nOral pour l’INSA\n● conseils : \n○ essayer de ne pas lire et de rajouter des commentaires à l’oral\n○ préparer des justiﬁcations des méthodes scientiﬁques\n○ lire les commentaires en bas de chaques diapositives qui aident pour l’oral (pourquoi avoir \nchoisi de faire une ACP pour des variables qualitatives)\n23",
      "metadata": {
        "drive_file_id": "1chkov0A3gKCQGhUwQd-gOlSYfwfyDZISBGRdLqPEQ48",
        "name": "Modélisation Statistique Risque",
        "mimeType": "application/vnd.google-apps.presentation",
        "modifiedTime": "2020-03-04T14:58:42.508Z",
        "webViewLink": "https://docs.google.com/presentation/d/1chkov0A3gKCQGhUwQd-gOlSYfwfyDZISBGRdLqPEQ48/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z"
      }
    },
    {
      "source": "gdrive",
      "uri": "gdrive://16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY",
      "title": "Copie de Rapport de Stage Youn Jéhanno",
      "mime": "application/vnd.google-apps.document",
      "content_hash": "c15c1f661c58b476831cc175bf52ba64f582f34f4225cedff22a2362944466fc",
      "raw_text": "﻿________________\r\n\r\nINSA Rennes\r\nMÉMOIRE DE STAGE\r\nCréation d’un modèle statistique de scoring pour la modélisation du risque de crédit des particuliers \r\n\r\n  \r\n\r\n\r\n\r\nMémoire CONFIDENTIEL\r\n\r\n\r\n4ème année Génie Mathématiques\r\nINSA Rennes\r\nNom de l’entreprise : Crédit Mutuel ARKEA\r\nResponsable de stage : Guillaume Le Coz\r\nEnseignant référent : Loïc Hervé\r\nDate du stage : 13 Janvier au 6 Mars 2020\r\n\r\n\r\n\r\n\r\n\r\n\r\n________________\r\n  \r\n________________\r\n\r\n\r\n\r\nSommaire\r\n\r\n\r\nRemerciements\r\nNote de Confidentialité\r\n1. Introduction\r\n2. Présentation de l’entreprise\r\n2.1. Groupe Arkéa\r\n2.1.1 Historique et organisation\r\n2.1.2 Quelques chiffres\r\n2.1.3 Souhait d’indépendance d’Arkéa\r\n2.1.4 Organisation de la direction des risques et du département Modélisation Statistique des Risques\r\n3. Environnement de travail\r\n3.1. Mise en situation\r\n3.1.1 Définition population observée\r\n3.1.2 Problématique actuelle\r\n3.2. Environnement de travail\r\n3.3.Description des bases de données utilisées\r\n4. Constitution de la base de données\r\n4.1. Détermination de la variable cible\r\n4.2. Exploration des données\r\n4.3. Définition du périmètre\r\n4.4. Construction de la base\r\n5. Analyse de la qualité des données\r\n5.1. Préparation des données\r\n5.2. Maquette d’exploration des données\r\n5.3. Redécoupage des variables (CHAID)\r\n5.4. Recodage des données manquantes\r\n6. Développement des modèles\r\n6.1. Sélection des variables discriminantes\r\n6.1.1. ACP\r\n6.1.2. V de Cramer\r\n6.1.3. Test de corrélation\r\n6.2. Echantillonnage\r\n6.3. Restriction des variables discriminantes - Stepwise & Bootstrap\r\n8.3.1 Forward step\r\n6.3.2 Bootstrap\r\n6.4. Sélection d’un ou plusieurs modèles\r\n6.5. Résultat\r\n6.6. Analyse des résultats\r\n7. Découpage du score\r\n7.1. Construction et validation du modèle\r\n7.1.1 Courbe ROC\r\n7.1.2 Indice de Gini\r\n8. Compétences développées\r\n\r\n\r\n________________\r\n\r\n\r\nRemerciements\r\n\r\n\r\nJe tiens à remercier toutes les personnes qui ont contribué au bon déroulement de mon stage et qui m'ont aidé lors de la rédaction de ce rapport.\r\n\r\n\r\nTout d'abord, j'adresse mes remerciements à mon maître de stage et ma responsable de service, Guillaume Le Coz et Audrey Ladan pour leur accueil et pour m’avoir apporté leur expérience du monde bancaire tout au long de ce stage. Leurs relectures fut d’une aide précieuse pour l’écriture de mon rapport de stage.\r\n\r\n\r\nJe tiens également à remercier l’ensemble du département Modélisation statistiques des risques pour leur gentillesse lorsque je leur posait des questions et leur bonne humeur lors des pauses café.\r\n\r\n\r\nNote de Confidentialité\r\nCe présent rapport ainsi que toutes les informations qu’il contient sont strictement confidentiels exclusivement au corps professoral de l’INSA de Rennes ou à une personne interne à l’équipe statistique du département modélisation statistique des risques.\r\nLes chiffres présents dans ce rapport ont été volontairement faussés toujours dans cet objectif de confidentialité. Ils restent toutefois cohérents.\r\nMerci de bien vouloir respecter la confidentialité de ce document.\r\n________________\r\n\r\n\r\n\r\n\r\n1. Introduction\r\n\r\n\r\nDans le cadre de ma formation de 4ème année Génie Mathématiques à l’INSA de Rennes, j’ai effectué un stage d’une durée de 2 mois au siège du Crédit Mutuel Arkéa, à Brest. J’ai intégré le département Modélisation Statistique des risques au sein du service “Modélisation Système de Notation Interne”. Le département est chargé d’évaluer le risque de crédit des clients en fonction de leur profil, le risque opérationnel ainsi que le calcul de l'exigence en fonds propres associée aux risques pris par la banque. A cela, s'ajoutent différentes études statistiques pour la Direction en raison de nombreux et nouveaux projets impliquants pour le Groupe.\r\nMa mission a consisté à créer un nouveau modèle statistique permettant d’obtenir une meilleure prédiction de la survenance de risques des clients épargnants purs et des clients avec compte créditeur et sans crédit.\r\n2. Présentation de l’entreprise\r\n2.1. Groupe Arkéa\r\n2.1.1 Historique et organisation\r\nLe Groupe ARKÉA est un groupe coopératif et mutualiste qui comprend plus de 20 filiales spécialisées qui couvrent tous les métiers de la banque, de la finance et de l'assurance, comme Fortuneo (banque en ligne), Suravenir Assurances, Federal Finance (gestion d'actifs pour compte de tiers), et Financo (crédit à la consommation). Son siège social est implanté au Relecq-Kerhuon dans le Finistère.\r\nIl est né de l'alliance des fédérations de Crédit Mutuel de Bretagne et du Sud-Ouest (CMB, CMSO). C’est le deuxième pôle régional du Crédit Mutuel. Ancré sur ses territoires et comptant près de 10 000 salariés. Il regroupe un grand nombre de fonctions supports et de métiers d'expertise (Finance, Organisation, Audit, Marketing, Juridique,...).\r\n2.1.2 Quelques chiffres\r\nLe Groupe ARKÉA en 2020 représente :\r\n❏     4,5 millions de clients\r\n❏     10 500 salariés\r\n❏     111.2 millions d'euros encours d'épargne (épargne financière, épargne assurance, dépôts).\r\n❏     6.7 milliards d'euros de fonds propres\r\n❏     437 millions d'euros de résultat net\r\n❏     135 milliards d’euros de bilan comptable \r\n2.1.3 Contexte d’indépendance\r\nDepuis 2015, le groupe Arkéa souhaite devenir indépendant du Crédit Mutuel. Cette désaffiliation est prévue pour janvier 2021. \r\nDans ce cadre, il a été engagé des travaux préparatoires de refonte complète des modèles statistiques (aujourd’hui réalisés sur l’ensemble du Crédit Mutuel) sur la population Arkéa.\r\nL’objectif de ces travaux est de prouver à la Banque Centrale Européenne qu’elle est capable de fonctionner indépendamment.\r\n\r\n\r\nCe stage s’inscrit dans ce contexte.\r\n2.1.4 Organisation de la direction des risques et du département Modélisation Statistique des Risques\r\nLe Groupe ARKÉA est divisé en différentes directions avec leurs activités et objectifs propres. L’une des directions, la direction des risques, est celle dans laquelle j’ai effectué mon stage. Elle est composée de 130 collaborateurs dont les principales missions sont la mise en place et le pilotage du dispositif global de gestion des risques du groupe.\r\nLe département Modélisation Statistique des Risques est composé d’une dizaine de statisticiens/data scientists. Les activités principales des différents services au sein du département sont les suivants :\r\n❏        Service Gestion de la donnée et modélisation du risque opérationnel\r\n●           Construire, suivre et maintenir les bases de données clients, métiers, IT,…  servant aux études et à la modélisation statistiques du Groupe et des filiales.\r\n●          Contribuer à la cartographie globale des risques du Groupe et en déduire un état de reporting à des fins de communication aux organes dirigeants.\r\n●     Elaborer et quantifier des modèles de risques opérationnels dans le cadre du calcul de l’exigence en fonds propres.\r\n●         Détecter et contribuer à la prévention d’incidents opérationnels via la création de modèles de comportement de détection de fraude externe.\r\n❏        Service « Modélisation Système de Notation Interne »\r\n●     Créer et assurer la maintenance des modèles de notation interne statistiques (pour les fédérations ou les filiales) en réalisant des études statistiques de comportement des clients en termes de risques de crédit.\r\n●           Rédiger des cahiers des charges de cotation à destination de l'informatique et assurer la coordination de la mise en place des nouveaux algorithmes de notation (recette, assistance de second niveau, suivi des modèles etc…), en lien avec l'informatique et toutes les structures du Groupe impliquées dans la maîtrise des risques et dans la qualité des données en amont de la cotation (direction des engagements, filiales …).\r\n❏     Service « Paramètres et provisions du risque de crédit »\r\n●         Procéder statistiquement aux calculs de paramètres selon les normes bâloise ou comptable IFRS 9 (Probabilité de défaut - PD, Pertes en cas de défaut - LGD et Facteur de conversion d’encours hors bilan en bilan - CCF), afin de permettre les calculs de l’exigence de fonds propres au titre du risque de crédit et du niveau de provisionnement du groupe.\r\n●          Accompagner le process par le suivi et la maintenance des modèles de protection (bâlois et provisionnement), en lien avec l'informatique et toutes les structures du Groupe impliquées dans la maîtrise des risques et dans la qualité des données en amont des systèmes de détermination des paramètres (direction des engagements, filiales …).\r\n3. Environnement de travail\r\n3.1. Mise en situation\r\n3.1.1 Définition population observée\r\nActuellement, les clients particuliers (personnes physiques) sont répartis en 11 segments distincts avec l’ordre des priorisation ci-dessous:\r\n  \r\n\r\nLes 6 segments 19,18,17 et 15 ont des cotations forfaitaires (même note attribuée à tous les membres de chaque segment) basées sur une étude des taux de survenance de risques observés.\r\nLes 5 autres segments possèdent un algorithme statistiques spécifique qui attribue une cotation à ses membres. L’étude se concentre sur les segments 10 et 11.\r\nÉpargnant pur : clients sans prêt, sans risque en cours, qui n’est pas nouveau client et qui n’a    pas de compte courant.\r\nCompte courant créditeur sans prêt : clients sans prêt, sans risque en cours et qui n’est pas nouveaux client avec au moins un compte courant et avec au maximum un jour de débit sur les 6 mois d’observation.\r\n3.1.2 Problématique actuelle\r\nLa segmentation actuelle des populations pourrait être améliorée en regroupant les deux segments, « Épargnants purs » et « CC créditeur », qui présentent un faible nombre de survenance de risques et des similarités d’un point de vue comportement et survenance de risques.\r\nActuellement, le modèle Épargnants Purs présente des insuffisances en terme de performance lié à ses faibles volumétries.\r\n\r\nMa mission a été de regrouper ces deux types de clients en un seul segment afin de voir si une meilleure prédiction globale serait réalisable.\r\n3.2. Environnement de travail\r\n\r\n\r\nLes bases de données utilisées sont issues de la BNS (Base Nationale Statistique) et c’est le logiciel R qui a servi à construire le modèle.\r\n\r\n\r\nL’étude des données a principalement été réalisée à partir de maquettes excel.\r\n\r\n\r\nPour ce projet, la majorité des fonctions utilisées sont issues d’un package interne ModelSNI regroupant un ensemble de syntaxes pour la construction d’un modèle de cotation.\r\n3.3.Description des bases de données utilisées\r\n\r\n\r\nLe modèle a été construit à partir des 8 bases de données suivantes :\r\n  \r\n\r\n4. Constitution de la base de données\r\n4.1. Détermination de la variable cible\r\n\r\n\r\nLa variable cible correspond à la survenance de risque (appelée top_def). Elle vaut :\r\n\r\n\r\n* 1, si le client était sain en début de période d’observation de la survenance de risques et a eut une survenance de risques au cours de la période (qui dure 12 mois);\r\n* 0, si le client était sain en début de période et est resté sain pendant toute la période.\r\n4.2. Exploration des données\r\n\r\n\r\nAvant de commencer à manipuler les bases de données, il est important de se familiariser avec les données et de comprendre les variables. Un fichier Excel décrit l’ensemble des variables utilisées et facilite la compréhension de la base de données. Cette phase d’exploration va aussi permettre de repérer d'éventuelles valeurs aberrantes ainsi que des valeurs manquantes\r\n4.3. Définition du périmètre\r\n\r\n\r\nIl est important de définir le périmètre de l’étude pour travailler avec les bonnes données.\r\n\r\n\r\n* Périmètre définit pour l’étude : \r\n   * CMB (Crédit Mutuel de Bretagne), CMSO (Crédit Mutuel du Sud Ouest) et Fortunéo\r\n   * Segment 10 et 11, Épargnant pur et Compte courant créditeur sans prêt\r\n\r\n\r\n* Horizon de prédiction : 1 an\r\n\r\n\r\n* Dates de la base de modélisation :\r\n   * Base de modélisation : concaténation de 2 bases ( Ce choix ne garantit pas l’indépendance de nos observations mais permet d’avoir une base de données plus importante et évite les effets de saisonnalité)\r\n      * Base à décembre 2015 avec avec survenance de risques observé sur l’année 2016\r\n      * Base à juin 2017 avec avec survenance de risques observé entre juin 2016 et juin 2017\r\n\r\n\r\n\r\n\r\n  \r\n\r\n\r\n\r\n4.4. Construction de la base\r\n\r\n\r\nLa base est construite à partir des bases de décembre 2015 et de juin 2017, elles mêmes construites grâce à la jointure de l’ensemble des bases décrites dans le tableaux précédent.\r\n\r\n\r\nCette partie est très importante et chronophage car il faut que la base soit la plus complète et la plus fiable possible afin de pouvoir obtenir des résultat plus satisfaisants. \r\nIl a fallu faire face à de nombreux problèmes comme des individus manquants dans certaines bases. Les bases étant de plus très volumineuses, il est d’autant plus difficiles de les trouver.\r\nLa vérification des bases de données à chaques étapes de conception est donc très importante.\r\n\r\n\r\nLa base construite est alors constituée de 1 600 000 observations et de 312 variables.\r\n\r\n\r\n5. Analyse de la qualité des données\r\n5.1. Préparation des données\r\n\r\n\r\nLa préparation des données est composée de deux parties :\r\n\r\n\r\n* Recodage de certaines variables\r\n   * ex : Les données manquantes en (9999999 ou 0) sont recodées en NA\r\n       Les dates < 1901 ou égales à 0 sont recodées à NA car il s’agit d’une\r\n      codification liées aux valeurs manquantes\r\n       Les variables avec >100 modalités sont recodées afin de pouvoir appliquer les \r\n      méthodes statistiques de sélection de variables\r\n       Les valeurs avec des signes aberrants afin de s’assurer une bonne qualité des \r\n      données\r\n\r\n\r\n* Construction de nouvelles variables (moyennes, région) :\r\n   * 26 variables calculant la moyenne sur 6 mois de variables relatives aux comportements bancaires\r\n   * 2 variables indiquant la région d’habitation des tiers\r\n   * 7 variables calculant des sommes de soldes sur différents comptes épargnes\r\n\r\n\r\nLa construction de nouvelles variables a plusieurs avantages :\r\n\r\n\r\n* Fait entrer dans le modèle plusieurs variables en évitant la présence de variables corrélées\r\n* Limite le nombre de variables dans le modèle\r\n* Prend en compte un historique du comportement du client\r\n5.2. Maquette d’exploration des données \r\n\r\n\r\nUne fois les nouvelles variables créées, il reste de nombreuses variables qui n’apporte aucune information pour notre étude. Il faut donc les identifier et les supprimer dans un soucis de clarté. Pour cela, nous utilisons des maquettes d’exploration des données par Entité (1 :CMB, 3: CMSO, 21: Fortuneo). Cette étude par entité permet de s’assurer une homogénéité de la donnée entre les différents périmètres étudiés :\r\n\r\n\r\n* Données quantitatives : les valeurs étudiées sont les suivantes :\r\n   * nombre de valeurs manquantes\r\n   * variable constante ou non\r\n   * moyenne\r\n   * ecart-type\r\n   * minimum\r\n   * max\r\n   * quantiles 0.01, 0.05, 0.10, 0.20, 0.25,0.30, 0.40,0.50, 0.60, 0.70, 0.75,0.80, 0.90, 0.95, 0.99\r\n   * le pourcentage de 0.\r\n  \r\n                     \r\n\tMaquette d’exploration des données des variables quantitatives pour LE CMB (1)\r\n\t\r\n\r\nExemple :\r\n\r\n\r\nLes variables ddbu_dero et dfin_dero sont tout le temps égales à 0. Elles sont constantes et ne sont pas gardées dans le modèle.\r\n\r\n\r\nLa variables R_EPAMT_EPATOT possède des centiles chez Fortunéo au moins 10 fois supérieurs aux centiles du CMB et du CMSO. La variable est tout de même gardée car ses informations sont intéressantes.\r\n\r\n\r\n\r\n\r\n* Données qualitatives : les valeurs étudiées sont les suivantes :\r\n   * nombre de valeurs manquantes\r\n   * variable constante ou non\r\n   * effectif\r\n   * fréquence\r\n\r\n\r\n  \r\n\r\n\tMaquette d’exploration des données des variables qualitatives\r\n\t\r\n\r\n  \r\n\r\n\tLégende des alertes des variables qualitatives\r\n\t\r\n\r\nA partir de toutes ces informations, il faut déterminer quelles variables sont utiles et lesquelles ne le sont pas. \r\nLes variables constantes ou uniquement constituées de valeurs manquantes ne sont pas gardées.  Nous supprimons 173 variables.\r\n\r\n\r\nPour les autres variables, la décision revient à l’appréciation du statisticien. En fonction des expériences des modèles précédents et de notre jugement, 35 variables sont supprimées.\r\n\r\n\r\nSuite à l’exploration, nous supprimons 208 variables. Notre base contient alors 118 variables. \r\n5.3. Redécoupage des variables (CHAID)\r\nL’algorithme CHAID regroupe pour chaque variable explicative ses modalités les moins liées à la variable à expliquer, c’est-à-dire celles dont les taux de survenance de risques sont les plus voisins au vu du test du khi-2. Il le fait de proche en proche par paires de modalités puis de regroupements de modalités, jusqu’à ce que ne subsistent plus que des regroupements de modalités aux taux de survenance de risques suffisamment distincts.\r\n\r\n\r\nCe redécoupage est utile car nous manipulons des bases de données contenant de nombreuses observations. Cela va permettre de répartir la population en groupes homogènes, avec des variables discriminantes plus performantes. \r\n\r\n\r\n* Transformation des variables continues : discrétisation\r\n\r\n\r\n* Transformation des variables qualitatives : regroupement de modalités\r\n\r\n\r\nLe découpage ne doit pas être trop fin de manière à être le plus discriminant possible. Dans notre cas, pour certaine variables, on observe une tendance linéaire du taux de survenance de risques en fonction des valeurs de la variable. Il est donc important que notre découpage suive cette tendance. Pour cela il faut trouver le bon seuil pour notre découpage car un découpage trop fin ne respectera pas la règle mais un découpage trop large fera perdre de l’information.\r\n\r\n\r\nLa base est maintenant constitué uniquement de données qualitatives. Il reste à corriger le problème des valeurs manquantes.\r\n5.4. Recodage des données manquantes\r\n\r\n\r\nLes valeurs manquantes de chaque variable sont regroupées dans une modalité puis leur taux de survenance de risques est analysé. Selon les volumes observés, les données manquantes sont affectées à la catégorie la plus proche (en terme de taux de survenance de risques ) ou bien une modalité spécifique aux données manquantes est créée.\r\n\r\n\r\nLorsque le taux de valeurs manquantes est supérieur à 10% pour une variable, on considère les données manquantes comme une modalité spécifique.\r\nDans le cas contraire, celles-ci sont regroupées avec la modalité ayant le taux de survenance de risques le plus proche.\r\n\r\n\r\n\r\n\r\n\r\n\r\n  \r\n\r\n\tMaquette du recodage des valeurs manquantes\r\n\t\r\n\r\n\r\n\r\nDans cet exemple, les données manquantes de la variables decp_EXI_AVOIR2 représentent moins de 10 % des observations. Elles sont donc regroupées avec la modalité [-INF,0] qui est la modalité possédant le taux de survenance de risques le plus proche des valeurs manquantes.\r\n\r\n\r\nUne analyse “à dire d’expert” est également réalisée afin de s’assurer de la cohérence du découpage et de l’affectation des valeurs manquantes. En effet, dans certains cas où le nombre de valeurs manquantes est faible, le taux de survenance de risques observé n’est pas significatif. L’affectation à la modalité ayant le taux de survenance de risques le plus proche n’est donc pas forcément la meilleure. Il convient donc de l’affecter à la modalité répondant le mieux à la logique bancaire.\r\n\r\n\r\nCette étape permet aussi de vérifier les découpages qui ont été effectué par l’algorithme CHAID. Si une modalité contient trop peu d’individus, elle est regroupée avec la modalité la plus proche en terme de valeur de la variable.\r\n\r\n\r\n6. Développement des modèles\r\n6.1. Sélection des variables discriminantes\r\n6.1.1. ACP \r\n\r\n\r\nL’ACP permet d’étudier et de visualiser des corrélations entre les variables, afin d’éventuellement limiter le nombre de variables à mesurer par la suite. L’objectif est d’obtenir des variables non corrélées qui sont des combinaisons linéaires des variables de départ, afin d’utiliser ces variables dans les méthodes de modélisation utilisées par la suite telles que la régression logistique ou l’analyse discriminante. Elle permet d’identifier des groupes homogènes d’observations, ou au contraire des observations atypiques.\r\n\r\n\r\nNous obtenons alors 25 composantes principales. A chaque composante principale, nous affectons les variables ayant un coefficient de corrélation supérieur à 0,7.\r\n6.1.2. V de Cramer\r\nPour estimer si les modalités sont probablement indépendantes ou au contraire liées, on a coutume de procéder à un test du V de Cramer qui reste stable si l’on augmente la taille de l’échantillon dans les mêmes proportions inter-modalités. Plus le V de Cramer d’une variable est proche de zéro, plus il y a indépendance entre les deux variables étudiées. \r\nNous réalisons un test du V de Cramer entre la variable cible top_def et toutes les variables du modèle :\r\n  \r\n\r\nLa variable decp_nbj_depaut correspond à la moyenne au cours des 6 derniers mois du nombre mensuel de jours en dépassement de l’autorisation de découvert sur au moins un compte courant . Elle possède un V de Cramer très élevé car les individus ayant un dépassement d’autorisation ont plus de chance d’avoir une survenance de risques.\r\nLa variables decp_solde_cpt_ep2_PEP correspond au solde moyen mensuel du mois 2 sur le Plan Epargne Populaire de la personne. Elle possède un V de Cramer faible car la variable prend ses valeurs sur un mois. Ce temps n’est pas assez long pour avoir des valeurs significatives.\r\nOn retient :\r\n* Pour chaque composante principale de l’ACP, la ou les variables ayant le plus fort V de Cramer.\r\n* On ne retient les variables ayant un V de Cramer < 0,03. \r\n* Les moyennes, ratios ou autres variables combinées sont privilégiées par rapport aux variables simples (ex: moyenne sur 6 mois plutôt que valeur sur un mois)\r\n* Les variables sélectionnées dans l’ancien modèle. Ces variables sont censées être les plus explicatives pour les segments clients épargnants purs ainsi que les clients avec compte créditeur et sans crédit.\r\nVariables sélectionnées pour la modélisation : 28\r\nL’idée est de ne pas se limiter aux variables ayant un V de Cramer élevé, mais de sélectionner des variables moins discriminantes mais qui apportent une information supplémentaire.\r\n6.1.3. Test de corrélation\r\nOn réalise un test du V de Cramer entre les 28 variables sélectionnées pour la modélisation pour observer les corrélations. Les variables ayant une corrélation trop élevée ( >0,70) sont retirées si elles possèdent un V de Cramer inférieur à la variable avec laquelle elles sont corrélées. En cas de V de Cramer très proches, les variables préexistantes sont préférées car plus fiables. Celles-ci font l’objet de suivis périodiques.\r\n  \r\n\r\n\tCorrélation entre les variables PCS, decp_nbj_depaut, decp_NBJ_DEPAUT_6M et decp_EXI_AVOIR2\r\n\t\r\n\r\nLes variables decp_nbj_depaut, decp_NBJ_DEPAUT_6M et decp_EXI_AVOIR2 ont des corrélations supérieures à 0,70. Cela signifie qu’elles possèdent globalement la même information. Seul decp_NBJ_DEPAUT_6M est gardées car c’est la variable ayant le meilleur V de Cramer.\r\nLa variable PCS est gardée car faiblement corrélée à toutes les autres. Sa corrélation est faible par rapport aux autres variables car la PCS correspond à la catégorie socioprofessionnelle du foyer et les autres variables font référence à des données bancaires. Cela apporte donc une information complémentaire.\r\nVariables sélectionnées pour la modélisation : 21\r\n6.2. Echantillonnage\r\nDécoupage de la base de modélisation en deux échantillons :\r\n\r\n\r\n* Échantillon d’apprentissage : 70% de la base de modélisation. Sert à construire les modèles\r\n\r\n\r\n* Échantillon test : 30% de la base de modélisation. Sert à valider les modèles.\r\n\r\n\r\n  \r\n\r\n\r\n\r\nChaque échantillon possède la même proportion de survenance de risques.\r\n\r\n\r\n6.3. Restriction des variables discriminantes \r\n                        - Stepwise & Bootstrap\r\n6.3.1 Régression stepwise\r\nLa régression Stepwise consiste à ajouter et à supprimer itérativement des prédicteurs, afin de trouver le sous-ensemble de variables dans l'ensemble de données résultant en le modèle le plus performant. L’entrée et la sortie d’une variable est testé  avec le test de rapport de vraisemblance.\r\nLe test de vraisemblance permet de comparer un modèle avec un sous-modèle et d’évaluer l’intérêt de la présence des termes complémentaires. On fait entrer ainsi de manière ascendante la modalité de la variable qui apparaît comme la plus significative au vu du test de rapport de vraisemblance. Puis, on teste la suppression de chaque variable du modèle avec le test de vraisemblance. Le processus s’arrête lorsque le modèle ne s’améliore plus.\r\nNous appliquons la fonction à notre base complète ainsi que sur notre base d’apprentissage. Les résultats sont les suivants :\r\n\r\n\r\n  \r\n\r\n\tRégression Stepwise sur échantillon d’apprentissage\r\n\t\r\n\r\nLes variables apparues parmi les 10 premières variables sont retenues. Nous retenons aussi les variables apparues dans le Stepwise sur échantillon total et qui ne sont pas apparues dans le Stepwise sur l’échantillon d’apprentissage. \r\nAu total, 14 variables sont retenues suite aux stepwises.\r\n6.3.2 Bootstrap\r\nL’idée du bootstrap est d’utiliser l’échantillon des observations pour permettre une inférence statistique plus fine. On réalise un certain nombre d’échantillons – qualifiés d’échantillon bootstrap- obtenus par tirage aléatoire d’observations de l’échantillon initial. Sur chacun des échantillons bootstrap, on estime les différents paramètres du modèle. On obtient par conséquent une suite de paramètres. Sous certaines conditions de régularité, la théorie montre que la distribution de la suite de paramètres obtenus converge vers la réelle distribution du paramètre.\r\nToutefois, l’inconvénient réside dans les importantes capacités de calcul que l’application de ces techniques exige. A chaque fois qu’un échantillon bootstrap est constitué, une étape d’estimation des paramètres doit être réalisée. On pourra voir qu’en pratique notre fonction est effectivement longue en temps d’exécution (3 jours pour 20 échantillons bootstrap).\r\nNous réalisons une régression logistique à sélection ascendante pour chacun des échantillons bootstrap.\r\n\r\n\r\nLe tableau suivant montre nos résultats pour 5 des 20 échantillons bootstrap ainsi que nombre d'occurrences et l’étape moyenne d’apparition.\r\n\r\n\r\n  \r\n\r\n\tBootstrap sur échantillon d’apprentissage\r\n\t\r\n\r\nLes variables apparues au moins 2 fois sur les 20 regression bootstrap ou dont l’étape moyenne d’apparition est inférieure à 6 sont conservées.\r\nSuite au Bootstrap, 11 variables apparaissent au moins deux fois dans les 20 échantillons bootstrap.  \r\n\r\n\r\nA l’issue de nos sélections Stepwise et Bootstrap, nous choisissons de garder 15 variables.\r\n6.4. Étapes Sélection d’un ou plusieurs modèles\r\n\r\n\r\nL’objectif est de trouver un modèle avec les variables les plus discriminantes parmis notre ensemble de 13 variables. \r\n\r\n\r\nToutes les combinaisons de 1 à 5 variables sont testées (au delà de 5, le temps de calcul devient prohibitif). Sur chacune de ces combinaisons, nous lançons une régression logistique . \r\n\r\n\r\nPour chaque combinaison, un modèle de régression logistique est construit. La comparaison de la qualité des modèles se fait avec les indicateurs suivants :\r\n* la déviance\r\n* le R²\r\n* l’aire sous la courbe ROC pour l’échantillon d’apprentissage\r\n* l’aire sous la courbe ROC du modèle construit sur l’échantillon d’apprentissage et testé sur l’échantillon de test\r\n* la moyenne des statistiques de Wald\r\n* le nombre de statistiques de Wald inférieures à 1,96²\r\n* le degré de liberté, c’est-à-dire le nombre total de modalités dans le modèle\r\nComme il est difficile de trouver une combinaison optimisant simultanément tous ces critères, nous retenons les meilleurs compromis. \r\nIci, l’objectif du test de Wald est de vérifier si chaques prédicteur apporte de l’information supplémentaire. Le prédicteur importe de l’information lorsque sa statistiques de test est inférieures à 1,96² (quantile de la loi normale centrée réduite à 0,95 et au carré). Nous décidons de retenir les modèles uniquement composé de prédicteur apportant de l’information. Nous décidons de choisir les modèles dont toutes les statistiques de Wald sont supérieures à 1,96².\r\nC’est principalement la valeur de l’aire sous la courbe ROC du modèle construit sur l’échantillon d’apprentissage et testé sur l’échantillon de test qui sera observée. Nous considérons aussi le nombre de degrés de liberté des modèles, en évitant un nombre trop élevé synonyme de manque de robustesse et un nombre trop faible synonyme de manque de finesse de la notation (plus difficile à découper en classes de cotation risque).\r\nNous retenons ensuite la variable qui est présente dans la majorité des combinaisons sélectionnées. Nous réitérons les tests précédents, en ajoutant, à chaque nouvelle combinaison de 5 variables, la variable sélectionnée à l'étape précédente et maintenant entrée dans la sélection. \r\n  \r\n  \r\n\r\n\tComparaison du meilleur modèle à 5 variables et du meilleur modèle à 6 variables\r\n\r\n\r\n\tDans notre cas, l’information ajouté avec un modèle à 6 variable n’est pas significatif par rapport au modèle à 5 variables (l’aire sous la courbe ROC du modèle construit sur l’échantillon d’apprentissage et testé sur l’échantillon de test passe de 0,765 à 0,769). Son nombre de degré de liberté est plus important que le modèle à 5 variable ( 10 degré de liberté en plus pour le modèle à 6 variables). Le modèle à 5 variables est donc préféré car il apporte suffisamment d’informations et il est plus robuste.\r\nL’aire sous la courbe ROC du modèle construit sur l’échantillon d’apprentissage et testé sur l’échantillon de test est de 0,765. Cette valeur est légèrement supérieure à l’aire sous la courbe ROC pour l’échantillon d’apprentissage qui est de 0,764. Son indice de Gini sur l’échantillon d’apprentissage vaut 0,53. Il vaut 0,52 sur la base entière.\r\nNotre modèle est ensuite lancé sur la base test qui regarde la survenance de risques des clients sur la période de juin 2018 à juin 2019. L’indice de Gini obtenu est \r\n\r\nDescription des 5 variables retenues :\r\n* NBPROD_HORSEP : Nombre de produits hors épargne \r\n  \r\n\r\n* SOL_FIN_MOIS : Solde en fin de mois de tous les produits d’épargne\r\n  \r\n\r\n* SLD_CPTEPA1_6M : Moyenne du solde moyen sur livrets sur 6 mois\r\n  \r\n\r\n* nbjdeb_cum : Cumul du nombre de jours débiteurs\r\n  \r\n\r\n* p1_csp : Code socioprofessionnel\r\n  \r\n\r\n\r\n\r\nCes tableaux permettent d’observer une valeur croissante du taux de survenance de risques en fonction du produit hors épargne et du cumul du nombre de jours débiteurs.\r\nLe taux de survenance de risques en fonction du nombre de produits hors épargne n’est pas linéaire. Il peut remettre en cause le découpage. Le taux de survenance de risques des clients possédant un produit hors épargne est proche du taux de survenance de risques des clients possédant 0 où 2 produits hors épargnes. Un regroupement des modalités pourrait être envisagé dans une future version\r\nD’autre part, le taux de survenance de risques à une valeur décroissante en fonction de la valeur du solde sur livret et des soldes en fin de mois de tous les produits d’épargnes.\r\nDe plus nous pouvons observer que selon la catégorie socioprofessionnelle, le taux de survenance de risques peut être multiplié par 5.\r\n\r\n\r\nDans notre nouveau modèle, seule la valeur du solde sur livret qui est présente dans les deux modèles actuels se retrouve dans le modèle.\r\n6.5. Résultat\r\nLe modèle de régression logistique est calculé sur les variables précédemment définies par la formule suivante : \r\nLOGIT                 =             - 5,67735               \r\n                                     - 1,37607                     si           NBPROD_HORSEP  ≤  0 ou non renseignée\r\n                                    +0                          si           0 < NBPROD_HORSEP ≤  1\r\n                                   + 0,18250                    si           1 < NBPROD_HORSEP ≤  2\r\n                                    + 0,39927                   si           2 < NBPROD_HORSEP ≤  3\r\n                + 0,76042                  si           3 < NBPROD_HORSEP  \r\n                                     - 1,24787                    si           SOL_FIN_MOIS  < 64,1\r\n                                    +  0,99118                   si           64,1 < SOL_FIN_MOIS ≤  803,16\r\n                                   + 0,55623                    si           803,16 < SOL_FIN_MOIS ≤  4054,34\r\n                                    + 0,33262                  si           4054,34 < SOL_FIN_MOIS ≤  11382,19\r\n                                    + 0                          si           11382,19 < SOL_FIN_MOIS ≤  25047,69\r\n                - 0,21347                  si           25047,69 < SOL_FIN_MOIS \r\n                                     + 0,79870                     si           nbjdeb_cum > 0 ou manquant\r\n                                    + 0                          si        nbjdeb_cum ≤  0\r\n                                      + 0,77437                    si           SLD_CPTEPA1_6M  < 404,76\r\n                                    +  0,34317                  si           404,76 < SLD_CPTEPA1_6M ≤  4078,82\r\n                                   + 0,05398                    si           4078,82 < SLD_CPTEPA1_6M ≤  14595,24\r\n                                   + 0                    si           14595,24 < SLD_CPTEPA1_6M\r\n                                    + 0,15753                si           SLD_CPTEPA1_6M manquant\r\n                                    + 0,39780                    si           p1_csp   ∈ {21,23,64,68,65,81,67}\r\n                                    + 0,21288                    si           p1_csp   ∈ {22,35,55,69,83,56,60}\r\n                                   - 0,58768                    si           p1_csp   ∈ {33,86,34,80,38,84}\r\n                                    - 0,41803                 si           p1_csp   ∈ {37,45,47,54,52}\r\n                                    + 0,08307                    si           p1_csp   ∈ {62,85,63}\r\n                                    + 0                           si           p1_csp   ∈ {11,43,31,48,12,13,46,53}\r\n                                    - 0,67290                   si           p1_csp   ∈ {42,44,70} ou manquant\r\n7. Découpage du score\r\n\r\n\r\nLe score est obtenu sur la base test qui regarde la survenance de risques des clients sur la période de juin 2018 à juin 2019. C’est sur cette base que le découpage du score est effectué.\r\nCette étape a pour but de découper le score obtenu précédemment, qui est une variable continue comprise entre 0 et 1 (probabilité), en 9 classes de cotation (A+, A-, B+, B-, C+, C-, D+, D-, E+) associées à des taux de survenance de risques croissants (de A+ à E+). La valeur des bornes (taux de survenance de risques ) des classes de cotation sont des valeurs préexistante. (grille de cotation des particuliers). La grille de cotation permet de comparer les différents segments entre eux.\r\nNotre objectif va être de regrouper nos clients dans chacune des classes de cotation. Pour ce faire, nous calculons dans un premier temps les centiles du score. Puis grâce à l’analyse de ces centiles via des tableaux croisés et des représentations graphiques, nous essayons de former des regroupement de centiles homogène et terme de score qui correspondent aux classes de cotation. Cela revient à essayer d’obtenir des classes de cotation avec des taux de survenance de risques de faible écart-type à l’intérieur de chacune des classes mais différents entre les classes. Ce regroupement se fait manuellement. Le principale critère étant que le taux de survenance de risques du centile médian pour chaque regroupement corresponde avec la valeur du taux de survenance de risques moyen de la classe de cotation.\r\nVoici le découpage obtenue en sélectionnant un centile sur dix :\r\n\r\n\r\n  \r\n  \r\n\r\n\tRedécoupage du score pour un centile sur dix avec grille de cotation particuliers\r\n\t\r\n\r\nLa majorité des clients sont classés en B+,B- et C+. Cela est en accord avec notre population étudiée qui présente un faible taux de survenance de risques même si il aurait été préférable d’obtenir plus de clients dans les segments A- voir A+.\r\nLes clients présents dans les derniers centiles présentent des taux de survenance de risques beaucoup plus élevé que le reste de la population. Notre étude a donc bien réussi à discriminer cette partie de la population qui correspond à la population cible.\r\n\r\n8. Conclusion\r\n\r\n\r\nLe nouveau modèle prédictif des clients Épargnants purs et des clients Compte Créditeur sans prêts possède un indice de Gini de 0,49 sur la base de test pour la période entre juin 2018 et juin 2019. Cet indice de Gini est faible mais il s’explique par une insuffisances en terme de volumétries de survenance de risques rendant une prédiction difficile. Néanmoins, le nouveau modèle obtient une meilleur performance que les deux modèles précédents. Le modèle actuel Épargnants Purs possède un indice de Gini égal à 0,21. Le modèle actuel Compte Créditeur sans prêt possède un indice de Gini égal à 0,51. Le nouveau modèle améliore donc fortement la performance pour le segment Épargnants Purs et légèrement pour le segment Compte Créditeur sans prêt.\r\nLe nouveau modèle sera donc préféré aux deux modèles actuels.\r\nCependant, le nouveau modèle peut encore être amélioré en effectuant un meilleur découpage des variables. Un redécoupage plus large apporterai plus de robustesse et pourrait amener à une meilleure prédiction.\r\n\r\n\r\n\r\n\r\n9. Compétences développées\r\n\r\n\r\nCe stage m’a permis de développer plusieurs compétences :\r\n\r\n\r\nCapacité de communication\r\n\r\n\r\nLa réalisation de mon stage a nécessité la communication avec les membres de l’équipe du service Modélisation des risques. Toutes les semaines, une présentation au reste de l’équipe de Modélisation SNI a lieu. Cette réunion permet donc de présenter les projets sur lesquels on travaille au reste de l’équipe. C’est un moment d’échange où tout le monde essaye d’aider dans les différents projets.  \r\nPour communiquer mes résultats il m’a aussi fallu les rendre compréhensible pour les autres membres de l’équipe à l’aide de graphes bien choisis.\r\n\r\n\r\nCapacité à se familiariser à un nouvel environnement\r\n\r\n\r\nLe monde bancaire était un monde qui m’était inconnu avant de faire mon stage. Il m’a donc fallu apprendre beaucoup de nouvelle notions en peu de temps afin de comprendre les données avec lesquels j’ai travaillé. Cela m’a permis de mieux comprendre les problématiques et d’analyser les résultats.\r\n\r\n\r\nCapacité de rigueur \r\n\r\n\r\nMes travaux effectués seront utilisés dans des projets futurs. Mon travail devait donc être soigneusement décrit afin que n’importe quelle personne puisse comprendre ce que j’ai effectué lors de mon stage. Cela m’a appris à structurer mon travail.\r\n\r\n\r\n10. Résumé\r\n\r\n\r\nLe Crédit mutuel Arkéa doit répondre aux exigences réglementaires de la Banque Centrale Européenne dans le cadre des accords Bâlois qui exigent une maîtrise complète des risques bancaires. Afin de pouvoir justifier ces éléments, tel que l’exigence de fond propre ou bien pour l’octroi de crédits, le Crédit mutuel Arkéa doit établir des modèles statistiques qui permettent d’estimer la probabilité de survenance de risques de ses clients. \r\nChaque client est affecté à un segment (Crédit immobilier, Épargnants Purs, Compte courant créditeur sans prêt, etc …). Pour chaque segment, un algorithme statistiques spécifique attribue une cotation à ses membres. Les algorithmes de cotation des segments Épargnants Purs et Compte courant créditeur sans prêt obtiennent des performances trop faibles lié à leurs faibles volumétries. Néanmoins, ces deux segments présentent des similarités d’un point de vue comportement et survenance de risques. Ma mission a été de regrouper ces deux types de clients en un seul segment afin de voir si une meilleure prédiction globale serait réalisable.\r\nTout d’abord, la base de données qui servira à la modélisation est construite sur le périmètre défini plus tôt. Les données sont ensuite analysées pour obtenir une base de données de meilleure qualité. Un modèle de scoring est enfin élaboré de manière à prédire la survenance de risques.\r\n\r\n\r\nCrédit Mutuel Arkéa must meet the regulatory requirements of the European Central Bank in the context of the Basel agreements which require complete control of banking risks. In order to be able to justify these elements, such as the capital requirement or else for the granting of credits, Crédit Mutuel Arkéa must establish statistical models which make it possible to estimate the probability of the occurrence of risks for its customers.\r\nEach client is assigned to a segment (Real estate credit, Pure Savers, Credit current account without loan, etc.). For each segment, a specific statistical algorithm assigns a rating to its members. The algorithms for scoring the Pure Saver and Loan Credit Account segments get too low performance linked to their low volumes. However, these two segments present similarities from a behavior and risk occurrence point of view. My mission was to group these two types of customers into a single segment in order to see if a better global prediction would be achievable.\r\nFirst, the database that will be used for modeling is built on the perimeter defined earlier. The data is then analyzed to obtain a better quality database. Finally, a scoring model is developed in order to predict the occurrence of risks.\r\n\r\n\r\n\r\n\r\n11. Annexe\r\n\r\n\r\nRépartition du taux de survenance de risques de chaques modalités pour chaques variables",
      "metadata": {
        "drive_file_id": "16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY",
        "name": "Copie de Rapport de Stage Youn Jéhanno",
        "mimeType": "application/vnd.google-apps.document",
        "modifiedTime": "2020-03-03T16:31:27.961Z",
        "webViewLink": "https://docs.google.com/document/d/16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z"
      }
    }
  ],
  "chunks": [
    {
      "doc_content_hash": "51d91a522f33e26ffd271601822eb1d7ca1d9119ba705468c62cfd20216e532a",
      "chunk_index": 0,
      "text": "﻿test je suis bien",
      "approx_tokens": 5,
      "metadata": {
        "drive_file_id": "1B2HYO2ne9MmjXdDKxGJNucT62HtNo-ECECBswbxn2qs",
        "name": "docuemnt_test",
        "mimeType": "application/vnd.google-apps.document",
        "modifiedTime": "2025-10-27T09:04:50.735Z",
        "webViewLink": "https://docs.google.com/document/d/1B2HYO2ne9MmjXdDKxGJNucT62HtNo-ECECBswbxn2qs/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z",
        "title": "docuemnt_test",
        "uri": "gdrive://1B2HYO2ne9MmjXdDKxGJNucT62HtNo-ECECBswbxn2qs"
      }
    },
    {
      "doc_content_hash": "2bd4dfc3a1adad44451831e812b1191c587f82160a47b287a476fef0a222002d",
      "chunk_index": 0,
      "text": "﻿INSA Rennes\r\nMÉMOIRE DE STAGE\r\nModélisation statistique du risque de défaut\r\n\r\n  \r\n\r\n\r\n\r\nMémoire CONFIDENTIEL\r\n\r\n\r\n4ème année Génie Mathématiques\r\nINSA Rennes\r\nNom de l’entreprise : Crédit Mutuel ARKEA\r\nResponsable de stage : Guillaume Le Coz\r\nEnseignant référent : Loïc Hervé\r\nDate du stage : 13 Janvier au 6 Mars 2020\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n________________\r\n\r\n\r\nSommaire\r\n\r\n\r\nRemerciements\r\nNote de Confidentialité\r\n1. Introduction\r\n2. Présentation de l’entreprise\r\n2.1. Groupe Arkéa\r\n2.1.1 Historique et organisation\r\n2.1.2 Quelques chiffres\r\n2.1.3 Souhait d’indépendance d’Arkéa\r\n2.1.4 Organisation de la direction des risques et du département Modélisation Statistique des Risques\r\n3. Environnement de travail\r\n3.1. Mise en situation\r\n3.1.1 Définition population observée\r\n3.1.2 Problématique actuelle\r\n3.2. Environnement de travail\r\n3.3.Description des bases de données utilisées\r\n4. Constitution de la base de données\r\n4.1. Détermination de la variable cible\r\n4.2. Exploration des données\r\n4.3. Définition du périmètre\r\n4.4. Construction de la base\r\n5. Analyse de la qualité des données\r\n5.1. Préparation des données\r\n5.2. Maquette d’exploration des données\r\n5.3. Redécoupage des variables (CHAID)\r\n5.4. Recodage des données manquantes\r\n6. Développement des modèles\r\n6.1. Sélection des variables discriminantes\r\n6.1.1. ACP\r\n6.1.2. V de Cramer\r\n6.1.3. Test de corrélation\r\n6.2. Echantillonnage\r\n6.3. Restriction des variables discriminantes - Stepwise & Bootstrap\r\n8.3.1 Forward step\r\n6.3.2 Bootstrap\r\n6.4. Sélection d’un ou plusieurs modèles\r\n6.5. Résultat\r\n6.6. Analyse des résultats\r\n7. Découpage du score\r\n7.1. Construction et validation du modèle\r\n7.1.1 Courbe ROC\r\n7.1.2 Indice de Gini\r\n8. Compétences développées\r\n\r\n\r\n________________\r\n\r\n\r\nRemerciements\r\n\r\n\r\nécrire les remerciements\r\n\r\n\r\nNote de Confidentialité\r\nCe présent rapport ainsi que toutes les informations qu’il contient sont strictement confidentiels exclusivement au corps professoral de l’INSA de Rennes ou à une personne interne à l’équipe statistique du département modélisation statistique des risques.\r\nLes chiffres présents dans ce rapport ont été volontairement faussés toujours dans cet objectif de confidentialité. Ils restent toutefois cohérents.\r\nMerci de bien vouloir respecter la confidentialité de ce document.\r\n________________\r\n\r\n\r\n\r\n\r\n1. Introduction\r\n\r\n\r\nDans le cadre de ma formation de 4ème année Génie Mathématiques à l’INSA de Rennes, j’ai effectué un stage d’une durée de 2 mois au siège du Crédit Mutuel Arkéa, à Brest. J’ai intégré le département Modélisation Statistique des risques au sein du service “Modélisation Système de Notation Interne”. Le département est chargé d’évaluer le risque de crédit des clients en fonction de leur profil, le risque opérationnel ainsi que le calcul de l'exigence en fonds propres associée aux risques pris par la banque. A cela, s'ajoutent différentes études statistiques pour la Direction en raison de nombreux et nouveaux projets impliquants pour le Groupe.\r\nMa mission a consisté à créer un nouveau modèle statistique permettant d’obtenir une meilleure prédiction du défaut des clients épargnants purs et des clients avec compte créditeur et sans crédit.\r\n2. Présentation de l’entreprise\r\n2.1. Groupe Arkéa\r\n2.1.1 Historique et organisation\r\nLe Groupe ARKÉA est un groupe coopératif et mutualiste qui comprend plus de 20 filiales spécialisées qui couvrent tous les métiers de la banque, de la finance et de l'assurance, comme Fortuneo (banque en ligne), Suravenir Assurances, Federal Finance (gestion d'actifs pour compte de tiers), et Financo (crédit à la consommation). Son siège social est implanté au Relecq-Kerhuon dans le Finistère.\r\nIl est né de l'alliance des fédérations de Crédit Mutuel de Bretagne et du Sud-Ouest (CMB, CMSO). C’est le deuxième pôle régional du Crédit Mutuel. Ancré sur ses territoires et comptant près de 10 000 salariés. Il regroupe un grand nombre de fonctions supports et de métiers d'expertise (Finance, Organisation, Audit, M",
      "approx_tokens": 1000,
      "metadata": {
        "drive_file_id": "1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0",
        "name": "Rapport de Stage Youn Jéhanno",
        "mimeType": "application/vnd.google-apps.document",
        "modifiedTime": "2021-06-10T13:23:55.779Z",
        "webViewLink": "https://docs.google.com/document/d/1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z",
        "title": "Rapport de Stage Youn Jéhanno",
        "uri": "gdrive://1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0"
      }
    },
    {
      "doc_content_hash": "2bd4dfc3a1adad44451831e812b1191c587f82160a47b287a476fef0a222002d",
      "chunk_index": 1,
      "text": "nsommation). Son siège social est implanté au Relecq-Kerhuon dans le Finistère.\r\nIl est né de l'alliance des fédérations de Crédit Mutuel de Bretagne et du Sud-Ouest (CMB, CMSO). C’est le deuxième pôle régional du Crédit Mutuel. Ancré sur ses territoires et comptant près de 10 000 salariés. Il regroupe un grand nombre de fonctions supports et de métiers d'expertise (Finance, Organisation, Audit, Marketing, Juridique,...).\r\n2.1.2 Quelques chiffres\r\nLe Groupe ARKÉA en 2020 représente :\r\n❏     4,5 millions de clients\r\n❏     10 500 salariés\r\n❏     111.2 millions d'euros encours d'épargne (épargne financière, épargne assurance, dépôts).\r\n❏     6.7 milliards d'euros de fonds propres\r\n❏     437 millions d'euros de résultat net\r\n❏     135 milliards d’euros de bilan comptable \r\n2.1.3 Contexte d’indépendance\r\nDepuis 2015, le groupe Arkéa souhaite devenir indépendant du Crédit Mutuel. Cette désaffiliation est prévue pour janvier 2021. \r\nDans ce cadre, il a été engagé des travaux préparatoires de refonte complète des modèles statistiques (aujourd’hui réalisés sur l’ensemble du Crédit Mutuel) sur la population Arkéa.\r\nL’objectif de ces travaux est de prouver à la Banque Centrale Européenne qu’elle est capable de fonctionner indépendamment.\r\n\r\n\r\nCe stage s’inscrit dans ce contexte.\r\n2.1.4 Organisation de la direction des risques et du département Modélisation Statistique des Risques\r\nLe Groupe ARKÉA est divisé en différentes directions avec leurs activités et objectifs propres. L’une des directions, la direction des risques, est celle dans laquelle j’ai effectué mon stage. Elle est composée de 130 collaborateurs dont les principales missions sont la mise en place et le pilotage du dispositif global de gestion des risques du groupe.\r\nLe département Modélisation Statistique des Risques est composé d’une dizaine de statisticiens/data scientists. Les activités principales des différents services au sein du département sont les suivants :\r\n❏        Service Gestion de la donnée et modélisation du risque opérationnel\r\n●           Construire, suivre et maintenir les bases de données clients, métiers, IT,…  servant aux études et à la modélisation statistiques du Groupe et des filiales.\r\n●          Contribuer à la cartographie globale des risques du Groupe et en déduire un état de reporting à des fins de communication aux organes dirigeants.\r\n●     Elaborer et quantifier des modèles de risques opérationnels dans le cadre du calcul de l’exigence en fonds propres.\r\n●         Détecter et contribuer à la prévention d’incidents opérationnels via la création de modèles de comportement de détection de fraude externe.\r\n❏        Service « Modélisation Système de Notation Interne »\r\n●     Créer et assurer la maintenance des modèles de notation interne statistiques (pour les fédérations ou les filiales) en réalisant des études statistiques de comportement des clients en termes de risques de crédit.\r\n●           Rédiger des cahiers des charges de cotation à destination de l'informatique et assurer la coordination de la mise en place des nouveaux algorithmes de notation (recette, assistance de second niveau, suivi des modèles etc…), en lien avec l'informatique et toutes les structures du Groupe impliquées dans la maîtrise des risques et dans la qualité des données en amont de la cotation (direction des engagements, filiales …).\r\n❏     Service « Paramètres et provisions du risque de crédit »\r\n●         Procéder statistiquement aux calculs de paramètres selon les normes bâloise ou comptable IFRS 9 (Probabilité de défaut - PD, Pertes en cas de défaut - LGD et Facteur de conversion d’encours hors bilan en bilan - CCF), afin de permettre les calculs de l’exigence de fonds propres au titre du risque de crédit et du niveau de provisionnement du groupe.\r\n●          Accompagner le process par le suivi et la maintenance des modèles de protection (bâlois et provisionnement), en lien avec l'informatique et toutes les structures du Groupe impliquées dans la maîtrise des risqu",
      "approx_tokens": 1000,
      "metadata": {
        "drive_file_id": "1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0",
        "name": "Rapport de Stage Youn Jéhanno",
        "mimeType": "application/vnd.google-apps.document",
        "modifiedTime": "2021-06-10T13:23:55.779Z",
        "webViewLink": "https://docs.google.com/document/d/1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z",
        "title": "Rapport de Stage Youn Jéhanno",
        "uri": "gdrive://1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0"
      }
    },
    {
      "doc_content_hash": "2bd4dfc3a1adad44451831e812b1191c587f82160a47b287a476fef0a222002d",
      "chunk_index": 2,
      "text": "version d’encours hors bilan en bilan - CCF), afin de permettre les calculs de l’exigence de fonds propres au titre du risque de crédit et du niveau de provisionnement du groupe.\r\n●          Accompagner le process par le suivi et la maintenance des modèles de protection (bâlois et provisionnement), en lien avec l'informatique et toutes les structures du Groupe impliquées dans la maîtrise des risques et dans la qualité des données en amont des systèmes de détermination des paramètres (direction des engagements, filiales …).\r\n3. Environnement de travail\r\n3.1. Mise en situation\r\n3.1.1 Définition population observée\r\nActuellement, les clients particuliers (personnes physiques) sont répartis en 11 segments distincts avec l’ordre des priorisation ci-dessous:\r\n  \r\n\r\nLes 6 segments 19,18,17 et 15 ont des cotations forfaitaires (même note attribuée à tous les membres de chaque segment) basées sur une étude des taux de défauts observés.\r\nLes 5 autres segments possèdent un algorithme statistiques spécifique qui attribue une cotation à ses membres. L’étude se concentre sur les segments 10 et 11.\r\nÉpargnant pur : clients sans prêt, sans risque en cours, qui n’est pas nouveau client et qui n’a    pas de compte courant.\r\nCompte courant créditeur sans prêt : clients sans prêt, sans risque en cours et qui n’est pas nouveaux client avec au moins un compte courant et avec au maximum un jour de débit sur les 6 mois d’observation.\r\n3.1.2 Problématique actuelle\r\nLa segmentation actuelle des populations pourrait être améliorée en regroupant les deux segment, « Épargnants purs » et « CC créditeur », qui présentent un faible nombre d’observations de défaut et des similarités d’un point de vue comportement et TD.\r\nActuellement, le modèle Épargnants Purs présente des insuffisances en terme de performance lié à ses faibles volumétries.\r\nMa mission a été de regrouper ces deux types de clients en un seul segment afin de voir si une meilleure prédiction globale serait réalisable.\r\n3.2. Environnement de travail\r\n\r\n\r\nLes bases de données utilisées sont issues de la BNS (Base Nationale Statistique) et c’est le logiciel R qui a servi à construire le modèle.\r\n\r\n\r\nL’étude des données a principalement été réalisée à partir de maquettes excel.\r\n\r\n\r\nPour ce projet, la majorité des fonctions utilisées sont issues d’un package interne ModelSNI regroupant un ensemble de syntaxes pour la construction d’un modèle de cotation.\r\n3.3.Description des bases de données utilisées\r\n\r\n\r\nLe modèle a été construit à partir des 8 bases de données suivantes :\r\n  \r\n\r\n4. Constitution de la base de données\r\n4.1. Détermination de la variable cible\r\n\r\n\r\nLa variable cible correspond au passage en défaut (appelée top_def). Elle vaut :\r\n\r\n\r\n* 1, si le client était sain en début de période d’observation du passage en défaut et est passé en défaut au cours de la période (qui dure 12 mois);\r\n* 0, si le client était sain en début de période et est resté sain pendant toute la période.\r\n4.2. Exploration des données\r\n\r\n\r\nAvant de commencer à manipuler les bases de données, il est important de se familiariser avec les données et de comprendre les variables. Un fichier Excel décrit l’ensemble des variables utilisées et facilite la compréhension de la base de données. Cette phase d’exploration va aussi permettre de repérer d'éventuelles valeurs aberrantes ainsi que des valeurs manquantes\r\n4.3. Définition du périmètre\r\n\r\n\r\nIl est important de définir le périmètre de l’étude pour travailler avec les bonnes données.\r\n\r\n\r\n* Périmètre définit pour l’étude : \r\n   * CMB (Crédit Mutuel de Bretagne), CMSO (Crédit Mutuel du Sud Ouest) et Fortunéo\r\n   * Segment 10 et 11, Épargnant pur et Compte courant créditeur sans prêt\r\n\r\n\r\n* Horizon de prédiction : 1 an\r\n\r\n\r\n* Dates de la base de modélisation :\r\n   * Base de modélisation : concaténation de 2 bases ( Ce choix ne garantit pas l’indépendance de nos observations mais permet d’avoir une base de données plus importante et évite les effets de saisonnalité)\r\n      *",
      "approx_tokens": 1000,
      "metadata": {
        "drive_file_id": "1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0",
        "name": "Rapport de Stage Youn Jéhanno",
        "mimeType": "application/vnd.google-apps.document",
        "modifiedTime": "2021-06-10T13:23:55.779Z",
        "webViewLink": "https://docs.google.com/document/d/1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z",
        "title": "Rapport de Stage Youn Jéhanno",
        "uri": "gdrive://1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0"
      }
    },
    {
      "doc_content_hash": "2bd4dfc3a1adad44451831e812b1191c587f82160a47b287a476fef0a222002d",
      "chunk_index": 3,
      "text": "el du Sud Ouest) et Fortunéo\r\n   * Segment 10 et 11, Épargnant pur et Compte courant créditeur sans prêt\r\n\r\n\r\n* Horizon de prédiction : 1 an\r\n\r\n\r\n* Dates de la base de modélisation :\r\n   * Base de modélisation : concaténation de 2 bases ( Ce choix ne garantit pas l’indépendance de nos observations mais permet d’avoir une base de données plus importante et évite les effets de saisonnalité)\r\n      * Base à décembre 2015 avec avec défaut observé sur l’année 2016\r\n      * Base à juin 2017 avec avec défaut observé entre juin 2016 et juin 2017\r\n\r\n\r\n\r\n\r\n  \r\n\r\n\r\n\r\n4.4. Construction de la base\r\n\r\n\r\nLa base est construite à partir des bases de décembre 2015 et de juin 2017, elles mêmes construites grâce à la jointure de l’ensemble des bases décrites dans le tableaux précédent.\r\n\r\n\r\nCette partie est très importante et chronophage car il faut que la base soit la plus complète et la plus fiable possible afin de pouvoir obtenir des résultat plus satisfaisants. \r\nIl a fallu faire face à de nombreux problèmes comme des individus manquants dans certaines bases. Les bases étant de plus très volumineuses, il est d’autant plus difficiles de les trouver.\r\nLa vérification des bases de données à chaques étapes de conception est donc très importante.\r\n\r\n\r\nLa base construite est alors constituée de 1 600 000 observations et de 312 variables.\r\n\r\n\r\n5. Analyse de la qualité des données\r\n5.1. Préparation des données\r\n\r\n\r\nLa préparation des données est composée de deux parties :\r\n\r\n\r\n* Recodage de certaines variables\r\n   * ex : Les données manquantes en (9999999 ou 0) sont recodées en NA\r\n       Les dates < 1901 ou égales à 0 sont recodées à NA car il s’agit d’une\r\n      codification liées aux valeurs manquantes\r\n       Les variables avec >100 modalités sont recodées afin de pouvoir appliquer les \r\n      méthodes statistiques de sélection de variables\r\n       Les valeurs avec des signes aberrants afin de s’assurer une bonne qualité des \r\n      données\r\n\r\n\r\n* Construction de nouvelles variables (moyennes, région) :\r\n   * 26 variables calculant la moyenne sur 6 mois de variables relatives aux comportements bancaires\r\n   * 2 variables indiquant la région d’habitation des tiers\r\n   * 7 variables calculant des sommes de soldes sur différents comptes épargnes\r\n\r\n\r\nLa construction de nouvelles variables a plusieurs avantages :\r\n\r\n\r\n* Fait entrer dans le modèle plusieurs variables en évitant la présence de variables corrélées\r\n* Limite le nombre de variables dans le modèle\r\n* Prend en compte un historique du comportement du client\r\n5.2. Maquette d’exploration des données \r\n\r\n\r\nUne fois les nouvelles variables créées, il reste de nombreuses variables qui n’apporte aucune information pour notre étude. Il faut donc les identifier et les supprimer dans un soucis de clarté. Pour cela, nous utilisons des maquettes d’exploration des données par Entité (1 :CMB, 3: CMSO, 21: Fortuneo). Cette étude par entité permet de s’assurer une homogénéité de la donnée entre les différents périmètres étudiés :\r\n\r\n\r\n* Données quantitatives : les valeurs étudiées sont les suivantes :\r\n   * nombre de valeurs manquantes\r\n   * variable constante ou non\r\n   * moyenne\r\n   * ecart-type\r\n   * minimum\r\n   * max\r\n   * quantiles 0.01, 0.05, 0.10, 0.20, 0.25,0.30, 0.40,0.50, 0.60, 0.70, 0.75,0.80, 0.90, 0.95, 0.99\r\n   * le pourcentage de 0.\r\n  \r\n                     \r\n\tMaquette d’exploration des données des variables quantitatives pour LE CMB (1)\r\n\t\r\n\r\nExemple :\r\n\r\n\r\nLes variables ddbu_dero et dfin_dero sont tout le temps égales à 0. Elles sont constantes et ne sont pas gardées dans le modèle.\r\n\r\n\r\nLa variables R_EPAMT_EPATOT possède des centiles chez Fortunéo au moins 10 fois supérieurs aux centiles du CMB et du CMSO. La variable est tout de même gardée car ses informations sont intéressantes.\r\n\r\n\r\n\r\n\r\n* Données qualitatives : les valeurs étudiées sont les suivantes :\r\n   * nombre de valeurs manquantes\r\n   * variable constante ou non\r\n   * effectif\r\n   * fréquence\r\n\r\n\r\n  \r\n\r\n\tMaquette d’exploratio",
      "approx_tokens": 1000,
      "metadata": {
        "drive_file_id": "1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0",
        "name": "Rapport de Stage Youn Jéhanno",
        "mimeType": "application/vnd.google-apps.document",
        "modifiedTime": "2021-06-10T13:23:55.779Z",
        "webViewLink": "https://docs.google.com/document/d/1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z",
        "title": "Rapport de Stage Youn Jéhanno",
        "uri": "gdrive://1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0"
      }
    },
    {
      "doc_content_hash": "2bd4dfc3a1adad44451831e812b1191c587f82160a47b287a476fef0a222002d",
      "chunk_index": 4,
      "text": "ables R_EPAMT_EPATOT possède des centiles chez Fortunéo au moins 10 fois supérieurs aux centiles du CMB et du CMSO. La variable est tout de même gardée car ses informations sont intéressantes.\r\n\r\n\r\n\r\n\r\n* Données qualitatives : les valeurs étudiées sont les suivantes :\r\n   * nombre de valeurs manquantes\r\n   * variable constante ou non\r\n   * effectif\r\n   * fréquence\r\n\r\n\r\n  \r\n\r\n\tMaquette d’exploration des données des variables qualitatives\r\n\t\r\n\r\n  \r\n\r\n\tLégende des alertes des variables qualitatives\r\n\t\r\n\r\nA partir de toutes ces informations, il faut déterminer quelles variables sont utiles et lesquelles ne le sont pas. \r\nLes variables constantes ou uniquement constituées de valeurs manquantes ne sont pas gardées.  Nous supprimons 173 variables.\r\n\r\n\r\nPour les autres variables, la décision revient à l’appréciation du statisticien. En fonction des expériences des modèles précédents et de notre jugement, 35 variables sont supprimées.\r\n\r\n\r\nSuite à l’exploration, nous supprimons 208 variables. Notre base contient alors 118 variables. \r\n5.3. Redécoupage des variables (CHAID)\r\nL’algorithme CHAID regroupe pour chaque variable explicative ses modalités les moins liées à la variable à expliquer, c’est-à-dire celles dont les taux de défaut sont les plus voisins au vu du test du khi-2. Il le fait de proche en proche par paires de modalités puis de regroupements de modalités, jusqu’à ce que ne subsistent plus que des regroupements de modalités aux taux de défaut suffisamment distincts.\r\n\r\n\r\nCe redécoupage est utile car nous manipulons des bases de données contenant de nombreuses observations. Cela va permettre de répartir la population en groupes homogènes, avec des variables discriminantes plus performantes. \r\n\r\n\r\n* Transformation des variables continues : discrétisation\r\n\r\n\r\n* Transformation des variables qualitatives : regroupement de modalités\r\n\r\n\r\nLe découpage ne doit pas être trop fin de manière à être le plus discriminant possible. Dans notre cas, pour certaines variables, on observe une tendance linéaire du taux de défaut en fonction des valeurs de la variable. Il est donc important que notre découpage suive cette tendance. Pour cela il faut trouver le bon seuil pour notre découpage car un découpage trop fin ne respectera pas la règle mais un découpage trop large fera perdre de l’information.\r\n\r\n\r\nLa base est maintenant constituée uniquement de données qualitatives. Il reste à corriger le problème des valeurs manquantes.\r\n5.4. Recodage des données manquantes\r\n\r\n\r\nLes valeurs manquantes de chaque variable sont regroupées dans une modalité puis leur taux de défauts est analysé. Selon les volumes observés, les données manquantes sont affectées à la catégorie la plus proche (en terme de taux de défaut) ou bien une modalité spécifique aux données manquantes est créée.\r\n\r\n\r\nLorsque le taux de valeurs manquantes est supérieur à 10% pour une variable, on considère les données manquantes comme une modalité spécifique.\r\nDans le cas contraire, celles-ci sont regroupées avec la modalité ayant le taux de défaut le plus proche.\r\n\r\n\r\n\r\n\r\n\r\n\r\n  \r\n\r\n\tMaquette du recodage des valeurs manquantes\r\n\t\r\n\r\n\r\n\r\nDans cet exemple, les données manquantes de la variables decp_EXI_AVOIR2 représentent moins de 10 % des observations. Elles sont donc regroupées avec la modalité [-INF,0] qui est la modalité possédant le taux de défaut le plus proche des valeurs manquantes.\r\n\r\n\r\nUne analyse “à dire d’expert” est également réalisée afin de s’assurer de la cohérence du découpage et de l’affectation des valeurs manquantes. En effet, dans certains cas où le nombre de valeurs manquantes est faible, le taux de défaut observé n’est pas significatif. L’affectation à la modalité ayant le taux de défaut le plus proche n’est donc pas forcément la meilleure. Il convient donc de l’affecter à la modalité répondant le mieux à la logique bancaire.\r\n\r\n\r\nCette étape permet aussi de vérifier les découpages qui ont été effectué par l’algorithme CHAID. Si une modalité contient trop peu d’ind",
      "approx_tokens": 1000,
      "metadata": {
        "drive_file_id": "1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0",
        "name": "Rapport de Stage Youn Jéhanno",
        "mimeType": "application/vnd.google-apps.document",
        "modifiedTime": "2021-06-10T13:23:55.779Z",
        "webViewLink": "https://docs.google.com/document/d/1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z",
        "title": "Rapport de Stage Youn Jéhanno",
        "uri": "gdrive://1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0"
      }
    },
    {
      "doc_content_hash": "2bd4dfc3a1adad44451831e812b1191c587f82160a47b287a476fef0a222002d",
      "chunk_index": 5,
      "text": "tes est faible, le taux de défaut observé n’est pas significatif. L’affectation à la modalité ayant le taux de défaut le plus proche n’est donc pas forcément la meilleure. Il convient donc de l’affecter à la modalité répondant le mieux à la logique bancaire.\r\n\r\n\r\nCette étape permet aussi de vérifier les découpages qui ont été effectué par l’algorithme CHAID. Si une modalité contient trop peu d’individus, elle est regroupée avec la modalité la plus proche en terme de valeur de la variable.\r\n\r\n\r\n6. Développement des modèles\r\n6.1. Sélection des variables discriminantes\r\n6.1.1. ACP \r\n\r\n\r\nL’ACP permet d’étudier et de visualiser des corrélations entre les variables, afin d’éventuellement limiter le nombre de variables à mesurer par la suite. L’objectif est d’obtenir des variables non corrélées qui sont des combinaisons linéaires des variables de départ, afin d’utiliser ces variables dans les méthodes de modélisation utilisées par la suite telles que la régression logistique ou l’analyse discriminante. Elle permet d’identifier des groupes homogènes d’observations, ou au contraire des observations atypiques.\r\n\r\n\r\nNous obtenons alors 25 composantes principales. A chaque composante principale, nous affectons les variables ayant un coefficient de corrélation supérieur à 0,7.\r\n6.1.2. V de Cramer\r\nPour estimer si les modalités sont probablement indépendantes ou au contraire liées, on a coutume de procéder à un test du V de Cramer qui reste stable si l’on augmente la taille de l’échantillon dans les mêmes proportions inter-modalités. Plus le V de Cramer d’une variable est proche de zéro, plus il y a indépendance entre les deux variables étudiées. \r\nNous réalisons un test du V de Cramer entre la variable cible top_def et toutes les variables du modèle :\r\n  \r\n\r\nLa variable decp_risq_deg correspond à l’identifiant d’un risque dégradant. Elle possède un V de Cramer très élevé car les individus ayant un risque dégradant entraînant une dégradation de la cotation primaire ont un fort taux de défaut.\r\nLa variables decp_solde_cpt_ep2_PEP correspond au solde moyen mensuel du mois 2 sur le Plan Epargne Populaire de la personne. Elle possède un V de Cramer faible car la variable prend ses valeurs sur un mois. Ce temps n’est pas assez long pour avoir des valeurs significatives.\r\nOn retient :\r\n* Pour chaque composante principale de l’ACP, la ou les variables ayant le plus fort V de Cramer.\r\n* On ne retient pas les variables ayant un V de Cramer < 0,03. Dans notre cas, les V de Cramer sont très faible car le défaut est très dur a expliquer pour notre population donc cette règle n’est pas appliquée pour certains cas et nous essayerons de prendre les variables ayant des V de Cramer suffisamment grands.\r\n* Les moyennes, ratios ou autres variables combinées sont privilégiées par rapport aux variables simples (ex: moyenne sur 6 mois plutôt que valeur sur un mois)\r\n* Les variables sélectionnées dans l’ancien modèle. Ces variables sont censées être les plus explicatives pour les segments clients épargnants purs ainsi que les clients avec compte créditeur et sans crédit.\r\nVariables sélectionnées pour la modélisation : 28\r\nL’idée est de ne pas se limiter aux variables ayant un V de Cramer élevé, mais de sélectionner des variables moins discriminantes mais qui apportent une information supplémentaire.\r\n6.1.3. Test de corrélation\r\nOn réalise un test du V de Cramer entre les 28 variables sélectionnées pour la modélisation pour observer les corrélations. Les variables ayant une corrélation trop élevée ( >0,70) sont retirées si elles possèdent un V de Cramer inférieur à la variable avec laquelle elles sont corrélées. En cas de V de Cramer très proches, les variables préexistantes sont préférées car plus fiables. Celles-ci font l’objet de suivis périodiques.\r\n  \r\n\r\n\tCorrélation entre les variables PCS, decp_nbj_depaut, decp_NBJ_DEPAUT_6M et decp_EXI_AVOIR2\r\n\t\r\n\r\nLes variables decp_nbj_depaut, decp_NBJ_DEPAUT_6M et decp_EXI_AVOIR2 ont des corrélations supérieures à 0,70.",
      "approx_tokens": 1000,
      "metadata": {
        "drive_file_id": "1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0",
        "name": "Rapport de Stage Youn Jéhanno",
        "mimeType": "application/vnd.google-apps.document",
        "modifiedTime": "2021-06-10T13:23:55.779Z",
        "webViewLink": "https://docs.google.com/document/d/1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z",
        "title": "Rapport de Stage Youn Jéhanno",
        "uri": "gdrive://1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0"
      }
    },
    {
      "doc_content_hash": "2bd4dfc3a1adad44451831e812b1191c587f82160a47b287a476fef0a222002d",
      "chunk_index": 6,
      "text": "iable avec laquelle elles sont corrélées. En cas de V de Cramer très proches, les variables préexistantes sont préférées car plus fiables. Celles-ci font l’objet de suivis périodiques.\r\n  \r\n\r\n\tCorrélation entre les variables PCS, decp_nbj_depaut, decp_NBJ_DEPAUT_6M et decp_EXI_AVOIR2\r\n\t\r\n\r\nLes variables decp_nbj_depaut, decp_NBJ_DEPAUT_6M et decp_EXI_AVOIR2 ont des corrélations supérieures à 0,70. Cela signifie qu’elles possèdent globalement la même information. Seul decp_NBJ_DEPAUT_6M est gardées car c’est la variable ayant le meilleur V de Cramer.\r\nLa variable PCS est gardée car faiblement corrélée à toutes les autres. Sa corrélation est faible par rapport aux autres variables car la PCS correspond à la catégorie socioprofessionnelle du foyer et les autres variables font référence à des données bancaires. Cela apporte donc une information complémentaire.\r\nVariables sélectionnées pour la modélisation : 21\r\n6.2. Echantillonnage\r\nDécoupage de la base de modélisation en deux échantillons :\r\n\r\n\r\n* Échantillon d’apprentissage : 70% de la base de modélisation. Sert à construire les modèles\r\n\r\n\r\n* Échantillon test : 30% de la base de modélisation. Sert à valider les modèles.\r\n\r\n\r\n  \r\n\r\n\r\n\r\nChaque échantillon possède la même proportion de défauts.\r\n\r\n\r\n6.3. Restriction des variables discriminantes \r\n                        - Stepwise & Bootstrap\r\n6.3.1 Régression stepwise\r\nLa régression Stepwise consiste à ajouter et à supprimer itérativement des prédicteurs, afin de trouver le sous-ensemble de variables dans l'ensemble de données résultant en le modèle le plus performant. L’entrée et la sortie d’une variable est testé  avec le test de rapport de vraisemblance.\r\nLe test de vraisemblance permet de comparer un modèle avec un sous-modèle et d’évaluer l’intérêt de la présence des termes complémentaires. On fait entrer ainsi de manière ascendante la modalité de la variable qui apparaît comme la plus significative au vu du test de rapport de vraisemblance. Puis, on teste la suppression de chaque variable du modèle avec le test de vraisemblance. Le processus s’arrête lorsque le modèle ne s’améliore plus.\r\nNous appliquons la fonction à notre base complète ainsi que sur notre base d’apprentissage. Les résultats sont les suivants :\r\n\r\n\r\n  \r\n\r\n\tRégression Stepwise sur échantillon d’apprentissage\r\n\t\r\n\r\nLes variables apparues parmi les 10 premières variables sont retenues. Nous retenons aussi les variables apparues dans le Stepwise sur échantillon total et qui ne sont pas apparues dans le Stepwise sur l’échantillon d’apprentissage. \r\nAu total, 14 variables sont retenues suite aux stepwises.\r\n6.3.2 Bootstrap\r\nL’idée du bootstrap est d’utiliser l’échantillon des observations pour permettre une inférence statistique plus fine. On réalise un certain nombre d’échantillons – qualifiés d’échantillon bootstrap- obtenus par tirage aléatoire d’observations de l’échantillon initial. Sur chacun des échantillons bootstrap, on estime les différents paramètres du modèle. On obtient par conséquent une suite de paramètres. Sous certaines conditions de régularité, la théorie montre que la distribution de la suite de paramètres obtenus converge vers la réelle distribution du paramètre.\r\nToutefois, l’inconvénient réside dans les importantes capacités de calcul que l’application de ces techniques exige. A chaque fois qu’un échantillon bootstrap est constitué, une étape d’estimation des paramètres doit être réalisée. On pourra voir qu’en pratique notre fonction est effectivement longue en temps d’exécution (3 jours pour 20 échantillons bootstrap).\r\nNous réalisons une régression logistique à sélection ascendante pour chacun des échantillons bootstrap.\r\n\r\n\r\nLe tableau suivant montre nos résultats pour 5 des 20 échantillons bootstrap ainsi que nombre d'occurrences et l’étape moyenne d’apparition.\r\n\r\n\r\n  \r\n\r\n\tBootstrap sur échantillon d’apprentissage\r\n\t\r\n\r\nLes variables apparues au moins 2 fois sur les 20 regression bootstrap ou dont l’étape moyenne d’apparition est infér",
      "approx_tokens": 1000,
      "metadata": {
        "drive_file_id": "1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0",
        "name": "Rapport de Stage Youn Jéhanno",
        "mimeType": "application/vnd.google-apps.document",
        "modifiedTime": "2021-06-10T13:23:55.779Z",
        "webViewLink": "https://docs.google.com/document/d/1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z",
        "title": "Rapport de Stage Youn Jéhanno",
        "uri": "gdrive://1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0"
      }
    },
    {
      "doc_content_hash": "2bd4dfc3a1adad44451831e812b1191c587f82160a47b287a476fef0a222002d",
      "chunk_index": 7,
      "text": "on logistique à sélection ascendante pour chacun des échantillons bootstrap.\r\n\r\n\r\nLe tableau suivant montre nos résultats pour 5 des 20 échantillons bootstrap ainsi que nombre d'occurrences et l’étape moyenne d’apparition.\r\n\r\n\r\n  \r\n\r\n\tBootstrap sur échantillon d’apprentissage\r\n\t\r\n\r\nLes variables apparues au moins 2 fois sur les 20 regression bootstrap ou dont l’étape moyenne d’apparition est inférieure à 6 sont conservées.\r\nSuite au Bootstrap, 11 variables apparaissent au moins deux fois dans les 20 échantillons bootstrap.  \r\n\r\n\r\nA l’issue de nos sélections Stepwise et Bootstrap, nous choisissons de garder 15 variables.\r\n6.4. Étapes Sélection d’un ou plusieurs modèles\r\n\r\n\r\nL’objectif est de trouver un modèle avec les variables les plus discriminantes parmis notre ensemble de 13 variables. \r\n\r\n\r\nToutes les combinaisons de 1 à 5 variables sont testées (au delà de 5, le temps de calcul devient prohibitif). Sur chacune de ces combinaisons, nous lançons une régression logistique . \r\n\r\n\r\nPour chaque combinaison, un modèle de régression logistique est construit. La comparaison de la qualité des modèles se fait avec les indicateurs suivants :\r\n* la déviance\r\n* le R²\r\n* l’aire sous la courbe ROC pour l’échantillon d’apprentissage\r\n* l’aire sous la courbe ROC du modèle construit sur l’échantillon d’apprentissage et testé sur l’échantillon de test\r\n* la moyenne des statistiques de Wald\r\n* le nombre de statistiques de Wald inférieures à 1,96²\r\n* le degré de liberté, c’est-à-dire le nombre total de modalités dans le modèle\r\nComme il est difficile de trouver une combinaison optimisant simultanément tous ces critères, nous retenons les meilleurs compromis. \r\nIci, l’objectif du test de Wald est de vérifier si chaques prédicteur apporte de l’information supplémentaire. Le prédicteur importe de l’information lorsque sa statistiques de test est inférieures à 1,96² (quantile de la loi normale centrée réduite à 0,95 et au carré). Nous décidons de retenir les modèles uniquement composé de prédicteur apportant de l’information. Nous décidons de choisir les modèles dont toutes les statistiques de Wald sont supérieures à 1,96².\r\nC’est principalement la valeur de l’aire sous la courbe ROC du modèle construit sur l’échantillon d’apprentissage et testé sur l’échantillon de test qui sera observée. Nous considérons aussi le nombre de degrés de liberté des modèles, en évitant un nombre trop élevé synonyme de manque de robustesse et un nombre trop faible synonyme de manque de finesse de la notation (plus difficile à découper en classes de cotation risque).\r\nNous retenons ensuite la variable qui est présente dans la majorité des combinaisons sélectionnées. Nous réitérons les tests précédents, en ajoutant, à chaque nouvelle combinaison de 5 variables, la variable sélectionnée à l'étape précédente et maintenant entrée dans la sélection. \r\n  \r\n  \r\n\r\n\tComparaison du meilleur modèle à 5 variables et du meilleur modèle à 6 variables\r\n\r\n\r\n\tDans notre cas, l’information ajouté avec un modèle à 6 variable n’est pas significatif par rapport au modèle à 5 variables (l’aire sous la courbe ROC du modèle construit sur l’échantillon d’apprentissage et testé sur l’échantillon de test passe de 0,765 à 0,769). Son nombre de degré de liberté est plus important que le modèle à 5 variable ( 10 degré de liberté en plus pour le modèle à 6 variables). Le modèle à 5 variables est donc préféré car il apporte suffisamment d’informations et il est plus robuste.\r\nL’aire sous la courbe ROC du modèle construit sur l’échantillon d’apprentissage et testé sur l’échantillon de test est de 0,765. Cette valeur est légèrement supérieure à l’aire sous la courbe ROC pour l’échantillon d’apprentissage qui est de 0,764. Son indice de Gini sur l’échantillon d’apprentissage vaut 0,53. Il vaut 0,52 sur la base entière.\r\nNotre modèle est ensuite lancé sur la base test qui regarde le passage en défaut des clients sur la période de juin 2018 à juin 2019. L’indice de Gini obtenu est \r\n\r\nDescription des 5 va",
      "approx_tokens": 1000,
      "metadata": {
        "drive_file_id": "1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0",
        "name": "Rapport de Stage Youn Jéhanno",
        "mimeType": "application/vnd.google-apps.document",
        "modifiedTime": "2021-06-10T13:23:55.779Z",
        "webViewLink": "https://docs.google.com/document/d/1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z",
        "title": "Rapport de Stage Youn Jéhanno",
        "uri": "gdrive://1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0"
      }
    },
    {
      "doc_content_hash": "2bd4dfc3a1adad44451831e812b1191c587f82160a47b287a476fef0a222002d",
      "chunk_index": 8,
      "text": " valeur est légèrement supérieure à l’aire sous la courbe ROC pour l’échantillon d’apprentissage qui est de 0,764. Son indice de Gini sur l’échantillon d’apprentissage vaut 0,53. Il vaut 0,52 sur la base entière.\r\nNotre modèle est ensuite lancé sur la base test qui regarde le passage en défaut des clients sur la période de juin 2018 à juin 2019. L’indice de Gini obtenu est \r\n\r\nDescription des 5 variables retenues :\r\n* NBPROD_HORSEP : Nombre de produits hors épargne \r\n  \r\n\r\n* SOL_FIN_MOIS : Solde en fin de mois de tous les produits d’épargne\r\n  \r\n\r\n* SLD_CPTEPA1_6M : Moyenne du solde moyen sur livrets sur 6 mois\r\n  \r\n\r\n* nbjdeb_cum : Cumul du nombre de jours débiteurs\r\n  \r\n\r\n* p1_csp : Code socioprofessionnel\r\n  \r\n\r\n\r\n\r\nCes tableaux permettent d’observer une valeur croissante du taux de défaut en fonction du produit hors épargne et du cumul du nombre de jours débiteurs.\r\nLe taux de défaut en fonction du nombre de produits hors épargne n’est pas linéaire. Il peut remettre en cause le découpage. Le taux de défaut des clients possédant un produit hors épargne est proche du taux de défaut des clients possédant 0 où 2 produits hors épargnes. Un regroupement des modalités pourrait être envisagé dans une future version\r\nD’autre part, le taux de défaut à une valeur décroissante en fonction de la valeur du solde sur livret et des soldes en fin de mois de tous les produits d’épargnes.\r\nDe plus nous pouvons observer que selon la catégorie socioprofessionnelle, le risque de défaut peut être multiplié par 5.\r\n\r\n\r\nDans notre nouveau modèle, seule la valeur du solde sur livret qui est présente dans les deux modèles actuels se retrouve dans le modèle.\r\n6.5. Résultat\r\nLe modèle de régression logistique est calculé sur les variables précédemment définies par la formule suivante : \r\nLOGIT                 =             - 5,67735               \r\n                                     - 1,37607                     si           NBPROD_HORSEP  ≤  0 ou non renseignée\r\n                                    +0                          si           0 < NBPROD_HORSEP ≤  1\r\n                                   + 0,18250                    si           1 < NBPROD_HORSEP ≤  2\r\n                                    + 0,39927                   si           2 < NBPROD_HORSEP ≤  3\r\n                + 0,76042                  si           3 < NBPROD_HORSEP  \r\n                                     - 1,24787                    si           SOL_FIN_MOIS  < 64,1\r\n                                    +  0,99118                   si           64,1 < SOL_FIN_MOIS ≤  803,16\r\n                                   + 0,55623                    si           803,16 < SOL_FIN_MOIS ≤  4054,34\r\n                                    + 0,33262                  si           4054,34 < SOL_FIN_MOIS ≤  11382,19\r\n                                    + 0                          si           11382,19 < SOL_FIN_MOIS ≤  25047,69\r\n                - 0,21347                  si           25047,69 < SOL_FIN_MOIS \r\n                                     + 0,79870                     si           nbjdeb_cum > 0 ou manquant\r\n                                    + 0                          si        nbjdeb_cum ≤  0\r\n                                      + 0,77437                    si           SLD_CPTEPA1_6M  < 404,76\r\n                                    +  0,34317                  si           404,76 < SLD_CPTEPA1_6M ≤  4078,82\r\n                                   + 0,05398                    si           4078,82 < SLD_CPTEPA1_6M ≤  14595,24\r\n                                   + 0                    si           14595,24 < SLD_CPTEPA1_6M\r\n                                    + 0,15753                si           SLD_CPTEPA1_6M manquant\r\n                                    + 0,39780                    si           p1_csp   ∈ {21,23,64,68,65,81,67}\r\n                                    + 0,21288                    si           p1_csp   ∈ {22,35,55,69,83,56,60}\r\n                                   - 0,58768              ",
      "approx_tokens": 1000,
      "metadata": {
        "drive_file_id": "1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0",
        "name": "Rapport de Stage Youn Jéhanno",
        "mimeType": "application/vnd.google-apps.document",
        "modifiedTime": "2021-06-10T13:23:55.779Z",
        "webViewLink": "https://docs.google.com/document/d/1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z",
        "title": "Rapport de Stage Youn Jéhanno",
        "uri": "gdrive://1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0"
      }
    },
    {
      "doc_content_hash": "2bd4dfc3a1adad44451831e812b1191c587f82160a47b287a476fef0a222002d",
      "chunk_index": 9,
      "text": " SLD_CPTEPA1_6M\r\n                                    + 0,15753                si           SLD_CPTEPA1_6M manquant\r\n                                    + 0,39780                    si           p1_csp   ∈ {21,23,64,68,65,81,67}\r\n                                    + 0,21288                    si           p1_csp   ∈ {22,35,55,69,83,56,60}\r\n                                   - 0,58768                    si           p1_csp   ∈ {33,86,34,80,38,84}\r\n                                    - 0,41803                 si           p1_csp   ∈ {37,45,47,54,52}\r\n                                    + 0,08307                    si           p1_csp   ∈ {62,85,63}\r\n                                    + 0                           si           p1_csp   ∈ {11,43,31,48,12,13,46,53}\r\n                                    - 0,67290                   si           p1_csp   ∈ {42,44,70} ou manquant\r\n7. Découpage du score\r\n\r\n\r\nLe score est obtenu sur la base test qui regarde le passage en défaut des clients sur la période de juin 2018 à juin 2019. C’est sur cette base que le découpage du score est effectué.\r\nCette étape a pour but de découper le score obtenu précédemment, qui est une variable continue comprise entre 0 et 1 (probabilité), en 9 classes de cotation (A+, A-, B+, B-, C+, C-, D+, D-, E+) associées à des taux de défaut croissants (de A+ à E+). La valeur des bornes (taux de défaut) des classes de cotation sont des valeurs préexistante.\r\nNotre objectif va être de regrouper nos clients dans chacune des classes de cotation. Pour ce faire, nous calculons dans un premier temps les centiles du score. Puis grâce à l’analyse de ces centiles via des tableaux croisés et des représentations graphiques, nous essayons de former des regroupement de centiles homogène et terme de score qui correspondent aux classes de cotation. Cela revient à essayer d’obtenir des classes de cotation avec des taux de défaut de faible écart-type à l’intérieur de chacune des classes mais différents entre les classes. Ce regroupement se fait manuellement. Le principale critère étant que le taux de défaut du centile médian pour chaque regroupement corresponde avec la valeur du taux de défaut moyen de la classe de cotation.\r\n\r\n\r\n9. Conclusion\r\n\r\n\r\nLe nouveau modèle prédictif des clients Épargnants purs et des clients Compte Créditeur sans prêts possède un indice de Gini de 0,49 sur la base de test pour la période entre juin 2018 et juin 2019. Cet indice de Gini est faible mais il s’explique par une insuffisances en terme de volumétries de défauts rendant une prédiction difficile. Néanmoins, le nouveau modèle obtient une meilleur performance que les deux modèles précédents. Le modèle actuel Épargnants Purs possède un indice de Gini égal à 0,21. Le modèle actuel Compte Créditeur sans prêt possède un indice de Gini égal à 0,51. Le nouveau modèle améliore donc fortement la performance pour le segment Épargnants Purs et légèrement pour le segment Compte Créditeur sans prêt.\r\nLe nouveau modèle sera donc préféré aux deux modèles actuels.\r\nCependant, le nouveau modèle peut encore être amélioré en effectuant un meilleur découpage des variables. Un redécoupage plus large apporterai plus de robustesse et pourrait amener à une meilleure prédiction.\r\n\r\n\r\n8. Compétences développées\r\n\r\n\r\nCe stage m’a permis de développer plusieurs compétences :\r\n\r\n\r\nCapacité de communication\r\n\r\n\r\nLa réalisation de mon stage a nécessité la communication avec les membres de l’équipe du service Modélisation des risques. Toutes les semaines, une présentation au reste de l’équipe de Modélisation SNI a lieu. Cette réunion permet donc de présenter les projets sur lesquels on travaille au reste de l’équipe. C’est un moment d’échange où tout le monde essaye d’aider dans les différents projets.  \r\nPour communiquer mes résultats il m’a aussi fallu les rendre compréhensible pour les autres membres de l’équipe à l’aide de graphes bien choisis.\r\n\r\n\r\nCapacité à se familiariser à un nouvel environnement\r\n\r\n\r\nLe monde bancaire étai",
      "approx_tokens": 1000,
      "metadata": {
        "drive_file_id": "1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0",
        "name": "Rapport de Stage Youn Jéhanno",
        "mimeType": "application/vnd.google-apps.document",
        "modifiedTime": "2021-06-10T13:23:55.779Z",
        "webViewLink": "https://docs.google.com/document/d/1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z",
        "title": "Rapport de Stage Youn Jéhanno",
        "uri": "gdrive://1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0"
      }
    },
    {
      "doc_content_hash": "2bd4dfc3a1adad44451831e812b1191c587f82160a47b287a476fef0a222002d",
      "chunk_index": 10,
      "text": " donc de présenter les projets sur lesquels on travaille au reste de l’équipe. C’est un moment d’échange où tout le monde essaye d’aider dans les différents projets.  \r\nPour communiquer mes résultats il m’a aussi fallu les rendre compréhensible pour les autres membres de l’équipe à l’aide de graphes bien choisis.\r\n\r\n\r\nCapacité à se familiariser à un nouvel environnement\r\n\r\n\r\nLe monde bancaire était un monde qui m’était inconnu avant de faire mon stage. Il m’a donc fallu apprendre beaucoup de nouvelle notions en peu de temps afin de comprendre les données avec lesquels j’ai travaillé. Cela m’a permis de mieux comprendre les problématiques et d’analyser les résultats.\r\n\r\n\r\nCapacité de rigueur \r\n\r\n\r\nMes travaux effectués seront utilisés dans des projets futurs. Mon travail devait donc être soigneusement décrit afin que n’importe quelle personne puisse comprendre ce que j’ai effectué lors de mon stage. Cela m’a appris à structurer mon travail.\r\n\r\n\r\n10. Résumé\r\n\r\n\r\nExigence réglementaire de la BCE dans le cadre des accords Bâlois \r\nLe Crédit Mutuel Arkéa doit\r\n\r\n\r\nDans le cadre de ma formation de 4ème année Génie Mathématiques à l’INSA de Rennes, j’ai effectué un stage d’une durée de 2 mois au siège du Crédit Mutuel Arkéa, à Brest. J’ai intégré le département Modélisation Statistique des risques au sein du service “Modélisation Système de Notation Interne”. Le département est chargé d’évaluer le risque de crédit des clients en fonction de leur profil, le risque opérationnel ainsi que le calcul de l'exigence en fonds propres associée aux risques pris par la banque. A cela, s'ajoutent différentes études statistiques pour la Direction en raison de nombreux et nouveaux projets impliquants pour le Groupe.\r\nMa mission a consisté à créer un nouveau modèle statistique permettant d’obtenir une meilleure prédiction du défaut des clients épargnants purs et des clients avec compte créditeur et sans crédit.\r\n\r\n\r\n11. Annexe\r\n\r\n\r\nRépartition du taux de défaut de chaques modalités pour chaques variables",
      "approx_tokens": 505,
      "metadata": {
        "drive_file_id": "1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0",
        "name": "Rapport de Stage Youn Jéhanno",
        "mimeType": "application/vnd.google-apps.document",
        "modifiedTime": "2021-06-10T13:23:55.779Z",
        "webViewLink": "https://docs.google.com/document/d/1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z",
        "title": "Rapport de Stage Youn Jéhanno",
        "uri": "gdrive://1TZq7H0HVBiSxfTG3eM08mIz_sVREOhhO-8CNUx4u5m0"
      }
    },
    {
      "doc_content_hash": "85f763b97b72aee482148446ca1e726608dc8eb06300d336793b527571ce5a4e",
      "chunk_index": 0,
      "text": "Création d’un modèle \nstatistique de scoring pour \nla modélisation du risque de \ncrédit des particuliers \nYoun Jéhanno, Mars 2020\n1\nÉtapes de la modélisation\nAnalyse de la qualité des données\nAnalyse de la qualité dDéveloppement des modèles\nes donnéesDéveloppement des modèles\nContexte\n● Contexte réglementaire (BCE, DR, équipe \nstatistique)\n● Problématique actuelle\nConstitution de la base de données\nAnalyse de la qualité des données\nDéveloppement des modèles\nDécoupage du score\n● Déﬁnition de la variable cible\n● Déﬁnition du périmètre\n● Construction de la base\n● Préparation des données\n● Exploration des données\n● Redécoupage des variables\n● Recodage des données manquantes\n● Sélection des variables discriminantes (ACP/ V \nde Cramer)\n● Régression logistique\n● Sélection du modèle\n● Découpage du score\n● Conclusion\n2\nContexte\n■ Stage de 2 mois réalisé dans le Service Modélisation SNI, au sein de la Direction des Risques\n■ Modèles statistiques : exigence réglementaire de la BCE dans le cadre des accords Bâlois (une \nmaîtrise complète des risques bancaires)\n■ Dans un contexte de volonté d’indépendance du Groupe Arkéa, l'objectif est de refondre \nl’ensemble des modèles du système de notation interne\n■ Problématique actuelle\n● Particuliers découpés en 11 segments \n(algorithmes spécifiques ou cotation forfaitaire)\n● Performance insuffisante des modèles\nde prévision des risques de crédit des \nParticuliers sur les segments 10 et 11\n■ Objectif du stage \n● Créer un nouveau modèle statistique regroupant les \nsegments 10 et 11 et permettant d’obtenir une \nmeilleure prédiction du risque \nSegments étudiés 3\nÉtapes de la modélisation\nAnalyse de la qualité des données\nAnalyse de la qualité dDéveloppement des modèles\nes donnéesDéveloppement des modèles\nContexte\n● Contexte réglementaire (BCE, réglementaire, \nDR, équipe statistique)\n● Problématique actuelle\nConstitution de la base de données\nAnalyse de la qualité des données\nDéveloppement des modèles\nDécoupage du score\n● Déﬁnition de la variable cible\n● Déﬁnition du périmètre\n● Construction de la base\n● Préparation des données\n● Exploration des données\n● Redécoupage des variables\n● Recodage des données manquantes\n● Sélection des variables discriminantes (ACP/ V \nde Cramer)\n● Régression logistique\n● Sélection du modèle\n● Découpage du score\n● Conclusion\n4\nvariable cible\n■ Variable à prédire : risque (critère réglementaire), identifié par les événements risques suivants\n● Créances douteuses  (présentant un risque probable ou certain de non recouvrement total ou partiel d’une créance. \nEx :  impayé depuis 90j dans le cadre des particuliers)\n● Contentieux (reflet de la situation d’un client dont la situation critique nécessite l’intervention de la maîtrise des \nrisques ou des services juridiques du Crédit Mutuel Arkéa)\n● Incidents de paiement, Surendettement, Interdit judiciaire, Liquidation judiciaire, etc …\n■ En terme informatique, la variable cible vaut :\n● 1 : si survenance d’un des risques ci-dessus dans les 12 mois.\n● 0 : sinon\n■ Les clients non sains en début de période sont exclus de la base de modélisation\nObjectif de l’étude : mesurer la probabilité de survenance d’un risque au cours des 12 prochains \nmois\n5\nConstitution de la base de données\n■ Périmètre défini pour l’étude :\n● Entités : CMB, CMSO, Fortuneo\n■ Dates de la base de modélisation\n● Base de décembre 2015 avec risque observé sur l’année 2016\n● Base de juin 2017 avec risque observé jusqu’à juin 2018 \n■ Données exploitées :\n● Données des clients présents dans l’entrepôt de données Arkéa \n(données socio-économiques, données de fonctionnement de compte, risques…)\nDéﬁnition du périmètre\n■ Dates de la base de test\n● Base de juin 2018 avec risque \nobservé jusqu’à juin 2019 \nn ~ 1 600 000 observations\n312 variables\n6\nConstitution de la base de données\n■ R : package interne ModelSNI\n● Permet d’harmoniser l’utilisation des différentes fonctions nécessaires à la modélisation sous R\n■ Maquette Excel : mise en forme des résultats de modélisation pou",
      "approx_tokens": 1000,
      "metadata": {
        "drive_file_id": "1chkov0A3gKCQGhUwQd-gOlSYfwfyDZISBGRdLqPEQ48",
        "name": "Modélisation Statistique Risque",
        "mimeType": "application/vnd.google-apps.presentation",
        "modifiedTime": "2020-03-04T14:58:42.508Z",
        "webViewLink": "https://docs.google.com/presentation/d/1chkov0A3gKCQGhUwQd-gOlSYfwfyDZISBGRdLqPEQ48/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z",
        "title": "Modélisation Statistique Risque",
        "uri": "gdrive://1chkov0A3gKCQGhUwQd-gOlSYfwfyDZISBGRdLqPEQ48"
      }
    },
    {
      "doc_content_hash": "85f763b97b72aee482148446ca1e726608dc8eb06300d336793b527571ce5a4e",
      "chunk_index": 1,
      "text": "mpte, risques…)\nDéﬁnition du périmètre\n■ Dates de la base de test\n● Base de juin 2018 avec risque \nobservé jusqu’à juin 2019 \nn ~ 1 600 000 observations\n312 variables\n6\nConstitution de la base de données\n■ R : package interne ModelSNI\n● Permet d’harmoniser l’utilisation des différentes fonctions nécessaires à la modélisation sous R\n■ Maquette Excel : mise en forme des résultats de modélisation pour les étapes suivantes :\n● Exploration des données\n● Recodage des données manquantes\n● Sélection des variables\n● Construction du modèle\n● Calibrage du modèle\nOutils utilisés\n7\nÉtapes de la modélisation\nAnalyse de la qualité des données\nAnalyse de la qualité dDéveloppement des modèles\nes donnéesDéveloppement des modèles\nContexte\n● Contexte réglementaire (BCE, réglementaire, \nDR, équipe statistique)\n● Problématique actuelle\nConstitution de la base de données\nAnalyse de la qualité des données\nDéveloppement des modèles\nDécoupage du score\n● Déﬁnition de la variable cible\n● Déﬁnition du périmètre\n● Construction de la base\n● Préparation des données\n● Exploration des données\n● Redécoupage des variables\n● Recodage des données manquantes\n● Sélection des variables discriminantes (ACP/ V \nde Cramer)\n● Régression logistique\n● Sélection du modèle\n● Découpage du score\n● Conclusion\n8\nAnalyse de la qualité des données\n■ Recodage de certaines variables\n● ex : Les données manquantes (999999 ou 0) sont recodées à NA\n■ Construction de nouvelles variables \n● ex : Moyenne sur 6 mois des données de fonctionnement de compte afin de capter des tendances sur \nplusieurs mois et éviter ainsi une trop grande sensibilité aux évènements exceptionnels et aux \nvariations saisonnières\n■ Problématique rencontrée\n● Individus absents dans certaines bases de données.\nExemple : - Épargnants purs ne disposant pas de comptes courants\n- Clients récents n’ayant pas d’informations bancaire sur les premiers \nmois entraînant des valeurs manquantes lors de la création de moyennes.\nPréparation des données\nn ~ 1 600 000 observations\n312 variables\n9\nAnalyse de la qualité des données\n■ Objectif de l’exploration :\n● Repérer les variables constantes, manquantes, redondantes\n■ Maquette d’exploration des données quantitatives par entité\nExploration des données\n■ Maquette d’exploration des données qualitatives par entité\n■ Résultats : L’exploration a permis de supprimer 194 variables \nn ~ 1 600 000 observations\n118 variables\n10\nAnalyse de la qualité des données\n■ Objectif :\n● Répartir la population en groupes homogènes, avec des variables discriminantes plus performantes.\n■ Algorithme CHAID\n● Méthode d’arbre de décision qui permet de discrétiser, en 2 classes ou plus, les variables en les \ndécoupant en déciles puis en utilisant le critère du χ2 pour regrouper les classes les plus proches en \nterme de taux de risque\n● Variables continues : discrétisation\n➢ Ex : Nombre produit hors épargne : [-Inf,0] ; (0,1] ; (1,2] ; (2,3] ; (3, Inf]\n● Variables qualitatives : regroupement des modalités\n➢ Ex : Regroupement de certaines catégories socioprofessionnelles\n■ Résultat : \n● 117 variables sont créées\n● Si une variable a été redécoupée, on ne conserve pas la variable d’origine\n \nRedécoupage des variables (CHAID) \n11\nAnalyse de la qualité des données\n■ Objectif :\n● Exploiter les données manquantes\n■ Regroupement modalité valeurs manquantes, \n2 cas : \n● Création modalité valeurs manquantes\n(proportion > 10%)\n● Affectation à une modalité \n(proportion < 10%)\n● Affectation logique bancaire\n■ Résultat :\n● 60 variables conservent les valeurs manquantes comme une modalité à part entière\n● Pour 36 variables, les valeurs manquantes sont affectées à une modalité\nRecodage des données manquantes\nMaquette de recodage des valeurs manquantes\n12\nÉtapes de la modélisation\nAnalyse de la qualité des données\nAnalyse de la qualité dDéveloppement des modèles\nes donnéesDéveloppement des modèles\nContexte\n● Contexte réglementaire (BCE, réglementaire, \nDR, équipe statistique)\n● Problématique actuelle\nConstitution de la base de",
      "approx_tokens": 1000,
      "metadata": {
        "drive_file_id": "1chkov0A3gKCQGhUwQd-gOlSYfwfyDZISBGRdLqPEQ48",
        "name": "Modélisation Statistique Risque",
        "mimeType": "application/vnd.google-apps.presentation",
        "modifiedTime": "2020-03-04T14:58:42.508Z",
        "webViewLink": "https://docs.google.com/presentation/d/1chkov0A3gKCQGhUwQd-gOlSYfwfyDZISBGRdLqPEQ48/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z",
        "title": "Modélisation Statistique Risque",
        "uri": "gdrive://1chkov0A3gKCQGhUwQd-gOlSYfwfyDZISBGRdLqPEQ48"
      }
    },
    {
      "doc_content_hash": "85f763b97b72aee482148446ca1e726608dc8eb06300d336793b527571ce5a4e",
      "chunk_index": 2,
      "text": "urs manquantes sont affectées à une modalité\nRecodage des données manquantes\nMaquette de recodage des valeurs manquantes\n12\nÉtapes de la modélisation\nAnalyse de la qualité des données\nAnalyse de la qualité dDéveloppement des modèles\nes donnéesDéveloppement des modèles\nContexte\n● Contexte réglementaire (BCE, réglementaire, \nDR, équipe statistique)\n● Problématique actuelle\nConstitution de la base de données\nAnalyse de la qualité des données\nDéveloppement des modèles\nDécoupage du score\n● Déﬁnition de la variable cible\n● Déﬁnition du périmètre\n● Construction de la base\n● Préparation des données\n● Exploration des données\n● Redécoupage des variables\n● Recodage des données manquantes\n● Sélection des variables discriminantes (ACP/ V \nde Cramer)\n● Régression logistique\n● Sélection du modèle\n● Découpage du score\n● Conclusion\n13\nDéveloppement des modèles \n■ Objectif\n● Diminuer le nombre de variables pour permettre la réalisation d’une régression logistique\n■ V de Cramer\n● Évalue la liaison entre les variables \ndu modèle et la variable cible\n■ ACP\n● Création de groupes de variables homogènes en \nterme de corrélation\n● Au total : 25 composantes principales\n■ Critères de sélection\n● Variable avec le plus fort V de Cramer \npour chaque composante principale\n● V de Cramer > 0,03\n● Variables modèles actuels\nSélection des variables discriminantes\n■ Résultat\n● Ces méthodes ont permis de retenir les \n28 variables les plus discriminantes \nparmi les 118 sélectionnées à l’étape \nprécédente\nn ~ 1 600 000 \nobservations\n28 variables\n14\n\nDéveloppement des modèles \n■ Objectif \n● Supprimer les variables contenant la même information\n■ Test de Corrélation \n● Corrélation > 0,80\n■ Critères de sélection\n● Variable avec le plus fort V de Cramer\n■ Résultat:\n● Cette étude permet de supprimer 7 variables et de garder 21 variables tout en \npréservant la diversité de l’information\nSélection des variables discriminantes\nTest de corrélation entre 4 variables présélectionnées\nn ~ 1 600 000 observations\n21 variables\n15\nDéveloppement des modèles \n■ Objectif :\n● La régression Stepwise consiste à ajouter itérativement des prédicteurs, afin de trouver le \nsous-ensemble de variables résultant en le modèle le plus performant\n■ Régression Stepwise\n● Base complète\n● Base d’apprentissage\n● 20 échantillons bootstrap\n■ Sélection des variables\n● Etape moyenne d’apparition inférieure à 10 \n● Apparues au moins 2 fois dans les 20 régressions\n■ Résultat :\n● La régression stepwise sur la base complète et la base d’apprentissage permet de \nsélectionner 13 variables\n● Les régression stepwise sur les 20 échantillons bootstrap font ressortir 2 nouvelles variables\nRestriction des variables discriminantes\nn ~ 1 600 000 observations\n15 variables\nBootstrap sur échantillon d’apprentissage\n16\nDéveloppement des modèles \n■ Test combinaisons de 5 variables\n● Critère de sélection\n➢ Aucune Statistique de Wald inférieures à 1,96², assure \nla performance des variables du modèle  \n➢ Aire sous la courbe ROC\n➢ Degré de liberté\n■ Test combinaisons de 5 variables avec 1 variable \nprésélectionnée \n● Évite de tester toutes les combinaisons de 6 variables trop chronophage (~6h pour 5 variables)\n● Variable présélectionnée : présente dans la majorité des combinaisons précédentes, ici, \ndecp_NBPROD_HORSEP (nombre de produits hors épargne)\n● Mêmes critères de sélection\n■ Résultat :\n● Le meilleur modèle à 6 variables n’augmente pas significativement la qualité. C’est le meilleur modèle \nà 5 variables qui est retenu. Son indice de Gini sur l’échantillon test vaut 0,53 (acceptable).\nSélection du modèle\nComparaison du meilleur modèle à 5 variables et du \nmeilleur modèle à 6 variables\n17\nDéveloppement des modèles \nVariables sélectionnées\n■ Observation\n● Les graphes des variables 1 et 2 montrent que plus le solde sur livret \net des produits d’épargne est faible et plus le client a des chances \nd’avoir une survenance de risques\n● Selon la catégorie socioprofessionnelle, le taux de survenance de \nrisques peut être multipli",
      "approx_tokens": 1000,
      "metadata": {
        "drive_file_id": "1chkov0A3gKCQGhUwQd-gOlSYfwfyDZISBGRdLqPEQ48",
        "name": "Modélisation Statistique Risque",
        "mimeType": "application/vnd.google-apps.presentation",
        "modifiedTime": "2020-03-04T14:58:42.508Z",
        "webViewLink": "https://docs.google.com/presentation/d/1chkov0A3gKCQGhUwQd-gOlSYfwfyDZISBGRdLqPEQ48/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z",
        "title": "Modélisation Statistique Risque",
        "uri": "gdrive://1chkov0A3gKCQGhUwQd-gOlSYfwfyDZISBGRdLqPEQ48"
      }
    },
    {
      "doc_content_hash": "85f763b97b72aee482148446ca1e726608dc8eb06300d336793b527571ce5a4e",
      "chunk_index": 3,
      "text": " modèle à 5 variables et du \nmeilleur modèle à 6 variables\n17\nDéveloppement des modèles \nVariables sélectionnées\n■ Observation\n● Les graphes des variables 1 et 2 montrent que plus le solde sur livret \net des produits d’épargne est faible et plus le client a des chances \nd’avoir une survenance de risques\n● Selon la catégorie socioprofessionnelle, le taux de survenance de \nrisques peut être multiplié par 5\nVariable 1\nVariable 2\nVariable 3\n18\n\nDéveloppement des modèles \nVariables sélectionnées\n■ Observation\n● Variable 4 : Les clients ayant au moins 1 jour débiteur sur 6 mois ont beaucoup plus de chance d’avoir une \nsurvenance de risques\n● Variable 5 : Le nombre de produit hors épargne augmente le taux de survenance de risques\n● Remarque : Le taux de survenance de risques en fonction du nombre de produits hors épargne n’est pas linéaire. \nIl peut remettre en cause le découpage qui aurait pu être large de façon à respecter la tendance croissante du \ntaux de survenance de risques\nVariable 4 \nVariable 5\n19\nÉtapes de la modélisation\nAnalyse de la qualité des données\nAnalyse de la qualité dDéveloppement des modèles\nes donnéesDéveloppement des modèles\nContexte\n● Contexte réglementaire (BCE, réglementaire, \nDR, équipe statistique)\n● Problématique actuelle\nConstitution de la base de données\nAnalyse de la qualité des données\nDéveloppement des modèles\nDécoupage du score\n● Déﬁnition de la variable cible\n● Déﬁnition du périmètre\n● Construction de la base\n● Préparation des données\n● Exploration des données\n● Redécoupage des variables\n● Recodage des données manquantes\n● Sélection des variables discriminantes (ACP/ V \nde Cramer)\n● Régression logistique\n● Sélection du modèle\n● Découpage du score\n● Conclusion\n20\nDécoupage du score\n■ Objectif :\n● Découper le score, en 9 classes de cotation associées à des taux de risque croissants (de A+ à E+)\n● Outil de comparaison des segments Particuliers\n■ Score calculé sur la base de test (Juin 2018 à Juin 2019)\n■ Etapes : \n● Découpage du score en centile\n● Regroupement et affectation des centiles aux classes de \ncotation de manière à ce que le centile médian des \nregroupements de centiles soit proche de la moyenne \nde la classe de cotation \n■ Résultat :\n● Score découpé de B+ à D-\nDécoupage du score\nMaquette découpage du score \npour 1 centile sur 10\nGrille de cotation unique \npour les particuliers\n21\nConclusion\n■ Comparaison avec les modèles actuels :\n● Indice de Gini sur période Juin 2018 à Juin 2019:\n● ⅓ Épargnants purs et ⅔ Compte Créditeur sans prêt → Amélioration globale\n■ Ouverture :\n● Découpage plus large des modalités lors du CHAID → Gagner en robustesse\n■ Plan personnel :\n● Découverte du monde de l’entreprise et bancaire\n● Capacité à s’intégrer dans un environnement nouveau\n22\nROC  de la base de test \nOral pour l’INSA\n● conseils : \n○ essayer de ne pas lire et de rajouter des commentaires à l’oral\n○ préparer des justiﬁcations des méthodes scientiﬁques\n○ lire les commentaires en bas de chaques diapositives qui aident pour l’oral (pourquoi avoir \nchoisi de faire une ACP pour des variables qualitatives)\n23",
      "approx_tokens": 771,
      "metadata": {
        "drive_file_id": "1chkov0A3gKCQGhUwQd-gOlSYfwfyDZISBGRdLqPEQ48",
        "name": "Modélisation Statistique Risque",
        "mimeType": "application/vnd.google-apps.presentation",
        "modifiedTime": "2020-03-04T14:58:42.508Z",
        "webViewLink": "https://docs.google.com/presentation/d/1chkov0A3gKCQGhUwQd-gOlSYfwfyDZISBGRdLqPEQ48/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z",
        "title": "Modélisation Statistique Risque",
        "uri": "gdrive://1chkov0A3gKCQGhUwQd-gOlSYfwfyDZISBGRdLqPEQ48"
      }
    },
    {
      "doc_content_hash": "c15c1f661c58b476831cc175bf52ba64f582f34f4225cedff22a2362944466fc",
      "chunk_index": 0,
      "text": "﻿________________\r\n\r\nINSA Rennes\r\nMÉMOIRE DE STAGE\r\nCréation d’un modèle statistique de scoring pour la modélisation du risque de crédit des particuliers \r\n\r\n  \r\n\r\n\r\n\r\nMémoire CONFIDENTIEL\r\n\r\n\r\n4ème année Génie Mathématiques\r\nINSA Rennes\r\nNom de l’entreprise : Crédit Mutuel ARKEA\r\nResponsable de stage : Guillaume Le Coz\r\nEnseignant référent : Loïc Hervé\r\nDate du stage : 13 Janvier au 6 Mars 2020\r\n\r\n\r\n\r\n\r\n\r\n\r\n________________\r\n  \r\n________________\r\n\r\n\r\n\r\nSommaire\r\n\r\n\r\nRemerciements\r\nNote de Confidentialité\r\n1. Introduction\r\n2. Présentation de l’entreprise\r\n2.1. Groupe Arkéa\r\n2.1.1 Historique et organisation\r\n2.1.2 Quelques chiffres\r\n2.1.3 Souhait d’indépendance d’Arkéa\r\n2.1.4 Organisation de la direction des risques et du département Modélisation Statistique des Risques\r\n3. Environnement de travail\r\n3.1. Mise en situation\r\n3.1.1 Définition population observée\r\n3.1.2 Problématique actuelle\r\n3.2. Environnement de travail\r\n3.3.Description des bases de données utilisées\r\n4. Constitution de la base de données\r\n4.1. Détermination de la variable cible\r\n4.2. Exploration des données\r\n4.3. Définition du périmètre\r\n4.4. Construction de la base\r\n5. Analyse de la qualité des données\r\n5.1. Préparation des données\r\n5.2. Maquette d’exploration des données\r\n5.3. Redécoupage des variables (CHAID)\r\n5.4. Recodage des données manquantes\r\n6. Développement des modèles\r\n6.1. Sélection des variables discriminantes\r\n6.1.1. ACP\r\n6.1.2. V de Cramer\r\n6.1.3. Test de corrélation\r\n6.2. Echantillonnage\r\n6.3. Restriction des variables discriminantes - Stepwise & Bootstrap\r\n8.3.1 Forward step\r\n6.3.2 Bootstrap\r\n6.4. Sélection d’un ou plusieurs modèles\r\n6.5. Résultat\r\n6.6. Analyse des résultats\r\n7. Découpage du score\r\n7.1. Construction et validation du modèle\r\n7.1.1 Courbe ROC\r\n7.1.2 Indice de Gini\r\n8. Compétences développées\r\n\r\n\r\n________________\r\n\r\n\r\nRemerciements\r\n\r\n\r\nJe tiens à remercier toutes les personnes qui ont contribué au bon déroulement de mon stage et qui m'ont aidé lors de la rédaction de ce rapport.\r\n\r\n\r\nTout d'abord, j'adresse mes remerciements à mon maître de stage et ma responsable de service, Guillaume Le Coz et Audrey Ladan pour leur accueil et pour m’avoir apporté leur expérience du monde bancaire tout au long de ce stage. Leurs relectures fut d’une aide précieuse pour l’écriture de mon rapport de stage.\r\n\r\n\r\nJe tiens également à remercier l’ensemble du département Modélisation statistiques des risques pour leur gentillesse lorsque je leur posait des questions et leur bonne humeur lors des pauses café.\r\n\r\n\r\nNote de Confidentialité\r\nCe présent rapport ainsi que toutes les informations qu’il contient sont strictement confidentiels exclusivement au corps professoral de l’INSA de Rennes ou à une personne interne à l’équipe statistique du département modélisation statistique des risques.\r\nLes chiffres présents dans ce rapport ont été volontairement faussés toujours dans cet objectif de confidentialité. Ils restent toutefois cohérents.\r\nMerci de bien vouloir respecter la confidentialité de ce document.\r\n________________\r\n\r\n\r\n\r\n\r\n1. Introduction\r\n\r\n\r\nDans le cadre de ma formation de 4ème année Génie Mathématiques à l’INSA de Rennes, j’ai effectué un stage d’une durée de 2 mois au siège du Crédit Mutuel Arkéa, à Brest. J’ai intégré le département Modélisation Statistique des risques au sein du service “Modélisation Système de Notation Interne”. Le département est chargé d’évaluer le risque de crédit des clients en fonction de leur profil, le risque opérationnel ainsi que le calcul de l'exigence en fonds propres associée aux risques pris par la banque. A cela, s'ajoutent différentes études statistiques pour la Direction en raison de nombreux et nouveaux projets impliquants pour le Groupe.\r\nMa mission a consisté à créer un nouveau modèle statistique permettant d’obtenir une meilleure prédiction de la survenance de risques des clients épargnants purs et des clients avec compte créditeur et sans crédit.\r\n2. Présentation de l’entreprise\r\n2.1. Groupe Arkéa\r",
      "approx_tokens": 1000,
      "metadata": {
        "drive_file_id": "16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY",
        "name": "Copie de Rapport de Stage Youn Jéhanno",
        "mimeType": "application/vnd.google-apps.document",
        "modifiedTime": "2020-03-03T16:31:27.961Z",
        "webViewLink": "https://docs.google.com/document/d/16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z",
        "title": "Copie de Rapport de Stage Youn Jéhanno",
        "uri": "gdrive://16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY"
      }
    },
    {
      "doc_content_hash": "c15c1f661c58b476831cc175bf52ba64f582f34f4225cedff22a2362944466fc",
      "chunk_index": 1,
      "text": " s'ajoutent différentes études statistiques pour la Direction en raison de nombreux et nouveaux projets impliquants pour le Groupe.\r\nMa mission a consisté à créer un nouveau modèle statistique permettant d’obtenir une meilleure prédiction de la survenance de risques des clients épargnants purs et des clients avec compte créditeur et sans crédit.\r\n2. Présentation de l’entreprise\r\n2.1. Groupe Arkéa\r\n2.1.1 Historique et organisation\r\nLe Groupe ARKÉA est un groupe coopératif et mutualiste qui comprend plus de 20 filiales spécialisées qui couvrent tous les métiers de la banque, de la finance et de l'assurance, comme Fortuneo (banque en ligne), Suravenir Assurances, Federal Finance (gestion d'actifs pour compte de tiers), et Financo (crédit à la consommation). Son siège social est implanté au Relecq-Kerhuon dans le Finistère.\r\nIl est né de l'alliance des fédérations de Crédit Mutuel de Bretagne et du Sud-Ouest (CMB, CMSO). C’est le deuxième pôle régional du Crédit Mutuel. Ancré sur ses territoires et comptant près de 10 000 salariés. Il regroupe un grand nombre de fonctions supports et de métiers d'expertise (Finance, Organisation, Audit, Marketing, Juridique,...).\r\n2.1.2 Quelques chiffres\r\nLe Groupe ARKÉA en 2020 représente :\r\n❏     4,5 millions de clients\r\n❏     10 500 salariés\r\n❏     111.2 millions d'euros encours d'épargne (épargne financière, épargne assurance, dépôts).\r\n❏     6.7 milliards d'euros de fonds propres\r\n❏     437 millions d'euros de résultat net\r\n❏     135 milliards d’euros de bilan comptable \r\n2.1.3 Contexte d’indépendance\r\nDepuis 2015, le groupe Arkéa souhaite devenir indépendant du Crédit Mutuel. Cette désaffiliation est prévue pour janvier 2021. \r\nDans ce cadre, il a été engagé des travaux préparatoires de refonte complète des modèles statistiques (aujourd’hui réalisés sur l’ensemble du Crédit Mutuel) sur la population Arkéa.\r\nL’objectif de ces travaux est de prouver à la Banque Centrale Européenne qu’elle est capable de fonctionner indépendamment.\r\n\r\n\r\nCe stage s’inscrit dans ce contexte.\r\n2.1.4 Organisation de la direction des risques et du département Modélisation Statistique des Risques\r\nLe Groupe ARKÉA est divisé en différentes directions avec leurs activités et objectifs propres. L’une des directions, la direction des risques, est celle dans laquelle j’ai effectué mon stage. Elle est composée de 130 collaborateurs dont les principales missions sont la mise en place et le pilotage du dispositif global de gestion des risques du groupe.\r\nLe département Modélisation Statistique des Risques est composé d’une dizaine de statisticiens/data scientists. Les activités principales des différents services au sein du département sont les suivants :\r\n❏        Service Gestion de la donnée et modélisation du risque opérationnel\r\n●           Construire, suivre et maintenir les bases de données clients, métiers, IT,…  servant aux études et à la modélisation statistiques du Groupe et des filiales.\r\n●          Contribuer à la cartographie globale des risques du Groupe et en déduire un état de reporting à des fins de communication aux organes dirigeants.\r\n●     Elaborer et quantifier des modèles de risques opérationnels dans le cadre du calcul de l’exigence en fonds propres.\r\n●         Détecter et contribuer à la prévention d’incidents opérationnels via la création de modèles de comportement de détection de fraude externe.\r\n❏        Service « Modélisation Système de Notation Interne »\r\n●     Créer et assurer la maintenance des modèles de notation interne statistiques (pour les fédérations ou les filiales) en réalisant des études statistiques de comportement des clients en termes de risques de crédit.\r\n●           Rédiger des cahiers des charges de cotation à destination de l'informatique et assurer la coordination de la mise en place des nouveaux algorithmes de notation (recette, assistance de second niveau, suivi des modèles etc…), en lien avec l'informatique et toutes les structures du Groupe impliquées dans la maîtrise des",
      "approx_tokens": 1000,
      "metadata": {
        "drive_file_id": "16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY",
        "name": "Copie de Rapport de Stage Youn Jéhanno",
        "mimeType": "application/vnd.google-apps.document",
        "modifiedTime": "2020-03-03T16:31:27.961Z",
        "webViewLink": "https://docs.google.com/document/d/16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z",
        "title": "Copie de Rapport de Stage Youn Jéhanno",
        "uri": "gdrive://16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY"
      }
    },
    {
      "doc_content_hash": "c15c1f661c58b476831cc175bf52ba64f582f34f4225cedff22a2362944466fc",
      "chunk_index": 2,
      "text": "istiques de comportement des clients en termes de risques de crédit.\r\n●           Rédiger des cahiers des charges de cotation à destination de l'informatique et assurer la coordination de la mise en place des nouveaux algorithmes de notation (recette, assistance de second niveau, suivi des modèles etc…), en lien avec l'informatique et toutes les structures du Groupe impliquées dans la maîtrise des risques et dans la qualité des données en amont de la cotation (direction des engagements, filiales …).\r\n❏     Service « Paramètres et provisions du risque de crédit »\r\n●         Procéder statistiquement aux calculs de paramètres selon les normes bâloise ou comptable IFRS 9 (Probabilité de défaut - PD, Pertes en cas de défaut - LGD et Facteur de conversion d’encours hors bilan en bilan - CCF), afin de permettre les calculs de l’exigence de fonds propres au titre du risque de crédit et du niveau de provisionnement du groupe.\r\n●          Accompagner le process par le suivi et la maintenance des modèles de protection (bâlois et provisionnement), en lien avec l'informatique et toutes les structures du Groupe impliquées dans la maîtrise des risques et dans la qualité des données en amont des systèmes de détermination des paramètres (direction des engagements, filiales …).\r\n3. Environnement de travail\r\n3.1. Mise en situation\r\n3.1.1 Définition population observée\r\nActuellement, les clients particuliers (personnes physiques) sont répartis en 11 segments distincts avec l’ordre des priorisation ci-dessous:\r\n  \r\n\r\nLes 6 segments 19,18,17 et 15 ont des cotations forfaitaires (même note attribuée à tous les membres de chaque segment) basées sur une étude des taux de survenance de risques observés.\r\nLes 5 autres segments possèdent un algorithme statistiques spécifique qui attribue une cotation à ses membres. L’étude se concentre sur les segments 10 et 11.\r\nÉpargnant pur : clients sans prêt, sans risque en cours, qui n’est pas nouveau client et qui n’a    pas de compte courant.\r\nCompte courant créditeur sans prêt : clients sans prêt, sans risque en cours et qui n’est pas nouveaux client avec au moins un compte courant et avec au maximum un jour de débit sur les 6 mois d’observation.\r\n3.1.2 Problématique actuelle\r\nLa segmentation actuelle des populations pourrait être améliorée en regroupant les deux segments, « Épargnants purs » et « CC créditeur », qui présentent un faible nombre de survenance de risques et des similarités d’un point de vue comportement et survenance de risques.\r\nActuellement, le modèle Épargnants Purs présente des insuffisances en terme de performance lié à ses faibles volumétries.\r\n\r\nMa mission a été de regrouper ces deux types de clients en un seul segment afin de voir si une meilleure prédiction globale serait réalisable.\r\n3.2. Environnement de travail\r\n\r\n\r\nLes bases de données utilisées sont issues de la BNS (Base Nationale Statistique) et c’est le logiciel R qui a servi à construire le modèle.\r\n\r\n\r\nL’étude des données a principalement été réalisée à partir de maquettes excel.\r\n\r\n\r\nPour ce projet, la majorité des fonctions utilisées sont issues d’un package interne ModelSNI regroupant un ensemble de syntaxes pour la construction d’un modèle de cotation.\r\n3.3.Description des bases de données utilisées\r\n\r\n\r\nLe modèle a été construit à partir des 8 bases de données suivantes :\r\n  \r\n\r\n4. Constitution de la base de données\r\n4.1. Détermination de la variable cible\r\n\r\n\r\nLa variable cible correspond à la survenance de risque (appelée top_def). Elle vaut :\r\n\r\n\r\n* 1, si le client était sain en début de période d’observation de la survenance de risques et a eut une survenance de risques au cours de la période (qui dure 12 mois);\r\n* 0, si le client était sain en début de période et est resté sain pendant toute la période.\r\n4.2. Exploration des données\r\n\r\n\r\nAvant de commencer à manipuler les bases de données, il est important de se familiariser avec les données et de comprendre les variables. Un fichier Excel décrit l’ensemble des variabl",
      "approx_tokens": 1000,
      "metadata": {
        "drive_file_id": "16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY",
        "name": "Copie de Rapport de Stage Youn Jéhanno",
        "mimeType": "application/vnd.google-apps.document",
        "modifiedTime": "2020-03-03T16:31:27.961Z",
        "webViewLink": "https://docs.google.com/document/d/16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z",
        "title": "Copie de Rapport de Stage Youn Jéhanno",
        "uri": "gdrive://16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY"
      }
    },
    {
      "doc_content_hash": "c15c1f661c58b476831cc175bf52ba64f582f34f4225cedff22a2362944466fc",
      "chunk_index": 3,
      "text": " risques et a eut une survenance de risques au cours de la période (qui dure 12 mois);\r\n* 0, si le client était sain en début de période et est resté sain pendant toute la période.\r\n4.2. Exploration des données\r\n\r\n\r\nAvant de commencer à manipuler les bases de données, il est important de se familiariser avec les données et de comprendre les variables. Un fichier Excel décrit l’ensemble des variables utilisées et facilite la compréhension de la base de données. Cette phase d’exploration va aussi permettre de repérer d'éventuelles valeurs aberrantes ainsi que des valeurs manquantes\r\n4.3. Définition du périmètre\r\n\r\n\r\nIl est important de définir le périmètre de l’étude pour travailler avec les bonnes données.\r\n\r\n\r\n* Périmètre définit pour l’étude : \r\n   * CMB (Crédit Mutuel de Bretagne), CMSO (Crédit Mutuel du Sud Ouest) et Fortunéo\r\n   * Segment 10 et 11, Épargnant pur et Compte courant créditeur sans prêt\r\n\r\n\r\n* Horizon de prédiction : 1 an\r\n\r\n\r\n* Dates de la base de modélisation :\r\n   * Base de modélisation : concaténation de 2 bases ( Ce choix ne garantit pas l’indépendance de nos observations mais permet d’avoir une base de données plus importante et évite les effets de saisonnalité)\r\n      * Base à décembre 2015 avec avec survenance de risques observé sur l’année 2016\r\n      * Base à juin 2017 avec avec survenance de risques observé entre juin 2016 et juin 2017\r\n\r\n\r\n\r\n\r\n  \r\n\r\n\r\n\r\n4.4. Construction de la base\r\n\r\n\r\nLa base est construite à partir des bases de décembre 2015 et de juin 2017, elles mêmes construites grâce à la jointure de l’ensemble des bases décrites dans le tableaux précédent.\r\n\r\n\r\nCette partie est très importante et chronophage car il faut que la base soit la plus complète et la plus fiable possible afin de pouvoir obtenir des résultat plus satisfaisants. \r\nIl a fallu faire face à de nombreux problèmes comme des individus manquants dans certaines bases. Les bases étant de plus très volumineuses, il est d’autant plus difficiles de les trouver.\r\nLa vérification des bases de données à chaques étapes de conception est donc très importante.\r\n\r\n\r\nLa base construite est alors constituée de 1 600 000 observations et de 312 variables.\r\n\r\n\r\n5. Analyse de la qualité des données\r\n5.1. Préparation des données\r\n\r\n\r\nLa préparation des données est composée de deux parties :\r\n\r\n\r\n* Recodage de certaines variables\r\n   * ex : Les données manquantes en (9999999 ou 0) sont recodées en NA\r\n       Les dates < 1901 ou égales à 0 sont recodées à NA car il s’agit d’une\r\n      codification liées aux valeurs manquantes\r\n       Les variables avec >100 modalités sont recodées afin de pouvoir appliquer les \r\n      méthodes statistiques de sélection de variables\r\n       Les valeurs avec des signes aberrants afin de s’assurer une bonne qualité des \r\n      données\r\n\r\n\r\n* Construction de nouvelles variables (moyennes, région) :\r\n   * 26 variables calculant la moyenne sur 6 mois de variables relatives aux comportements bancaires\r\n   * 2 variables indiquant la région d’habitation des tiers\r\n   * 7 variables calculant des sommes de soldes sur différents comptes épargnes\r\n\r\n\r\nLa construction de nouvelles variables a plusieurs avantages :\r\n\r\n\r\n* Fait entrer dans le modèle plusieurs variables en évitant la présence de variables corrélées\r\n* Limite le nombre de variables dans le modèle\r\n* Prend en compte un historique du comportement du client\r\n5.2. Maquette d’exploration des données \r\n\r\n\r\nUne fois les nouvelles variables créées, il reste de nombreuses variables qui n’apporte aucune information pour notre étude. Il faut donc les identifier et les supprimer dans un soucis de clarté. Pour cela, nous utilisons des maquettes d’exploration des données par Entité (1 :CMB, 3: CMSO, 21: Fortuneo). Cette étude par entité permet de s’assurer une homogénéité de la donnée entre les différents périmètres étudiés :\r\n\r\n\r\n* Données quantitatives : les valeurs étudiées sont les suivantes :\r\n   * nombre de valeurs manquantes\r\n   * variable constante ou non\r\n   * moyenn",
      "approx_tokens": 1000,
      "metadata": {
        "drive_file_id": "16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY",
        "name": "Copie de Rapport de Stage Youn Jéhanno",
        "mimeType": "application/vnd.google-apps.document",
        "modifiedTime": "2020-03-03T16:31:27.961Z",
        "webViewLink": "https://docs.google.com/document/d/16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z",
        "title": "Copie de Rapport de Stage Youn Jéhanno",
        "uri": "gdrive://16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY"
      }
    },
    {
      "doc_content_hash": "c15c1f661c58b476831cc175bf52ba64f582f34f4225cedff22a2362944466fc",
      "chunk_index": 4,
      "text": "ns un soucis de clarté. Pour cela, nous utilisons des maquettes d’exploration des données par Entité (1 :CMB, 3: CMSO, 21: Fortuneo). Cette étude par entité permet de s’assurer une homogénéité de la donnée entre les différents périmètres étudiés :\r\n\r\n\r\n* Données quantitatives : les valeurs étudiées sont les suivantes :\r\n   * nombre de valeurs manquantes\r\n   * variable constante ou non\r\n   * moyenne\r\n   * ecart-type\r\n   * minimum\r\n   * max\r\n   * quantiles 0.01, 0.05, 0.10, 0.20, 0.25,0.30, 0.40,0.50, 0.60, 0.70, 0.75,0.80, 0.90, 0.95, 0.99\r\n   * le pourcentage de 0.\r\n  \r\n                     \r\n\tMaquette d’exploration des données des variables quantitatives pour LE CMB (1)\r\n\t\r\n\r\nExemple :\r\n\r\n\r\nLes variables ddbu_dero et dfin_dero sont tout le temps égales à 0. Elles sont constantes et ne sont pas gardées dans le modèle.\r\n\r\n\r\nLa variables R_EPAMT_EPATOT possède des centiles chez Fortunéo au moins 10 fois supérieurs aux centiles du CMB et du CMSO. La variable est tout de même gardée car ses informations sont intéressantes.\r\n\r\n\r\n\r\n\r\n* Données qualitatives : les valeurs étudiées sont les suivantes :\r\n   * nombre de valeurs manquantes\r\n   * variable constante ou non\r\n   * effectif\r\n   * fréquence\r\n\r\n\r\n  \r\n\r\n\tMaquette d’exploration des données des variables qualitatives\r\n\t\r\n\r\n  \r\n\r\n\tLégende des alertes des variables qualitatives\r\n\t\r\n\r\nA partir de toutes ces informations, il faut déterminer quelles variables sont utiles et lesquelles ne le sont pas. \r\nLes variables constantes ou uniquement constituées de valeurs manquantes ne sont pas gardées.  Nous supprimons 173 variables.\r\n\r\n\r\nPour les autres variables, la décision revient à l’appréciation du statisticien. En fonction des expériences des modèles précédents et de notre jugement, 35 variables sont supprimées.\r\n\r\n\r\nSuite à l’exploration, nous supprimons 208 variables. Notre base contient alors 118 variables. \r\n5.3. Redécoupage des variables (CHAID)\r\nL’algorithme CHAID regroupe pour chaque variable explicative ses modalités les moins liées à la variable à expliquer, c’est-à-dire celles dont les taux de survenance de risques sont les plus voisins au vu du test du khi-2. Il le fait de proche en proche par paires de modalités puis de regroupements de modalités, jusqu’à ce que ne subsistent plus que des regroupements de modalités aux taux de survenance de risques suffisamment distincts.\r\n\r\n\r\nCe redécoupage est utile car nous manipulons des bases de données contenant de nombreuses observations. Cela va permettre de répartir la population en groupes homogènes, avec des variables discriminantes plus performantes. \r\n\r\n\r\n* Transformation des variables continues : discrétisation\r\n\r\n\r\n* Transformation des variables qualitatives : regroupement de modalités\r\n\r\n\r\nLe découpage ne doit pas être trop fin de manière à être le plus discriminant possible. Dans notre cas, pour certaine variables, on observe une tendance linéaire du taux de survenance de risques en fonction des valeurs de la variable. Il est donc important que notre découpage suive cette tendance. Pour cela il faut trouver le bon seuil pour notre découpage car un découpage trop fin ne respectera pas la règle mais un découpage trop large fera perdre de l’information.\r\n\r\n\r\nLa base est maintenant constitué uniquement de données qualitatives. Il reste à corriger le problème des valeurs manquantes.\r\n5.4. Recodage des données manquantes\r\n\r\n\r\nLes valeurs manquantes de chaque variable sont regroupées dans une modalité puis leur taux de survenance de risques est analysé. Selon les volumes observés, les données manquantes sont affectées à la catégorie la plus proche (en terme de taux de survenance de risques ) ou bien une modalité spécifique aux données manquantes est créée.\r\n\r\n\r\nLorsque le taux de valeurs manquantes est supérieur à 10% pour une variable, on considère les données manquantes comme une modalité spécifique.\r\nDans le cas contraire, celles-ci sont regroupées avec la modalité ayant le taux de survenance de risques le plus proche.\r\n\r\n\r\n\r\n\r\n\r",
      "approx_tokens": 1000,
      "metadata": {
        "drive_file_id": "16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY",
        "name": "Copie de Rapport de Stage Youn Jéhanno",
        "mimeType": "application/vnd.google-apps.document",
        "modifiedTime": "2020-03-03T16:31:27.961Z",
        "webViewLink": "https://docs.google.com/document/d/16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z",
        "title": "Copie de Rapport de Stage Youn Jéhanno",
        "uri": "gdrive://16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY"
      }
    },
    {
      "doc_content_hash": "c15c1f661c58b476831cc175bf52ba64f582f34f4225cedff22a2362944466fc",
      "chunk_index": 5,
      "text": " proche (en terme de taux de survenance de risques ) ou bien une modalité spécifique aux données manquantes est créée.\r\n\r\n\r\nLorsque le taux de valeurs manquantes est supérieur à 10% pour une variable, on considère les données manquantes comme une modalité spécifique.\r\nDans le cas contraire, celles-ci sont regroupées avec la modalité ayant le taux de survenance de risques le plus proche.\r\n\r\n\r\n\r\n\r\n\r\n\r\n  \r\n\r\n\tMaquette du recodage des valeurs manquantes\r\n\t\r\n\r\n\r\n\r\nDans cet exemple, les données manquantes de la variables decp_EXI_AVOIR2 représentent moins de 10 % des observations. Elles sont donc regroupées avec la modalité [-INF,0] qui est la modalité possédant le taux de survenance de risques le plus proche des valeurs manquantes.\r\n\r\n\r\nUne analyse “à dire d’expert” est également réalisée afin de s’assurer de la cohérence du découpage et de l’affectation des valeurs manquantes. En effet, dans certains cas où le nombre de valeurs manquantes est faible, le taux de survenance de risques observé n’est pas significatif. L’affectation à la modalité ayant le taux de survenance de risques le plus proche n’est donc pas forcément la meilleure. Il convient donc de l’affecter à la modalité répondant le mieux à la logique bancaire.\r\n\r\n\r\nCette étape permet aussi de vérifier les découpages qui ont été effectué par l’algorithme CHAID. Si une modalité contient trop peu d’individus, elle est regroupée avec la modalité la plus proche en terme de valeur de la variable.\r\n\r\n\r\n6. Développement des modèles\r\n6.1. Sélection des variables discriminantes\r\n6.1.1. ACP \r\n\r\n\r\nL’ACP permet d’étudier et de visualiser des corrélations entre les variables, afin d’éventuellement limiter le nombre de variables à mesurer par la suite. L’objectif est d’obtenir des variables non corrélées qui sont des combinaisons linéaires des variables de départ, afin d’utiliser ces variables dans les méthodes de modélisation utilisées par la suite telles que la régression logistique ou l’analyse discriminante. Elle permet d’identifier des groupes homogènes d’observations, ou au contraire des observations atypiques.\r\n\r\n\r\nNous obtenons alors 25 composantes principales. A chaque composante principale, nous affectons les variables ayant un coefficient de corrélation supérieur à 0,7.\r\n6.1.2. V de Cramer\r\nPour estimer si les modalités sont probablement indépendantes ou au contraire liées, on a coutume de procéder à un test du V de Cramer qui reste stable si l’on augmente la taille de l’échantillon dans les mêmes proportions inter-modalités. Plus le V de Cramer d’une variable est proche de zéro, plus il y a indépendance entre les deux variables étudiées. \r\nNous réalisons un test du V de Cramer entre la variable cible top_def et toutes les variables du modèle :\r\n  \r\n\r\nLa variable decp_nbj_depaut correspond à la moyenne au cours des 6 derniers mois du nombre mensuel de jours en dépassement de l’autorisation de découvert sur au moins un compte courant . Elle possède un V de Cramer très élevé car les individus ayant un dépassement d’autorisation ont plus de chance d’avoir une survenance de risques.\r\nLa variables decp_solde_cpt_ep2_PEP correspond au solde moyen mensuel du mois 2 sur le Plan Epargne Populaire de la personne. Elle possède un V de Cramer faible car la variable prend ses valeurs sur un mois. Ce temps n’est pas assez long pour avoir des valeurs significatives.\r\nOn retient :\r\n* Pour chaque composante principale de l’ACP, la ou les variables ayant le plus fort V de Cramer.\r\n* On ne retient les variables ayant un V de Cramer < 0,03. \r\n* Les moyennes, ratios ou autres variables combinées sont privilégiées par rapport aux variables simples (ex: moyenne sur 6 mois plutôt que valeur sur un mois)\r\n* Les variables sélectionnées dans l’ancien modèle. Ces variables sont censées être les plus explicatives pour les segments clients épargnants purs ainsi que les clients avec compte créditeur et sans crédit.\r\nVariables sélectionnées pour la modélisation : 28\r\nL’idée est de ne pas se limiter aux varia",
      "approx_tokens": 1000,
      "metadata": {
        "drive_file_id": "16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY",
        "name": "Copie de Rapport de Stage Youn Jéhanno",
        "mimeType": "application/vnd.google-apps.document",
        "modifiedTime": "2020-03-03T16:31:27.961Z",
        "webViewLink": "https://docs.google.com/document/d/16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z",
        "title": "Copie de Rapport de Stage Youn Jéhanno",
        "uri": "gdrive://16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY"
      }
    },
    {
      "doc_content_hash": "c15c1f661c58b476831cc175bf52ba64f582f34f4225cedff22a2362944466fc",
      "chunk_index": 6,
      "text": "ilégiées par rapport aux variables simples (ex: moyenne sur 6 mois plutôt que valeur sur un mois)\r\n* Les variables sélectionnées dans l’ancien modèle. Ces variables sont censées être les plus explicatives pour les segments clients épargnants purs ainsi que les clients avec compte créditeur et sans crédit.\r\nVariables sélectionnées pour la modélisation : 28\r\nL’idée est de ne pas se limiter aux variables ayant un V de Cramer élevé, mais de sélectionner des variables moins discriminantes mais qui apportent une information supplémentaire.\r\n6.1.3. Test de corrélation\r\nOn réalise un test du V de Cramer entre les 28 variables sélectionnées pour la modélisation pour observer les corrélations. Les variables ayant une corrélation trop élevée ( >0,70) sont retirées si elles possèdent un V de Cramer inférieur à la variable avec laquelle elles sont corrélées. En cas de V de Cramer très proches, les variables préexistantes sont préférées car plus fiables. Celles-ci font l’objet de suivis périodiques.\r\n  \r\n\r\n\tCorrélation entre les variables PCS, decp_nbj_depaut, decp_NBJ_DEPAUT_6M et decp_EXI_AVOIR2\r\n\t\r\n\r\nLes variables decp_nbj_depaut, decp_NBJ_DEPAUT_6M et decp_EXI_AVOIR2 ont des corrélations supérieures à 0,70. Cela signifie qu’elles possèdent globalement la même information. Seul decp_NBJ_DEPAUT_6M est gardées car c’est la variable ayant le meilleur V de Cramer.\r\nLa variable PCS est gardée car faiblement corrélée à toutes les autres. Sa corrélation est faible par rapport aux autres variables car la PCS correspond à la catégorie socioprofessionnelle du foyer et les autres variables font référence à des données bancaires. Cela apporte donc une information complémentaire.\r\nVariables sélectionnées pour la modélisation : 21\r\n6.2. Echantillonnage\r\nDécoupage de la base de modélisation en deux échantillons :\r\n\r\n\r\n* Échantillon d’apprentissage : 70% de la base de modélisation. Sert à construire les modèles\r\n\r\n\r\n* Échantillon test : 30% de la base de modélisation. Sert à valider les modèles.\r\n\r\n\r\n  \r\n\r\n\r\n\r\nChaque échantillon possède la même proportion de survenance de risques.\r\n\r\n\r\n6.3. Restriction des variables discriminantes \r\n                        - Stepwise & Bootstrap\r\n6.3.1 Régression stepwise\r\nLa régression Stepwise consiste à ajouter et à supprimer itérativement des prédicteurs, afin de trouver le sous-ensemble de variables dans l'ensemble de données résultant en le modèle le plus performant. L’entrée et la sortie d’une variable est testé  avec le test de rapport de vraisemblance.\r\nLe test de vraisemblance permet de comparer un modèle avec un sous-modèle et d’évaluer l’intérêt de la présence des termes complémentaires. On fait entrer ainsi de manière ascendante la modalité de la variable qui apparaît comme la plus significative au vu du test de rapport de vraisemblance. Puis, on teste la suppression de chaque variable du modèle avec le test de vraisemblance. Le processus s’arrête lorsque le modèle ne s’améliore plus.\r\nNous appliquons la fonction à notre base complète ainsi que sur notre base d’apprentissage. Les résultats sont les suivants :\r\n\r\n\r\n  \r\n\r\n\tRégression Stepwise sur échantillon d’apprentissage\r\n\t\r\n\r\nLes variables apparues parmi les 10 premières variables sont retenues. Nous retenons aussi les variables apparues dans le Stepwise sur échantillon total et qui ne sont pas apparues dans le Stepwise sur l’échantillon d’apprentissage. \r\nAu total, 14 variables sont retenues suite aux stepwises.\r\n6.3.2 Bootstrap\r\nL’idée du bootstrap est d’utiliser l’échantillon des observations pour permettre une inférence statistique plus fine. On réalise un certain nombre d’échantillons – qualifiés d’échantillon bootstrap- obtenus par tirage aléatoire d’observations de l’échantillon initial. Sur chacun des échantillons bootstrap, on estime les différents paramètres du modèle. On obtient par conséquent une suite de paramètres. Sous certaines conditions de régularité, la théorie montre que la distribution de la suite de paramètres obtenus converge vers l",
      "approx_tokens": 1000,
      "metadata": {
        "drive_file_id": "16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY",
        "name": "Copie de Rapport de Stage Youn Jéhanno",
        "mimeType": "application/vnd.google-apps.document",
        "modifiedTime": "2020-03-03T16:31:27.961Z",
        "webViewLink": "https://docs.google.com/document/d/16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z",
        "title": "Copie de Rapport de Stage Youn Jéhanno",
        "uri": "gdrive://16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY"
      }
    },
    {
      "doc_content_hash": "c15c1f661c58b476831cc175bf52ba64f582f34f4225cedff22a2362944466fc",
      "chunk_index": 7,
      "text": "rtain nombre d’échantillons – qualifiés d’échantillon bootstrap- obtenus par tirage aléatoire d’observations de l’échantillon initial. Sur chacun des échantillons bootstrap, on estime les différents paramètres du modèle. On obtient par conséquent une suite de paramètres. Sous certaines conditions de régularité, la théorie montre que la distribution de la suite de paramètres obtenus converge vers la réelle distribution du paramètre.\r\nToutefois, l’inconvénient réside dans les importantes capacités de calcul que l’application de ces techniques exige. A chaque fois qu’un échantillon bootstrap est constitué, une étape d’estimation des paramètres doit être réalisée. On pourra voir qu’en pratique notre fonction est effectivement longue en temps d’exécution (3 jours pour 20 échantillons bootstrap).\r\nNous réalisons une régression logistique à sélection ascendante pour chacun des échantillons bootstrap.\r\n\r\n\r\nLe tableau suivant montre nos résultats pour 5 des 20 échantillons bootstrap ainsi que nombre d'occurrences et l’étape moyenne d’apparition.\r\n\r\n\r\n  \r\n\r\n\tBootstrap sur échantillon d’apprentissage\r\n\t\r\n\r\nLes variables apparues au moins 2 fois sur les 20 regression bootstrap ou dont l’étape moyenne d’apparition est inférieure à 6 sont conservées.\r\nSuite au Bootstrap, 11 variables apparaissent au moins deux fois dans les 20 échantillons bootstrap.  \r\n\r\n\r\nA l’issue de nos sélections Stepwise et Bootstrap, nous choisissons de garder 15 variables.\r\n6.4. Étapes Sélection d’un ou plusieurs modèles\r\n\r\n\r\nL’objectif est de trouver un modèle avec les variables les plus discriminantes parmis notre ensemble de 13 variables. \r\n\r\n\r\nToutes les combinaisons de 1 à 5 variables sont testées (au delà de 5, le temps de calcul devient prohibitif). Sur chacune de ces combinaisons, nous lançons une régression logistique . \r\n\r\n\r\nPour chaque combinaison, un modèle de régression logistique est construit. La comparaison de la qualité des modèles se fait avec les indicateurs suivants :\r\n* la déviance\r\n* le R²\r\n* l’aire sous la courbe ROC pour l’échantillon d’apprentissage\r\n* l’aire sous la courbe ROC du modèle construit sur l’échantillon d’apprentissage et testé sur l’échantillon de test\r\n* la moyenne des statistiques de Wald\r\n* le nombre de statistiques de Wald inférieures à 1,96²\r\n* le degré de liberté, c’est-à-dire le nombre total de modalités dans le modèle\r\nComme il est difficile de trouver une combinaison optimisant simultanément tous ces critères, nous retenons les meilleurs compromis. \r\nIci, l’objectif du test de Wald est de vérifier si chaques prédicteur apporte de l’information supplémentaire. Le prédicteur importe de l’information lorsque sa statistiques de test est inférieures à 1,96² (quantile de la loi normale centrée réduite à 0,95 et au carré). Nous décidons de retenir les modèles uniquement composé de prédicteur apportant de l’information. Nous décidons de choisir les modèles dont toutes les statistiques de Wald sont supérieures à 1,96².\r\nC’est principalement la valeur de l’aire sous la courbe ROC du modèle construit sur l’échantillon d’apprentissage et testé sur l’échantillon de test qui sera observée. Nous considérons aussi le nombre de degrés de liberté des modèles, en évitant un nombre trop élevé synonyme de manque de robustesse et un nombre trop faible synonyme de manque de finesse de la notation (plus difficile à découper en classes de cotation risque).\r\nNous retenons ensuite la variable qui est présente dans la majorité des combinaisons sélectionnées. Nous réitérons les tests précédents, en ajoutant, à chaque nouvelle combinaison de 5 variables, la variable sélectionnée à l'étape précédente et maintenant entrée dans la sélection. \r\n  \r\n  \r\n\r\n\tComparaison du meilleur modèle à 5 variables et du meilleur modèle à 6 variables\r\n\r\n\r\n\tDans notre cas, l’information ajouté avec un modèle à 6 variable n’est pas significatif par rapport au modèle à 5 variables (l’aire sous la courbe ROC du modèle construit sur l’échantillon d’apprentissage et testé su",
      "approx_tokens": 1000,
      "metadata": {
        "drive_file_id": "16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY",
        "name": "Copie de Rapport de Stage Youn Jéhanno",
        "mimeType": "application/vnd.google-apps.document",
        "modifiedTime": "2020-03-03T16:31:27.961Z",
        "webViewLink": "https://docs.google.com/document/d/16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z",
        "title": "Copie de Rapport de Stage Youn Jéhanno",
        "uri": "gdrive://16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY"
      }
    },
    {
      "doc_content_hash": "c15c1f661c58b476831cc175bf52ba64f582f34f4225cedff22a2362944466fc",
      "chunk_index": 8,
      "text": " variable sélectionnée à l'étape précédente et maintenant entrée dans la sélection. \r\n  \r\n  \r\n\r\n\tComparaison du meilleur modèle à 5 variables et du meilleur modèle à 6 variables\r\n\r\n\r\n\tDans notre cas, l’information ajouté avec un modèle à 6 variable n’est pas significatif par rapport au modèle à 5 variables (l’aire sous la courbe ROC du modèle construit sur l’échantillon d’apprentissage et testé sur l’échantillon de test passe de 0,765 à 0,769). Son nombre de degré de liberté est plus important que le modèle à 5 variable ( 10 degré de liberté en plus pour le modèle à 6 variables). Le modèle à 5 variables est donc préféré car il apporte suffisamment d’informations et il est plus robuste.\r\nL’aire sous la courbe ROC du modèle construit sur l’échantillon d’apprentissage et testé sur l’échantillon de test est de 0,765. Cette valeur est légèrement supérieure à l’aire sous la courbe ROC pour l’échantillon d’apprentissage qui est de 0,764. Son indice de Gini sur l’échantillon d’apprentissage vaut 0,53. Il vaut 0,52 sur la base entière.\r\nNotre modèle est ensuite lancé sur la base test qui regarde la survenance de risques des clients sur la période de juin 2018 à juin 2019. L’indice de Gini obtenu est \r\n\r\nDescription des 5 variables retenues :\r\n* NBPROD_HORSEP : Nombre de produits hors épargne \r\n  \r\n\r\n* SOL_FIN_MOIS : Solde en fin de mois de tous les produits d’épargne\r\n  \r\n\r\n* SLD_CPTEPA1_6M : Moyenne du solde moyen sur livrets sur 6 mois\r\n  \r\n\r\n* nbjdeb_cum : Cumul du nombre de jours débiteurs\r\n  \r\n\r\n* p1_csp : Code socioprofessionnel\r\n  \r\n\r\n\r\n\r\nCes tableaux permettent d’observer une valeur croissante du taux de survenance de risques en fonction du produit hors épargne et du cumul du nombre de jours débiteurs.\r\nLe taux de survenance de risques en fonction du nombre de produits hors épargne n’est pas linéaire. Il peut remettre en cause le découpage. Le taux de survenance de risques des clients possédant un produit hors épargne est proche du taux de survenance de risques des clients possédant 0 où 2 produits hors épargnes. Un regroupement des modalités pourrait être envisagé dans une future version\r\nD’autre part, le taux de survenance de risques à une valeur décroissante en fonction de la valeur du solde sur livret et des soldes en fin de mois de tous les produits d’épargnes.\r\nDe plus nous pouvons observer que selon la catégorie socioprofessionnelle, le taux de survenance de risques peut être multiplié par 5.\r\n\r\n\r\nDans notre nouveau modèle, seule la valeur du solde sur livret qui est présente dans les deux modèles actuels se retrouve dans le modèle.\r\n6.5. Résultat\r\nLe modèle de régression logistique est calculé sur les variables précédemment définies par la formule suivante : \r\nLOGIT                 =             - 5,67735               \r\n                                     - 1,37607                     si           NBPROD_HORSEP  ≤  0 ou non renseignée\r\n                                    +0                          si           0 < NBPROD_HORSEP ≤  1\r\n                                   + 0,18250                    si           1 < NBPROD_HORSEP ≤  2\r\n                                    + 0,39927                   si           2 < NBPROD_HORSEP ≤  3\r\n                + 0,76042                  si           3 < NBPROD_HORSEP  \r\n                                     - 1,24787                    si           SOL_FIN_MOIS  < 64,1\r\n                                    +  0,99118                   si           64,1 < SOL_FIN_MOIS ≤  803,16\r\n                                   + 0,55623                    si           803,16 < SOL_FIN_MOIS ≤  4054,34\r\n                                    + 0,33262                  si           4054,34 < SOL_FIN_MOIS ≤  11382,19\r\n                                    + 0                          si           11382,19 < SOL_FIN_MOIS ≤  25047,69\r\n                - 0,21347                  si           25047,69 < SOL_FIN_MOIS \r\n                                     + 0,79870                     si           nbjdeb_cum > ",
      "approx_tokens": 1000,
      "metadata": {
        "drive_file_id": "16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY",
        "name": "Copie de Rapport de Stage Youn Jéhanno",
        "mimeType": "application/vnd.google-apps.document",
        "modifiedTime": "2020-03-03T16:31:27.961Z",
        "webViewLink": "https://docs.google.com/document/d/16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z",
        "title": "Copie de Rapport de Stage Youn Jéhanno",
        "uri": "gdrive://16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY"
      }
    },
    {
      "doc_content_hash": "c15c1f661c58b476831cc175bf52ba64f582f34f4225cedff22a2362944466fc",
      "chunk_index": 9,
      "text": "                                  + 0,33262                  si           4054,34 < SOL_FIN_MOIS ≤  11382,19\r\n                                    + 0                          si           11382,19 < SOL_FIN_MOIS ≤  25047,69\r\n                - 0,21347                  si           25047,69 < SOL_FIN_MOIS \r\n                                     + 0,79870                     si           nbjdeb_cum > 0 ou manquant\r\n                                    + 0                          si        nbjdeb_cum ≤  0\r\n                                      + 0,77437                    si           SLD_CPTEPA1_6M  < 404,76\r\n                                    +  0,34317                  si           404,76 < SLD_CPTEPA1_6M ≤  4078,82\r\n                                   + 0,05398                    si           4078,82 < SLD_CPTEPA1_6M ≤  14595,24\r\n                                   + 0                    si           14595,24 < SLD_CPTEPA1_6M\r\n                                    + 0,15753                si           SLD_CPTEPA1_6M manquant\r\n                                    + 0,39780                    si           p1_csp   ∈ {21,23,64,68,65,81,67}\r\n                                    + 0,21288                    si           p1_csp   ∈ {22,35,55,69,83,56,60}\r\n                                   - 0,58768                    si           p1_csp   ∈ {33,86,34,80,38,84}\r\n                                    - 0,41803                 si           p1_csp   ∈ {37,45,47,54,52}\r\n                                    + 0,08307                    si           p1_csp   ∈ {62,85,63}\r\n                                    + 0                           si           p1_csp   ∈ {11,43,31,48,12,13,46,53}\r\n                                    - 0,67290                   si           p1_csp   ∈ {42,44,70} ou manquant\r\n7. Découpage du score\r\n\r\n\r\nLe score est obtenu sur la base test qui regarde la survenance de risques des clients sur la période de juin 2018 à juin 2019. C’est sur cette base que le découpage du score est effectué.\r\nCette étape a pour but de découper le score obtenu précédemment, qui est une variable continue comprise entre 0 et 1 (probabilité), en 9 classes de cotation (A+, A-, B+, B-, C+, C-, D+, D-, E+) associées à des taux de survenance de risques croissants (de A+ à E+). La valeur des bornes (taux de survenance de risques ) des classes de cotation sont des valeurs préexistante. (grille de cotation des particuliers). La grille de cotation permet de comparer les différents segments entre eux.\r\nNotre objectif va être de regrouper nos clients dans chacune des classes de cotation. Pour ce faire, nous calculons dans un premier temps les centiles du score. Puis grâce à l’analyse de ces centiles via des tableaux croisés et des représentations graphiques, nous essayons de former des regroupement de centiles homogène et terme de score qui correspondent aux classes de cotation. Cela revient à essayer d’obtenir des classes de cotation avec des taux de survenance de risques de faible écart-type à l’intérieur de chacune des classes mais différents entre les classes. Ce regroupement se fait manuellement. Le principale critère étant que le taux de survenance de risques du centile médian pour chaque regroupement corresponde avec la valeur du taux de survenance de risques moyen de la classe de cotation.\r\nVoici le découpage obtenue en sélectionnant un centile sur dix :\r\n\r\n\r\n  \r\n  \r\n\r\n\tRedécoupage du score pour un centile sur dix avec grille de cotation particuliers\r\n\t\r\n\r\nLa majorité des clients sont classés en B+,B- et C+. Cela est en accord avec notre population étudiée qui présente un faible taux de survenance de risques même si il aurait été préférable d’obtenir plus de clients dans les segments A- voir A+.\r\nLes clients présents dans les derniers centiles présentent des taux de survenance de risques beaucoup plus élevé que le reste de la population. Notre étude a donc bien réussi à discriminer cette partie de la population qui correspond à la population cible.\r\n\r\n8.",
      "approx_tokens": 1000,
      "metadata": {
        "drive_file_id": "16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY",
        "name": "Copie de Rapport de Stage Youn Jéhanno",
        "mimeType": "application/vnd.google-apps.document",
        "modifiedTime": "2020-03-03T16:31:27.961Z",
        "webViewLink": "https://docs.google.com/document/d/16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z",
        "title": "Copie de Rapport de Stage Youn Jéhanno",
        "uri": "gdrive://16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY"
      }
    },
    {
      "doc_content_hash": "c15c1f661c58b476831cc175bf52ba64f582f34f4225cedff22a2362944466fc",
      "chunk_index": 10,
      "text": "résente un faible taux de survenance de risques même si il aurait été préférable d’obtenir plus de clients dans les segments A- voir A+.\r\nLes clients présents dans les derniers centiles présentent des taux de survenance de risques beaucoup plus élevé que le reste de la population. Notre étude a donc bien réussi à discriminer cette partie de la population qui correspond à la population cible.\r\n\r\n8. Conclusion\r\n\r\n\r\nLe nouveau modèle prédictif des clients Épargnants purs et des clients Compte Créditeur sans prêts possède un indice de Gini de 0,49 sur la base de test pour la période entre juin 2018 et juin 2019. Cet indice de Gini est faible mais il s’explique par une insuffisances en terme de volumétries de survenance de risques rendant une prédiction difficile. Néanmoins, le nouveau modèle obtient une meilleur performance que les deux modèles précédents. Le modèle actuel Épargnants Purs possède un indice de Gini égal à 0,21. Le modèle actuel Compte Créditeur sans prêt possède un indice de Gini égal à 0,51. Le nouveau modèle améliore donc fortement la performance pour le segment Épargnants Purs et légèrement pour le segment Compte Créditeur sans prêt.\r\nLe nouveau modèle sera donc préféré aux deux modèles actuels.\r\nCependant, le nouveau modèle peut encore être amélioré en effectuant un meilleur découpage des variables. Un redécoupage plus large apporterai plus de robustesse et pourrait amener à une meilleure prédiction.\r\n\r\n\r\n\r\n\r\n9. Compétences développées\r\n\r\n\r\nCe stage m’a permis de développer plusieurs compétences :\r\n\r\n\r\nCapacité de communication\r\n\r\n\r\nLa réalisation de mon stage a nécessité la communication avec les membres de l’équipe du service Modélisation des risques. Toutes les semaines, une présentation au reste de l’équipe de Modélisation SNI a lieu. Cette réunion permet donc de présenter les projets sur lesquels on travaille au reste de l’équipe. C’est un moment d’échange où tout le monde essaye d’aider dans les différents projets.  \r\nPour communiquer mes résultats il m’a aussi fallu les rendre compréhensible pour les autres membres de l’équipe à l’aide de graphes bien choisis.\r\n\r\n\r\nCapacité à se familiariser à un nouvel environnement\r\n\r\n\r\nLe monde bancaire était un monde qui m’était inconnu avant de faire mon stage. Il m’a donc fallu apprendre beaucoup de nouvelle notions en peu de temps afin de comprendre les données avec lesquels j’ai travaillé. Cela m’a permis de mieux comprendre les problématiques et d’analyser les résultats.\r\n\r\n\r\nCapacité de rigueur \r\n\r\n\r\nMes travaux effectués seront utilisés dans des projets futurs. Mon travail devait donc être soigneusement décrit afin que n’importe quelle personne puisse comprendre ce que j’ai effectué lors de mon stage. Cela m’a appris à structurer mon travail.\r\n\r\n\r\n10. Résumé\r\n\r\n\r\nLe Crédit mutuel Arkéa doit répondre aux exigences réglementaires de la Banque Centrale Européenne dans le cadre des accords Bâlois qui exigent une maîtrise complète des risques bancaires. Afin de pouvoir justifier ces éléments, tel que l’exigence de fond propre ou bien pour l’octroi de crédits, le Crédit mutuel Arkéa doit établir des modèles statistiques qui permettent d’estimer la probabilité de survenance de risques de ses clients. \r\nChaque client est affecté à un segment (Crédit immobilier, Épargnants Purs, Compte courant créditeur sans prêt, etc …). Pour chaque segment, un algorithme statistiques spécifique attribue une cotation à ses membres. Les algorithmes de cotation des segments Épargnants Purs et Compte courant créditeur sans prêt obtiennent des performances trop faibles lié à leurs faibles volumétries. Néanmoins, ces deux segments présentent des similarités d’un point de vue comportement et survenance de risques. Ma mission a été de regrouper ces deux types de clients en un seul segment afin de voir si une meilleure prédiction globale serait réalisable.\r\nTout d’abord, la base de données qui servira à la modélisation est construite sur le périmètre défini plus tôt. Les données sont ensuite ",
      "approx_tokens": 1000,
      "metadata": {
        "drive_file_id": "16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY",
        "name": "Copie de Rapport de Stage Youn Jéhanno",
        "mimeType": "application/vnd.google-apps.document",
        "modifiedTime": "2020-03-03T16:31:27.961Z",
        "webViewLink": "https://docs.google.com/document/d/16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z",
        "title": "Copie de Rapport de Stage Youn Jéhanno",
        "uri": "gdrive://16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY"
      }
    },
    {
      "doc_content_hash": "c15c1f661c58b476831cc175bf52ba64f582f34f4225cedff22a2362944466fc",
      "chunk_index": 11,
      "text": "ries. Néanmoins, ces deux segments présentent des similarités d’un point de vue comportement et survenance de risques. Ma mission a été de regrouper ces deux types de clients en un seul segment afin de voir si une meilleure prédiction globale serait réalisable.\r\nTout d’abord, la base de données qui servira à la modélisation est construite sur le périmètre défini plus tôt. Les données sont ensuite analysées pour obtenir une base de données de meilleure qualité. Un modèle de scoring est enfin élaboré de manière à prédire la survenance de risques.\r\n\r\n\r\nCrédit Mutuel Arkéa must meet the regulatory requirements of the European Central Bank in the context of the Basel agreements which require complete control of banking risks. In order to be able to justify these elements, such as the capital requirement or else for the granting of credits, Crédit Mutuel Arkéa must establish statistical models which make it possible to estimate the probability of the occurrence of risks for its customers.\r\nEach client is assigned to a segment (Real estate credit, Pure Savers, Credit current account without loan, etc.). For each segment, a specific statistical algorithm assigns a rating to its members. The algorithms for scoring the Pure Saver and Loan Credit Account segments get too low performance linked to their low volumes. However, these two segments present similarities from a behavior and risk occurrence point of view. My mission was to group these two types of customers into a single segment in order to see if a better global prediction would be achievable.\r\nFirst, the database that will be used for modeling is built on the perimeter defined earlier. The data is then analyzed to obtain a better quality database. Finally, a scoring model is developed in order to predict the occurrence of risks.\r\n\r\n\r\n\r\n\r\n11. Annexe\r\n\r\n\r\nRépartition du taux de survenance de risques de chaques modalités pour chaques variables",
      "approx_tokens": 481,
      "metadata": {
        "drive_file_id": "16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY",
        "name": "Copie de Rapport de Stage Youn Jéhanno",
        "mimeType": "application/vnd.google-apps.document",
        "modifiedTime": "2020-03-03T16:31:27.961Z",
        "webViewLink": "https://docs.google.com/document/d/16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY/edit?usp=drivesdk",
        "ingested_at": "2025-10-27T09:09:35.596455Z",
        "title": "Copie de Rapport de Stage Youn Jéhanno",
        "uri": "gdrive://16yYzkn4LrGvFfabGRKIgc6wCYtpfgf8PaN4SYtI79zY"
      }
    }
  ]
}